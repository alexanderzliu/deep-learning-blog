<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Liu">
<meta name="dcterms.date" content="2023-11-01">

<title>Deep Learning - Practical Deep Learning for Coders: Tabular Modeling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Deep Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Practical Deep Learning for Coders: Tabular Modeling</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Deep Learning</div>
                <div class="quarto-category">Tabular Data</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alex Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 1, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>In this lesson we’ll be exploring methods of tabular modeling. Tabular data is represented in a table i.e a spreadsheet with columns and rows. Our goal is to use the data in our columns to predict the value of another column.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip3 install fastbook kaggle waterfallcharts treeinterpreter dtreeviz</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastbook</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>fastbook.setup_book()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastbook <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pandas.api.types <span class="im">import</span> is_string_dtype, is_numeric_dtype, is_categorical_dtype</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.tabular.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dtreeviz.trees <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Image, display_svg, SVG</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> treeinterpreter <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>pd.options.display.max_rows <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>pd.options.display.max_columns <span class="op">=</span> <span class="dv">8</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>/Users/alexander/anaconda3/envs/.venv-dlb/bin/python: No module named pip3</code></pre>
</div>
</div>
<p>Some information before we start:</p>
<p>Continous variable - Data that can be represented numerically that can be mathematically operated on. We can feed these into our models directly because the mathematical operations we’ll do in our model (multiplication, addition, etc.) will make sense straight away.</p>
<p>Categorical variable - Variables that take on a variety of discrete values. These need to be converted to a number before we can use them as input in our model.</p>
<p>In our recent excercises we’ve just been training neural networks to solve every problem we’ve had.</p>
<p>hewwo, its bubu speaking now. you have 8 days to live. you can extend this time by buying bubu a thai iced tea. &gt;-&lt;</p>
<p>This hasn’t been because neural networks are the magical solution to all our problems, although I desperately wish this were the case. It would be so nice if deep learning could fight my demons for me.</p>
<p>Rather, it’s because their flexibility allows them to excel at handling problems related to the unstructured data we’ve been workign with so far - images, audio, and text.</p>
<p>When it comes to the structured, tabular data we’ll be working with here, we’ll make use of decision trees and gradient boosting machines. Simpler, but equally effective modeling techniques.</p>
<p>Decision trees will be our tool of choice for tabular data for a number of reasons. They’re typically faster and easier to train, requiring less specialized GPU hardware and hyperparameter tuning than neural networks. Libraries that implement decisions trees also have tools and methods for interpreting these models, such as visualizing which columns were the most important factors in making a prediction.</p>
<p>Note: there are some instances where a neural network would be better for tabular data - like if one of the columns is unstructured data like a string of text.</p>
<p>Yay, I’m now so excited to use decision trees. In fact why don’t I use one to model this feeling. It’ll try to predict if I am excited at any given moment or not. The condition it’ll split on is “are decision trees involved” and it’ll predict “Yes” if so, and “No” if not. Amazing, we already have an 100% accurate model.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>creds <span class="op">=</span> <span class="st">'{"username":"alexanderzliu","key":"59b8c0e913b2beb56e71628b918cba51"}'</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>cred_path <span class="op">=</span> Path(<span class="st">'~/.kaggle/kaggle.json'</span>).expanduser()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> cred_path.exists():</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    cred_path.parent.mkdir(exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    cred_path.write_text(creds)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    cred_path.chmod(<span class="bn">0o600</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>comp <span class="op">=</span> <span class="st">'bluebook-for-bulldozers'</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> URLs.path(comp)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>path</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>Path.BASE_PATH <span class="op">=</span> path</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> kaggle <span class="im">import</span> api</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> path.exists():</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    path.mkdir(parents<span class="op">=</span>true)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    api.competition_download_cli(comp, path<span class="op">=</span>path)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    shutil.unpack_archive(<span class="bu">str</span>(path<span class="op">/</span><span class="ss">f'</span><span class="sc">{</span>comp<span class="sc">}</span><span class="ss">.zip'</span>), <span class="bu">str</span>(path))</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>path.ls(file_type<span class="op">=</span><span class="st">'text'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>(#7) [Path('random_forest_benchmark_test.csv'),Path('Valid.csv'),Path('median_benchmark.csv'),Path('Test.csv'),Path('ValidSolution.csv'),Path('Machine_Appendix.csv'),Path('TrainAndValid.csv')]</code></pre>
</div>
</div>
<p>After setting up our data we’ll read it in and look at some of the columns. Seems like there’s a lot of columns for us to parse through</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(path<span class="op">/</span><span class="st">'TrainAndValid.csv'</span>, low_memory<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>df.columns</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>Index(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',
       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',
       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',
       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',
       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',
       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',
       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',
       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',
       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',
       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',
       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',
       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',
       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],
      dtype='object')</code></pre>
</div>
</div>
<p>Let’s take care of an ‘ordinal’ column. This is kinda a middle point between our categorical and continous variables where it means our variable is categorical, but the categories have a natural ordering.</p>
<p>We’ll make our ‘ProductSize’ column values take on the type ‘category’. Then we can pass in a list of those categories with a specific order, setting the ‘ordered’ parameter to True.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Product Size levels: </span><span class="sc">{</span>df[<span class="st">'ProductSize'</span>]<span class="sc">.</span>unique()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>sizes <span class="op">=</span> <span class="st">'Large'</span>,<span class="st">'Large / Medium'</span>,<span class="st">'Medium'</span>,<span class="st">'Small'</span>,<span class="st">'Mini'</span>,<span class="st">'Compact'</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'ProductSize'</span>] <span class="op">=</span> df[<span class="st">'ProductSize'</span>].astype(<span class="st">'category'</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'ProductSize'</span>].cat.set_categories(sizes, ordered<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Product Size levels: [nan 'Medium' 'Small' 'Large / Medium' 'Mini' 'Large' 'Compact']</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>0            NaN
1         Medium
2            NaN
3          Small
4            NaN
           ...  
412693      Mini
412694      Mini
412695      Mini
412696      Mini
412697      Mini
Name: ProductSize, Length: 412698, dtype: category
Categories (6, object): ['Large' &lt; 'Large / Medium' &lt; 'Medium' &lt; 'Small' &lt; 'Mini' &lt; 'Compact']</code></pre>
</div>
</div>
<p>The Kaggle competition asks uses to use the root mean squared log error (RMSLE) between actual and predicted auction prices as our metric - our statistic informing us of how well our model is performing. Roots, logs and trees; data scientists can’t get enough of plants.</p>
<p>We’ll take the log of our dependent variable now , the SalePrice, so that we can just use RMSE operations in the future to get our RMSLE.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>dep_var <span class="op">=</span> <span class="st">'SalePrice'</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>df[dep_var] <span class="op">=</span> np.log(df[dep_var])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Below is an illustration of a decision tree. Who knew such a messed-up looking conifer could be so powerful.</p>
<p>The tree asks a series of binary (yes/no) questions about the data. After each question, the data is split on branching yes/no paths. Eventually, the bottom of the tree (a leaf node) is reached, where that node will have a prediction for data point, which depends on the answers that led to that particular leaf node.</p>
<p>In this example, we first answer a question about age. This leads to questions about either our diet or excercise habits, depending on our first answer. Then depending on this next answer, we’re either labeled fit or unfit.</p>
<p>You’re all probably well aware of the category I fall into. As a super sexy data scientist, I am a indeed a fit model that fits models.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="decision_tree.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Decision Tree</figcaption>
</figure>
</div>
<p>Outside this toy example, our challenge will be to come up with this series of questions that will split our data into as distinct categories as possible, so that predictions we make are accurate because the conditions we split the data do a good job at discerning it from the other categories of our dependent variable.</p>
<p>This is how we’ll approach the construction process:</p>
<ol type="1">
<li>Loop through each column of the dataset</li>
<li>For each column, loop through each possible levels (values or categories) of that column</li>
<li>Try splitting the data into two groups, based on whether they are greater than or less than that level if it is continous, or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable.</li>
<li>Find the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. We’ll essentially create a very simple mini-model where our predictions will be the average sale price of the items that fall into our two groups after splitting.</li>
<li>After looping through all columns and possible levels for each, pick the split point that gave the best predictions using that simple model.</li>
<li>We now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, repeat steps 1 - 5 for these new datasets.</li>
<li>Continue this process recursively, until you have reached some stopping criterion for each group, e.g.&nbsp;stop splitting a group further when it has only a certain number of items in it.</li>
</ol>
<p>Let’s talk about handling dates. Dates are interesting because they contain a lot of both continuous and categorical data. For example - they tell us how far away another target date is, but also what day of the week, month of the year, or even day of the year (like for important holidays) it is.</p>
<p>We want to parse out all the data contained in our date, and will use a fastai function to do so - .add_datapart(), by passing in our data column in our train and test sets.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> add_datepart(df, <span class="st">'saledate'</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>df_test <span class="op">=</span> pd.read_csv(path<span class="op">/</span><span class="st">'Test.csv'</span>, low_memory<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>df_test <span class="op">=</span> add_datepart(df_test, <span class="st">'saledate'</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">' '</span>.join(o <span class="cf">for</span> o <span class="kw">in</span> df.columns <span class="cf">if</span> o.startswith(<span class="st">'sale'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/fastai/tabular/core.py:23: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/fastai/tabular/core.py:23: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed'</code></pre>
</div>
</div>
<p>Predictions for our test set will be trying to predict the SalePrice for bull dozers sold in the future. To mimic the relationship that the test set will have with the rest of our data (being in the future), we will also separate our training and validation sets in a similar manner.</p>
<p>We’ll make our our training set data from before October 2011, and validation set comprise of sales after that date, so that we validate our model on data from after our training set, which our test set will also be.</p>
<p>Using np.where() we can get the set of row indices that fulfill our date conditions.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>cond <span class="op">=</span> (df.saleYear<span class="op">&lt;</span><span class="dv">2011</span>) <span class="op">|</span> (df.saleMonth<span class="op">&lt;</span><span class="dv">10</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>train_idx <span class="op">=</span> np.where( cond)[<span class="dv">0</span>]</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>valid_idx <span class="op">=</span> np.where(<span class="op">~</span>cond)[<span class="dv">0</span>]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>splits <span class="op">=</span> (<span class="bu">list</span>(train_idx),<span class="bu">list</span>(valid_idx))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s start processing the rest of our data. We’ll make use of the TabularPandas class, which is a wrapper for our dataframe that provides us access to some more convenient functions.</p>
<p>We’ll need to pass in a few things when creating our TabularPandas object. First the ‘procs’, which will are data processing operations we want applied.</p>
<p>Categorify - turn columns into numerical category columns. FillMissing - fill rows with missing data (na) with the median of that column.</p>
<p>Then, the continous and categorical variables, which we’ll extract using the cont_cat_split() method.</p>
<p>Finally, our y-label/dependent variable and the training/validation splits.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>procs <span class="op">=</span> [Categorify, FillMissing]</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>cont,cat <span class="op">=</span> cont_cat_split(df, <span class="dv">1</span>, dep_var<span class="op">=</span>dep_var)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>to <span class="op">=</span> TabularPandas(df, procs, cat, cont, y_names<span class="op">=</span>dep_var, splits<span class="op">=</span>splits)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Taking a little bit of ac closer look at the ‘Categorify’ proc, we can see that the values are still represented as strings when we look at them, which is for our convenience in interpreting the data.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>to1 <span class="op">=</span> TabularPandas(df, procs, [<span class="st">'state'</span>, <span class="st">'ProductGroup'</span>, <span class="st">'Drive_System'</span>, <span class="st">'Enclosure'</span>], [], y_names<span class="op">=</span>dep_var, splits<span class="op">=</span>splits)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>to1.show(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">state</th>
<th data-quarto-table-cell-role="th">ProductGroup</th>
<th data-quarto-table-cell-role="th">Drive_System</th>
<th data-quarto-table-cell-role="th">Enclosure</th>
<th data-quarto-table-cell-role="th">SalePrice</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Alabama</td>
<td>WL</td>
<td>#na#</td>
<td>EROPS w AC</td>
<td>11.097410</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>North Carolina</td>
<td>WL</td>
<td>#na#</td>
<td>EROPS w AC</td>
<td>10.950807</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>New York</td>
<td>SSL</td>
<td>#na#</td>
<td>OROPS</td>
<td>9.210340</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>They’re actually just stored as numbers where all of the levels in a column are replaced by numbers. The levels aren’t indexed in any particular order, unless they ARE ordered as we saw in the ProductSize category. Otherwise, they’re just assigned as unique levels are seen from the top to bottom of the data frame.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>to1.items[[<span class="st">'state'</span>, <span class="st">'ProductGroup'</span>, <span class="st">'Drive_System'</span>, <span class="st">'Enclosure'</span>]].head(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">state</th>
<th data-quarto-table-cell-role="th">ProductGroup</th>
<th data-quarto-table-cell-role="th">Drive_System</th>
<th data-quarto-table-cell-role="th">Enclosure</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1</td>
<td>6</td>
<td>0</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>33</td>
<td>6</td>
<td>0</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>32</td>
<td>3</td>
<td>0</td>
<td>6</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>With this pre-processing done, let’s save our work right now, which will be us pickling our TabularPandas object. YUM I LOVE DILL.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>save_pickle(path<span class="op">/</span><span class="st">'to.pkl'</span>,to)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Okay, lets start to actually create a decision tree. We’ll use the .train and .valid attributes from our TabularPandas object to split our dataset.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>to <span class="op">=</span> load_pickle(path<span class="op">/</span><span class="st">'to.pkl'</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>xs,y <span class="op">=</span> to.train.xs,to.train.y</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>valid_xs,valid_y <span class="op">=</span> to.valid.xs,to.valid.y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then we’ll use the DecisionTreeRegressor class from sklearn (a nice library for non-deep learning machine learning models)</p>
<p>We’ll pass in an argument for the maximum leaf nodes it should have, fit it (which will be done through the 7-step method done mentioned above)</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> DecisionTreeRegressor(max_leaf_nodes<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>m.fit(xs, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here we can take a look at the tree. For each node it tells us the condition and value that it branches on (except for the leaf nodes), the squared error, the number of samples contained in that particular node, and the value, which is the average of the samples dependent variable.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>draw_tree(m, xs, size<span class="op">=</span><span class="dv">10</span>, leaves_parallel<span class="op">=</span><span class="va">True</span>, precision<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>ExecutableNotFound: failed to execute Path('dot'), make sure the Graphviz executables are on your systems' PATH</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>&lt;graphviz.sources.Source at 0x107dc6750&gt;</code></pre>
</div>
</div>
<p>There are some bulldozers that were somehow made in the year 1000, which is cap because dirt wasn’t even invented back then. We’ll just make the earliest manufacturing year 1950.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>xs.loc[xs[<span class="st">'YearMade'</span>]<span class="op">&lt;</span><span class="dv">1900</span>, <span class="st">'YearMade'</span>] <span class="op">=</span> <span class="dv">1950</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>valid_xs.loc[valid_xs[<span class="st">'YearMade'</span>]<span class="op">&lt;</span><span class="dv">1900</span>, <span class="st">'YearMade'</span>] <span class="op">=</span> <span class="dv">1950</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s make a new model. It’ll be larger because we aren’t specifying any stopping points. As a default, it’ll create levels until all the leaves are pure - that is, they only have samples from one level of our dependent variable.</p>
<p>We’ll also create a funciton for our models root mean squared error.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> DecisionTreeRegressor()</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>m.fit(xs, y)<span class="op">;</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> r_mse(pred,y): <span class="cf">return</span> <span class="bu">round</span>(math.sqrt(((pred<span class="op">-</span>y)<span class="op">**</span><span class="dv">2</span>).mean()), <span class="dv">6</span>)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> m_rmse(m, xs, y): <span class="cf">return</span> r_mse(m.predict(xs), y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A perfect model. JK it is not my bubu (that’s my pet name for my girlfriend - I am pandering to her :D). Our error is 0 on the validation set because we’ve create a huge tree with pure leaf nodes. The validation set’s error is quite a bit higher.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training set RMSE: </span><span class="sc">{</span>m_rmse(m, xs, y)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation set RMSE: </span><span class="sc">{</span>m_rmse(m, valid_xs, valid_y)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training set RMSE: 0.0
Validation set RMSE: 0.333112</code></pre>
</div>
</div>
<p>That’s cause our model is vastly overfitted. We can see that there are almost as many leaves as samples. This means that our model won’t generalize well to unseen data because it’s too specific (re: overfitted).</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>m.get_n_leaves(), <span class="bu">len</span>(xs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>(324365, 404710)</code></pre>
</div>
</div>
<p>Let’s fix this overfitting a bit. Now we’ll stop our tree creation when there are less than 25 samples in a leaf. This should make it generalize more well.</p>
<p>You can think about this as the model not uncovering patterns or relationships between our independent variables and dependent variable (e.g.&nbsp;newer bulldozers having higher sale prices). Instead our model is essentially memorizing the decision path it needs for a specific sample (e.g.&nbsp;This bulldozers sale price is 12. It has a Drive_System of X, Enclosure of Y, state is …and so on and so forth for all of our independent variables. The tree is going to split on all of these and create a leaf node with a value of 12).</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> DecisionTreeRegressor(min_samples_leaf<span class="op">=</span><span class="dv">25</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>m.fit(to.train.xs, to.train.y)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>(0.243019, 0.30936)</code></pre>
</div>
</div>
<p>Let’s talk about bagging. There are many types of bagging you might be familiar with - sandbagging, carpetbagging, teabagging. But today we’re dealing with some pure, unadultered bagging.</p>
<p>Bagging is basically getting predictions from a bunch of different models and taking the average of those predictions to use as our prediction. It’s a specific ‘ensembling’ approach, where multiple models are used to get your prediction. Here’s how we’ll bag:</p>
<ol type="1">
<li>Choose a random subset of our data<br>
</li>
<li>Train a model using this subset</li>
<li>Repeat steps 1 &amp; 2 a number of times</li>
<li>Make a prediction with all our models and then take the average of each of those model’s predictions to use as our prediction</li>
</ol>
<p>This approach works because by training our models on different sets of the data will produce uncorrelated error for our predictions, which should average out to 0.</p>
<p>Random forests is the technique when you bag with decision trees. Specifically, they not only use a random subset of the data for each tree’s training, but also choose from a random subset of the columns when deciding each split for the decision trees, essentially enforcing some variety in the splits of the trees.</p>
<p>Let’s start training a random forest of our own. There are some parameters we should go over here. - n_estimators: The number of decision trees in our random forest - max_samples: the number of samples we should take from our dataset to train each of our decision trees - max_features: The proportion of total columns to potentiallly split on at each level (.5 means we’ll consider half of the columns we have to split on at a given level) - min_samples_leaf: minimum number of samples a leaf should have - n_jobs: CPU-related argument; probably don’t need to worry about this too much</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rf(xs, y, n_estimators<span class="op">=</span><span class="dv">40</span>, max_samples<span class="op">=</span><span class="dv">200_000</span>,</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>       max_features<span class="op">=</span><span class="fl">0.5</span>, min_samples_leaf<span class="op">=</span><span class="dv">5</span>, <span class="op">**</span>kwargs):</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> RandomForestRegressor(n_jobs<span class="op">=-</span><span class="dv">1</span>, n_estimators<span class="op">=</span>n_estimators,</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>        max_samples<span class="op">=</span>max_samples, max_features<span class="op">=</span>max_features,</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>        min_samples_leaf<span class="op">=</span>min_samples_leaf, oob_score<span class="op">=</span><span class="va">True</span>).fit(xs, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can see how our error has improved with this random forest as compared to our singular decision tree. A cool thing about random forests is that they aren’t super sensitive to hyperparmeter changes, and work pretty well right off the bat with just their default arguments.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> rf(xs, y)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>(0.17133, 0.232621)</code></pre>
</div>
</div>
<p>After plotting our error as a function of the number of decision trees in our forest, we can see the error goes down until tapering out at around 30.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> np.stack([t.predict(valid_xs) <span class="cf">for</span> t <span class="kw">in</span> m.estimators_])</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>plt.plot([r_mse(preds[:i<span class="op">+</span><span class="dv">1</span>].mean(<span class="dv">0</span>), valid_y) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">40</span>)])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names
/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-23-output-2.png" width="585" height="414"></p>
</div>
</div>
<p>Let’s talk about OOBE. This fun little acronym stands for ‘out of bag error’ - an phrase that’s even more fun! This is our error for the rows that weren’t part of the random sample used in training our particular decision tree.</p>
<p>This is lower than our validation error, so there must be something bumping that number up.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>r_mse(m.oob_prediction_, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>0.21123</code></pre>
</div>
</div>
<p>Our preds is a stacked array of all of our trees’ predictions of all of the auctions. It’s got a shape of (40, 7900), which corresponds to 40 rows, one for each of our trees, and over 7000 columns, one for each of the auctions.</p>
<p>We’ll get the standard deviation of our predictions along the 0-index axis, which is between all our trees for a given auction. This value can be interpreted as how much our trees agree on their predictions for a given auction. We can see our the different values of standard deviations from our first 5 auctions how much the models (dis)agree on their predictions.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>preds_std <span class="op">=</span> preds.std(<span class="dv">0</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>preds_std[:<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>array([0.25915422, 0.12424009, 0.11996796, 0.24362901, 0.13341298])</code></pre>
</div>
</div>
<p>Not all features are created equal. Some of our columns might be used more than others to determine a split, or will result in a split that partitions the dataset in a significant manner. These will be of particular importance to us in being a more significant indicator of our dependent variable. If only there was some way to the determine the importance of our features. Our feature importance, if you will…</p>
<p>WELL WE’RE IN LUCK! Our random forest object has an attribute that will tell us the importances of our features, aptly named .feature_importances_.</p>
<p>Let’s take a look at our most important features.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rf_feat_importance(m, df):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame({<span class="st">'cols'</span>:df.columns, <span class="st">'imp'</span>:m.feature_importances_}</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>                       ).sort_values(<span class="st">'imp'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>fi <span class="op">=</span> rf_feat_importance(m, xs)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>fi[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">cols</th>
<th data-quarto-table-cell-role="th">imp</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">57</td>
<td>YearMade</td>
<td>0.170818</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">30</td>
<td>Coupler_System</td>
<td>0.110065</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>ProductSize</td>
<td>0.096737</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>fiProductClassDesc</td>
<td>0.087127</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">54</td>
<td>ModelID</td>
<td>0.056342</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">65</td>
<td>saleElapsed</td>
<td>0.051404</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3</td>
<td>fiSecondaryDesc</td>
<td>0.049120</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">32</td>
<td>Hydraulics_Flow</td>
<td>0.048373</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">31</td>
<td>Grouser_Tracks</td>
<td>0.042956</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">12</td>
<td>Enclosure</td>
<td>0.041079</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The way that importance is calculated, is that all branches of all the decision trees are iterated through. The ‘improvement’ of the model as a result of that split (usually measured by the gini or impurity reduction) is weighted by the number of samples in that node, and is summed for each feature across all branches of all trees. These sums are then normalized to 1.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_fi(fi):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fi.plot(<span class="st">'cols'</span>, <span class="st">'imp'</span>, <span class="st">'barh'</span>, figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">7</span>), legend<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>plot_fi(fi[:<span class="dv">30</span>])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-27-output-1.png" width="1094" height="561"></p>
</div>
</div>
<p>Let’s focus on just what’s important. We’ll remove all the columns with a really small feature importance value and train another model on our only important columns.</p>
<p>We can see that the performance is basically the same as our model trained on the entire dataset, but now the task of scrutinizing is much more manageable with this subset of columns.</p>
<p>Let’s make it a habit of simplying our model when possible as one of the first things we do to try and improve it.</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>to_keep <span class="op">=</span> fi[fi.imp<span class="op">&gt;</span><span class="fl">0.005</span>].cols</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(to_keep)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>xs_imp <span class="op">=</span> xs[to_keep]</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>valid_xs_imp <span class="op">=</span> valid_xs[to_keep]</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> rf(xs_imp, y)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Important feature training error: </span><span class="sc">{</span>m_rmse(m, xs_imp, y)<span class="sc">}</span><span class="ss">, Important feature valid error: </span><span class="sc">{</span>m_rmse(m, valid_xs_imp, valid_y)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f" All columns: </span><span class="sc">{</span><span class="bu">len</span>(xs.columns)<span class="sc">}</span><span class="ss">, Important columns: </span><span class="sc">{</span><span class="bu">len</span>(xs_imp.columns)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Important feature training error: 0.181966, Important feature valid error: 0.23228
 All columns: 66, Important columns: 21</code></pre>
</div>
</div>
<p>Another step we can take to simplying our model (by removing unnecessary columns) is to remove redundant ones. Here’s a visualization of the our most similar columns.</p>
<p>As expected, something like ‘saleYear’ and ‘saleElapsed’ are very similar. Even though they might have different values, the model will be able to extract the same insights - how long ago it was sold - from the columns.</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>cluster_columns(xs_imp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-29-output-1.png" width="925" height="488"></p>
</div>
</div>
<p>Let’s experiment to see if dropping columns has any affect on our performance (hint: if we’re making a point to illustrate this technique it probably won’t). I’m a big fan of dropping things - the beat, some bars, some facts, and columns are no exception.</p>
<p>This method will give us our OOB score, and the parameters to the model we’re training in this function are set up to train it quickly. We can see the baseline OOB error for our model with all important columns below.</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_oob(df):</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span><span class="dv">40</span>, min_samples_leaf<span class="op">=</span><span class="dv">15</span>,</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>        max_samples<span class="op">=</span><span class="dv">50000</span>, max_features<span class="op">=</span><span class="fl">0.5</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, oob_score<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    m.fit(df, y)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> m.oob_score_</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>get_oob(xs_imp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>0.8757528501983463</code></pre>
</div>
</div>
<p>Dropping one of each our potentially redudant variables at a time, the OOB error doesn’t change much.</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>{c:get_oob(xs_imp.drop(c, axis<span class="op">=</span><span class="dv">1</span>)) <span class="cf">for</span> c <span class="kw">in</span> (</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'saleYear'</span>, <span class="st">'saleElapsed'</span>, <span class="st">'ProductGroupDesc'</span>,<span class="st">'ProductGroup'</span>,</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'fiModelDesc'</span>, <span class="st">'fiBaseModel'</span>,</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Hydraulics_Flow'</span>,<span class="st">'Grouser_Tracks'</span>, <span class="st">'Coupler_System'</span>)}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>{'saleYear': 0.8748843391113379,
 'saleElapsed': 0.870177319246467,
 'ProductGroupDesc': 0.8766598105150334,
 'ProductGroup': 0.8757226956292669,
 'fiModelDesc': 0.8742575959480965,
 'fiBaseModel': 0.8747567022539106,
 'Hydraulics_Flow': 0.8763373418656346,
 'Grouser_Tracks': 0.8764419294629359,
 'Coupler_System': 0.8757376058228592}</code></pre>
</div>
</div>
<p>Even dropping multiple redundant columns at a time still holds up</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>to_drop <span class="op">=</span> [<span class="st">'saleYear'</span>, <span class="st">'ProductGroupDesc'</span>, <span class="st">'fiBaseModel'</span>, <span class="st">'Grouser_Tracks'</span>]</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>get_oob(xs_imp.drop(to_drop, axis<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>0.8727226538412444</code></pre>
</div>
</div>
<p>Let’s create a dataframe with our dropped variables (and save it as a good practice).</p>
<p>As a final check, we’ll look at the validation errors from this random forest model training on our dataset with redundancies eliminated. Looks pretty similar to our performance from having the entire dataset, so it looks like we’ve been able to effectively simplify our model even further. I told you it would all work out.</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>xs_final <span class="op">=</span> xs_imp.drop(to_drop, axis<span class="op">=</span><span class="dv">1</span>) <span class="co">#create dataframe without dropped columns</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>valid_xs_final <span class="op">=</span> valid_xs_imp.drop(to_drop, axis<span class="op">=</span><span class="dv">1</span>) <span class="co">#create dataframe without dropped columns</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>save_pickle(path<span class="op">/</span><span class="st">'xs_final.pkl'</span>, xs_final) <span class="co">#save df</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>save_pickle(path<span class="op">/</span><span class="st">'valid_xs_final.pkl'</span>, valid_xs_final) <span class="co">#save df</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>xs_final <span class="op">=</span> load_pickle(path<span class="op">/</span><span class="st">'xs_final.pkl'</span>) <span class="co">#reload df</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>valid_xs_final <span class="op">=</span> load_pickle(path<span class="op">/</span><span class="st">'valid_xs_final.pkl'</span>) <span class="co">#reload df</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> rf(xs_final, y) <span class="co">#train random forest model</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>m_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y) <span class="co">#get validation error</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>(0.183987, 0.233538)</code></pre>
</div>
</div>
<p>Now let’s talk about ‘partial dependence’. These plots try and isolate the effect that a single independent variable has on our dependent variable, without that relationship being affected by changes in our other variables.</p>
<p>How this will work is we’ll take every level of our independent variable (IV) in question (we’re looking at ‘YearMade’ and ‘ProductSize’ in this example because they are our most important features) and for our entire dataset, replace every value of that independent variable with one of the levels of our IV. Then we’ll use our model to make predictions , where all our other features are the same as before, except for our IV which has had all of its values swapped with one of its levels. Then we’ll repeat this process for every level in that IV. This will give us predictions where everything is held constant except for the IV we’re investigating.</p>
<p>This translates to our working dataset by keeping all the columns the same except ‘YearMade’. We’ll replace all the values in that column with 1950, make our predictions, take the average, and that’ll be our prediction for bulldozers from 1950. Then we’ll replace all the values in the column with 1951, and rinse and repeat with making and averaging predictions for all the levels in the ‘YearMade’ column.</p>
<p>In plotting this, we can see that the year and price have a very strong, positively correlated relationship, as we’d except. The ‘ProductSize’ doesn’t quite have as clean a relationship though, which we’ll talk about shortly.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> plot_partial_dependence</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>fig,ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>plot_partial_dependence(m, valid_xs_final, [<span class="st">'YearMade'</span>,<span class="st">'ProductSize'</span>],</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>                        grid_resolution<span class="op">=</span><span class="dv">20</span>, ax<span class="op">=</span>ax)<span class="op">;</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We’ve seen how much an impact our features have aggregated across our entire dataset, but what if we wanted to drill down into a single row and analyze how our features influenced the prediction for that one data point?</p>
<p>We can do this with the treeinterpreter library.</p>
<p>Let’s pass in the first 5 rows of our validation set and pass them to our treeinterpreter, along with our model.</p>
<p>The output we’ll get from the .predict() method is: - prediction: self-explanatory (our prediction from our random forest) - bias: the prediction we’d get just by taking the mean of our dependent variable. This would be equivalent to if our model was just the root of the tree with no branches - contributions: The increase or decrease in our prediction based on branch split we take on our path to a leaf node a.k.a the change in our prediction caused by the independent variables. Summing this up, and adding it to our bias will equal the prediction.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>row <span class="op">=</span> valid_xs_final.iloc[:<span class="dv">5</span>]</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>prediction,bias,contributions <span class="op">=</span> treeinterpreter.predict(m, row.values)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Plotting these contributions, we can visualize the impact that each of the independent variables had on our prediction.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>waterfall(valid_xs_final.columns, contributions[<span class="dv">0</span>], threshold<span class="op">=</span><span class="fl">0.08</span>, </span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>          rotation_value<span class="op">=</span><span class="dv">45</span>,formatting<span class="op">=</span><span class="st">'{:,.3f}'</span>)<span class="op">;</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We’ve covered some pretty effective yet simple ways of improving and interpreting our model. But it’s not all sunshine and rainbows when it comes to machine learning.</p>
<p>Random forests have a pretty large issue when it comes to extrapolating data - that is making predictions for data it hasn’t seen before, that we’ll illustrate in an example.</p>
<p>Let’s train a model on the first 30 rows of a slightly noisy but mostly linear dataset</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>x_lin <span class="op">=</span> torch.linspace(<span class="dv">0</span>,<span class="dv">20</span>, steps<span class="op">=</span><span class="dv">40</span>)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>y_lin <span class="op">=</span> x_lin <span class="op">+</span> torch.randn_like(x_lin)</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>plt.scatter(x_lin, y_lin)<span class="op">;</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>xs_lin <span class="op">=</span> x_lin.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>x_lin.shape,xs_lin.shape</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>x_lin[:,<span class="va">None</span>].shape</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>m_lin <span class="op">=</span> RandomForestRegressor().fit(xs_lin[:<span class="dv">30</span>],y_lin[:<span class="dv">30</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-34-output-1.png" width="570" height="414"></p>
</div>
</div>
<p>When we try to make a prediction on the rest of the dataset (the data is blue, our predictions are red). From this graph, we can see that predictions for an x-value greater than 12.5 are all too low, with them all being around 16.</p>
<p>This is because of how are predictions are generated. Remember that they’re just the average of the dependent variable values for the samples in our leaf nodes. Thus, they’ll all be a fixed number, based on data that the model has seen before. So when it makes predictions on new data, it’s essentially saying “you’re closest to this group that I’ve seen before, so that’ll be your prediction. You might actually be really far off from this group, but you’re even farther off from all the other groups, so that’s the best I can do for you.”</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(x_lin, y_lin, <span class="dv">20</span>)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(x_lin, m_lin.predict(xs_lin), color<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-35-output-1.png" width="570" height="414"></p>
</div>
</div>
<p>With this extrapolation problem in mind, we want to make sure the distribution of our training and validation sets is similar - that our validation set does not contain any ‘Out-of-Domain’ data.</p>
<p>We can assess the distribution of our training and validation sets in a pretty clever way. Let’s combine the two, and instead of trying to predict a SalesPrice, introduce a new dependent variable which will be whether or not the data was originally in the training or validation set, and try to predict that.</p>
<p>Looking at the features that are most important in discerning the original set of a sample, we see the top 3 are ‘saleElapsed’, ‘SalesID’, and ‘MachineID’. This makes sense since the original way we split up the set was by the date.</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>df_dom <span class="op">=</span> pd.concat([xs_final, valid_xs_final])</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>is_valid <span class="op">=</span> np.array([<span class="dv">0</span>]<span class="op">*</span><span class="bu">len</span>(xs_final) <span class="op">+</span> [<span class="dv">1</span>]<span class="op">*</span><span class="bu">len</span>(valid_xs_final))</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> rf(df_dom, is_valid)</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>rf_feat_importance(m, df_dom)[:<span class="dv">6</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">cols</th>
<th data-quarto-table-cell-role="th">imp</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">5</td>
<td>saleElapsed</td>
<td>0.854117</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>SalesID</td>
<td>0.113978</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>MachineID</td>
<td>0.024497</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>YearMade</td>
<td>0.002955</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>ModelID</td>
<td>0.001058</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>fiProductClassDesc</td>
<td>0.000660</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Let’s use a technique we used above, and see what the performance of our model would be when one of these columns is removed.</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> rf(xs_final, y)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'orig'</span>, m_rmse(m, valid_xs_final, valid_y))</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> c <span class="kw">in</span> (<span class="st">'SalesID'</span>,<span class="st">'saleElapsed'</span>,<span class="st">'MachineID'</span>):</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> rf(xs_final.drop(c,axis<span class="op">=</span><span class="dv">1</span>), y)</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(c, m_rmse(m, valid_xs_final.drop(c,axis<span class="op">=</span><span class="dv">1</span>), valid_y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>orig 0.232237
SalesID 0.229874
saleElapsed 0.234587
MachineID 0.230984</code></pre>
</div>
</div>
<p>We’ll drop the ‘SalesID’ and ‘MachineID’ columns, which actually improves the accuracy of our model.</p>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>time_vars <span class="op">=</span> [<span class="st">'SalesID'</span>,<span class="st">'MachineID'</span>]</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>xs_final_time <span class="op">=</span> xs_final.drop(time_vars, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>valid_xs_time <span class="op">=</span> valid_xs_final.drop(time_vars, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> rf(xs_final_time, y)</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>m_rmse(m, valid_xs_time, valid_y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>0.228792</code></pre>
</div>
</div>
<p>Another thing that can help is to make sure we’re only using relevant data. Since it seems like the age of the bulldozer is a very important feature to our predictions, we’ll actually just use a subset of our data, the most recent 20 years or so, to train our model so that any outdated relationships aren’t captured in our dataset.</p>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>xs[<span class="st">'saleYear'</span>].hist()<span class="op">;</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>filt <span class="op">=</span> xs[<span class="st">'saleYear'</span>]<span class="op">&gt;</span><span class="dv">2004</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>xs_filt <span class="op">=</span> xs_final_time[filt]</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>y_filt <span class="op">=</span> y[filt]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-39-output-1.png" width="600" height="414"></p>
</div>
</div>
<p>This model trained on only data from after 2004 is performing better than anything else we’ve seen so far.</p>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> rf(xs_filt, y_filt)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>m_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>(0.178646, 0.229726)</code></pre>
</div>
</div>
<p>Alright, would we really be data scientists if we didn’t throw a neural network at everything? Let’s see how they can help. Let’s pre-process our data the same way we did before passing it into the TabularPandas object, and also use only the subset of columns that we’ve analyzed to be most relevant.</p>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>df_nn <span class="op">=</span> pd.read_csv(path<span class="op">/</span><span class="st">'TrainAndValid.csv'</span>, low_memory<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>df_nn[<span class="st">'ProductSize'</span>] <span class="op">=</span> df_nn[<span class="st">'ProductSize'</span>].astype(<span class="st">'category'</span>)</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>df_nn[<span class="st">'ProductSize'</span>].cat.set_categories(sizes, ordered<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>df_nn[dep_var] <span class="op">=</span> np.log(df_nn[dep_var])</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>df_nn <span class="op">=</span> add_datepart(df_nn, <span class="st">'saledate'</span>)</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>df_nn_final <span class="op">=</span> df_nn[<span class="bu">list</span>(xs_final_time.columns) <span class="op">+</span> [dep_var]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/fastai/tabular/core.py:23: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.</code></pre>
</div>
</div>
<p>We’ll also have to handle categorical and continous variables differently with neural networks, because we can use embeddings here. We’ll use the cont_cat_split function and pass in a ‘max_card’ argument, which will treat variables as categorical if there are less levels than max_card.</p>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>cont_nn,cat_nn <span class="op">=</span> cont_cat_split(df_nn_final, max_card<span class="op">=</span><span class="dv">9000</span>, dep_var<span class="op">=</span>dep_var)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Our ‘saleElapsed’ column is treated as a continous variable, which is what we want because we wouldn’t be able to extrapolate for sales in the future if it was a categorical variable.</p>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>cont_nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>['saleElapsed']</code></pre>
</div>
</div>
<p>It looks like we have two categories with a lot of unique values. fiModelDesc has over 5000 and sound sjust like fiModelDescriptor. They literally just sound redundant, so let’s see if we can get away with removing one of them (hint: it won’t)</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>df_nn_final[cat_nn].nunique()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>YearMade                73
Coupler_System           2
ProductSize              6
fiProductClassDesc      74
ModelID               5281
fiSecondaryDesc        177
Hydraulics_Flow          3
Enclosure                6
fiModelDesc           5059
fiModelDescriptor      140
Drive_System             4
Hydraulics              12
Track_Type               2
ProductGroup             6
dtype: int64</code></pre>
</div>
</div>
<p>Hint: it didn’t</p>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>xs_filt2 <span class="op">=</span> xs_filt.drop(<span class="st">'fiModelDescriptor'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>valid_xs_time2 <span class="op">=</span> valid_xs_time.drop(<span class="st">'fiModelDescriptor'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>m2 <span class="op">=</span> rf(xs_filt2, y_filt)</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(m_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y))</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>cat_nn.remove(<span class="st">'fiModelDescriptor'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.177728 0.230042</code></pre>
</div>
</div>
<p>Now that we’ve done some exploration and refinement of our dataset we can create a TabularPandas object to handle the rest of our processing. We’ll use a large batch size because tabular models are generally GPU RAM friendly.</p>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>procs_nn <span class="op">=</span> [Categorify, FillMissing, Normalize]</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>to_nn <span class="op">=</span> TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>                      splits<span class="op">=</span>splits, y_names<span class="op">=</span>dep_var)</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> to_nn.dataloaders(<span class="dv">1024</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll create our tabular model with the tabular_learner class. We specify a y_range for our regression model, based on the max and mins we see in the training set. Since we have a lot of data, we’ll make our layers a bit larger, and use MSE as our loss function as defined in the Kaggle competition.</p>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> to_nn.train.y</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y.<span class="bu">min</span>(),y.<span class="bu">max</span>())</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> tabular_learner(dls, y_range<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">12</span>), layers<span class="op">=</span>[<span class="dv">500</span>,<span class="dv">250</span>],</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>                        n_out<span class="op">=</span><span class="dv">1</span>, loss_func<span class="op">=</span>F.mse_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>8.465899 11.863583</code></pre>
</div>
</div>
<p>Using the lr_find() method to get our optimal learning rate, let’s train the model for a few epochs.</p>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>learn.lr_find()</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">5</span>, <span class="fl">1e-2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.072696</td>
<td>0.067694</td>
<td>00:06</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.055061</td>
<td>0.069597</td>
<td>00:06</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.048782</td>
<td>0.068780</td>
<td>00:07</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.044022</td>
<td>0.051267</td>
<td>00:06</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.041040</td>
<td>0.050207</td>
<td>00:06</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-48-output-5.png" width="596" height="436"></p>
</div>
</div>
<p>Using it for predictions, we can see that it does a better job than our random forest. The trade off is that it took longer to train, and we need to do some more work to find the hyperparameters that allowed it to perform so well.</p>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>preds,targs <span class="op">=</span> learn.get_preds()</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>learn.save(<span class="st">'nn'</span>) <span class="co">#save model incase we want to come back to it</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(r_mse(preds,targs))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.224069</code></pre>
</div>
</div>
<p>Let’s take ensembling even further. Our random forest was an ensemble of decision trees, but there’s no rules saying that we have to use the same architecture model architecture. We’ll combine our already ensembled model, the random forest, and our neural network and create an enseensemblemble.</p>
<p>We’ll need to convert their outputs to the same shape and object type, but after doing that we can see the average of the predictions from these two models is better than either of them separately.</p>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>rf_preds <span class="op">=</span> m.predict(valid_xs_time)</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>ens_preds <span class="op">=</span> (to_np(preds.squeeze()) <span class="op">+</span> rf_preds) <span class="op">/</span><span class="dv">2</span></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>r_mse(ens_preds,valid_y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>0.22146</code></pre>
</div>
</div>
<p>Let’s quickly talk about another decision tree ensemble - gradient boosting machines. Instead of taking the average of a bunch of predictions like in random forest, with gradient boosting our final prediction will come from the sum of our set of predictions. It works like so:</p>
<ol type="1">
<li>Train a small model that will inevitably underfit the data set</li>
<li>Calculate the predictions in the training set for this model</li>
<li>Subtract the predictions from the targets to get our ‘residuals’ which is our error for each of our data points</li>
<li>Repeat steps 1 - 3, but instead of using our original targets, we now target our residuals calculated from the last iteration instead</li>
<li>Continue doing this until you reach some stopping criterion e.g.&nbsp;a max number of trees is created, or our validation error is getting worse.</li>
</ol>
<p>By doing this, we’re sequentially trying to build another tree that will bridge the gap that the last one had between the predictions and target (which are our residuals after the first tree), over and over again.</p>
<p>Then, to get our final prediction, we sum up all of the predictions we get from the trees in our ensemble.</p>
<p>Let’s finish of this entry with some more of our favorite thing: text explanations. We actually ended up doing a lot of different, useful things this notebook and I want to recap them.</p>
<p>We took a look at 3 different models:</p>
<ol type="1">
<li>Random Forests: Pros:
<ul>
<li>Fast to train</li>
<li>Easy to get started with by requiring minimal preprocessing or hyperparameter tuning</li>
<li>Resilient to overfitting with enough trees</li>
<li>Can provide insight into data with partial dependence analysis or feature importance Cons:</li>
<li>May be less accurate than other approaches, and if extrapolation is needed then thats a problem</li>
</ul></li>
<li>Neural Networks: Pros:
<ul>
<li>Good results &amp; extrapolate well</li>
<li>Can use parts (embedding layer) in other models even if the predictions/rest of the layers aren’t Cons:</li>
<li>Long to train</li>
<li>Finnicky -&gt; requires more preprocessing and hyperparameter tuning to get good performance</li>
<li>Can overfit</li>
</ul></li>
<li>Gradient Boosting: Pros:
<ul>
<li>Fast to train (about the same as random forest)</li>
<li>Can be more accurate than random forest Cons:</li>
<li>Require hyperparameter tuning</li>
<li>Can overfit</li>
</ul></li>
</ol>
<p>And also did some important work in data exploration and processing our data set to improve our results, and understanding of our data.</p>
<p>We: - Look at feature importance to determine our most impact columns and remove the least import ones - Removed redundant columns - Deal with out-of-domain data issues by filtering our data set to try and get our training and validation set to have - Did partial dependence to see the impact one independent variable has on our dependent variable</p>
<p>And we’re able to do all of that from the starting off by just training a model and seeing where it could take us. These have been useful techniques in improving our understanding of the data, and we were able to make use of them by just jumping in and getting our bearings from where we landed. We literally modeled first, and asked questions later (and then modeled again some more). Having a bias towards action, is a good attitude to have when in deep learning.</p>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>