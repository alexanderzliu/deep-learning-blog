<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Liu">
<meta name="dcterms.date" content="2023-10-17">

<title>Deep Learning - Practical Deep Learning for Coders: Lesson 3 Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Deep Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Practical Deep Learning for Coders: Lesson 3 Notes</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">fast.ai</div>
                <div class="quarto-category">Neural Networks</div>
                <div class="quarto-category">Gradient Descent</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alex Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 17, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Okay let’s go finally learning about what everyone want’s to know about when learning about AI and Machine Learning: stochastic gradient descent.</p>
<p>The first two lesson’s of this course had us familiarize ourselves with what a deep learning application looks like without really getting into any of the theory or inner workings. We used the fastai library that provides a layer of abstraction for the techniques used in fine-tuning and training models (as it should). Now we get into components of machine learning that so many other MOOCs start with right off the bat - loss functions and gradient descent. It was only inevitable.</p>
<p>Anyways, let’s get on with the notebook. The example we’re working through is an image classification model that can classify any image as a 3 or a 7. Or you could just go on r/truerateme for that.</p>
<p>Alright let’s import all our standard fastbook and fastai libraries.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>{sys.executable} <span class="op">-</span>m pip install fastbook</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastbook</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>fastbook.setup_book()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.vision.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastbook <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>matplotlib.rc(<span class="st">'image'</span>, cmap<span class="op">=</span><span class="st">'Greys'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Requirement already satisfied: fastbook in /opt/anaconda3/lib/python3.8/site-packages (0.0.29)
Requirement already satisfied: fastai&gt;=2.6 in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (2.7.13)
Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (2.14.5)
Requirement already satisfied: ipywidgets&lt;8 in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (7.6.3)
Requirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (2.25.1)
Requirement already satisfied: pip in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (21.0.1)
Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (0.1.99)
Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (1.2.4)
Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (4.34.0)
Requirement already satisfied: graphviz in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (0.20.1)
Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (20.9)
Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.24.1)
Requirement already satisfied: spacy&lt;4 in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (3.7.2)
Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (5.4.1)
Requirement already satisfied: pillow&gt;=9.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (10.1.0)
Requirement already satisfied: fastcore&lt;1.6,&gt;=1.5.29 in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.5.29)
Requirement already satisfied: fastdownload&lt;2,&gt;=0.0.5 in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.0.7)
Requirement already satisfied: torch&lt;2.2,&gt;=1.10 in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (2.1.0)
Requirement already satisfied: torchvision&gt;=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.16.0)
Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (3.3.4)
Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.6.2)
Requirement already satisfied: fastprogress&gt;=0.2.4 in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.0.3)
Requirement already satisfied: ipykernel&gt;=4.5.1 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets&lt;8-&gt;fastbook) (5.3.4)
Requirement already satisfied: traitlets&gt;=4.3.1 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets&lt;8-&gt;fastbook) (5.0.5)
Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets&lt;8-&gt;fastbook) (3.5.1)
Requirement already satisfied: ipython&gt;=4.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets&lt;8-&gt;fastbook) (7.22.0)
Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets&lt;8-&gt;fastbook) (1.0.0)
Requirement already satisfied: nbformat&gt;=4.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets&lt;8-&gt;fastbook) (5.1.3)
Requirement already satisfied: tornado&gt;=4.2 in /opt/anaconda3/lib/python3.8/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;8-&gt;fastbook) (6.1)
Requirement already satisfied: appnope in /opt/anaconda3/lib/python3.8/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;8-&gt;fastbook) (0.1.2)
Requirement already satisfied: jupyter-client in /opt/anaconda3/lib/python3.8/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;8-&gt;fastbook) (6.1.12)
Requirement already satisfied: pygments in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (2.8.1)
Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (3.0.17)
Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (5.0.6)
Requirement already satisfied: setuptools&gt;=18.5 in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (52.0.0.post20210125)
Requirement already satisfied: backcall in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.2.0)
Requirement already satisfied: pexpect&gt;4.3 in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (4.8.0)
Requirement already satisfied: pickleshare in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.7.5)
Requirement already satisfied: jedi&gt;=0.16 in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.17.2)
Requirement already satisfied: parso&lt;0.8.0,&gt;=0.7.0 in /opt/anaconda3/lib/python3.8/site-packages (from jedi&gt;=0.16-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.7.0)
Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /opt/anaconda3/lib/python3.8/site-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets&lt;8-&gt;fastbook) (3.2.0)
Requirement already satisfied: jupyter-core in /opt/anaconda3/lib/python3.8/site-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets&lt;8-&gt;fastbook) (4.7.1)
Requirement already satisfied: ipython-genutils in /opt/anaconda3/lib/python3.8/site-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.2.0)
Requirement already satisfied: pyrsistent&gt;=0.14.0 in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.17.3)
Requirement already satisfied: six&gt;=1.11.0 in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&lt;8-&gt;fastbook) (1.15.0)
Requirement already satisfied: attrs&gt;=17.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&lt;8-&gt;fastbook) (20.3.0)
Requirement already satisfied: ptyprocess&gt;=0.5 in /opt/anaconda3/lib/python3.8/site-packages (from pexpect&gt;4.3-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.7.0)
Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.2.5)
Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.8)
Requirement already satisfied: weasel&lt;0.4.0,&gt;=0.1.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.3.3)
Requirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (6.4.0)
Requirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.4.2)
Requirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.5)
Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.10)
Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (4.66.1)
Requirement already satisfied: wasabi&lt;1.2.0,&gt;=0.9.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.1.2)
Requirement already satisfied: thinc&lt;8.3.0,&gt;=8.1.8 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.2.1)
Requirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.10)
Requirement already satisfied: typer&lt;0.10.0,&gt;=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.9.0)
Requirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.11 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.12)
Requirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.4.8)
Requirement already satisfied: numpy&gt;=1.15.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.20.1)
Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.11.3)
Requirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.3.0)
Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.9)
Requirement already satisfied: pyparsing&gt;=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging-&gt;fastbook) (2.4.7)
Requirement already satisfied: annotated-types&gt;=0.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.6.0)
Requirement already satisfied: typing-extensions&gt;=4.6.1 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (4.8.0)
Requirement already satisfied: pydantic-core==2.10.1 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.10.1)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests-&gt;fastbook) (2.10)
Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests-&gt;fastbook) (2020.12.5)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests-&gt;fastbook) (1.26.4)
Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests-&gt;fastbook) (4.0.0)
Requirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /opt/anaconda3/lib/python3.8/site-packages (from thinc&lt;8.3.0,&gt;=8.1.8-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.7.11)
Requirement already satisfied: confection&lt;1.0.0,&gt;=0.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from thinc&lt;8.3.0,&gt;=8.1.8-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.1.3)
Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.8/site-packages (from torch&lt;2.2,&gt;=1.10-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.12)
Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.8/site-packages (from torch&lt;2.2,&gt;=1.10-&gt;fastai&gt;=2.6-&gt;fastbook) (1.8)
Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.8/site-packages (from torch&lt;2.2,&gt;=1.10-&gt;fastai&gt;=2.6-&gt;fastbook) (2023.6.0)
Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.8/site-packages (from torch&lt;2.2,&gt;=1.10-&gt;fastai&gt;=2.6-&gt;fastbook) (2.5)
Requirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from typer&lt;0.10.0,&gt;=0.3.0-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (7.1.2)
Requirement already satisfied: cloudpathlib&lt;0.17.0,&gt;=0.7.0 in /opt/anaconda3/lib/python3.8/site-packages (from weasel&lt;0.4.0,&gt;=0.1.0-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.16.0)
Requirement already satisfied: notebook&gt;=4.4.1 in /opt/anaconda3/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (6.3.0)
Requirement already satisfied: prometheus-client in /opt/anaconda3/lib/python3.8/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.10.1)
Requirement already satisfied: argon2-cffi in /opt/anaconda3/lib/python3.8/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (20.1.0)
Requirement already satisfied: terminado&gt;=0.8.3 in /opt/anaconda3/lib/python3.8/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.9.4)
Requirement already satisfied: nbconvert in /opt/anaconda3/lib/python3.8/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (6.0.7)
Requirement already satisfied: pyzmq&gt;=17 in /opt/anaconda3/lib/python3.8/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (20.0.0)
Requirement already satisfied: Send2Trash&gt;=1.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (1.5.0)
Requirement already satisfied: python-dateutil&gt;=2.1 in /opt/anaconda3/lib/python3.8/site-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;8-&gt;fastbook) (2.8.1)
Requirement already satisfied: cffi&gt;=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from argon2-cffi-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (1.14.5)
Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.8/site-packages (from cffi&gt;=1.0.0-&gt;argon2-cffi-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (2.20)
Requirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.8/site-packages (from datasets-&gt;fastbook) (0.70.15)
Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.8/site-packages (from datasets-&gt;fastbook) (3.4.1)
Requirement already satisfied: dill&lt;0.3.8,&gt;=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from datasets-&gt;fastbook) (0.3.7)
Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.8/site-packages (from datasets-&gt;fastbook) (3.8.4)
Requirement already satisfied: pyarrow&gt;=8.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from datasets-&gt;fastbook) (13.0.0)
Requirement already satisfied: huggingface-hub&lt;1.0.0,&gt;=0.14.0 in /opt/anaconda3/lib/python3.8/site-packages (from datasets-&gt;fastbook) (0.17.3)
Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (6.0.4)
Requirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (4.0.2)
Requirement already satisfied: aiosignal&gt;=1.1.2 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.3.1)
Requirement already satisfied: frozenlist&gt;=1.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.3.3)
Requirement already satisfied: charset-normalizer&lt;4.0,&gt;=2.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (3.0.1)
Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.8.2)
Requirement already satisfied: MarkupSafe&gt;=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.1.1)
Requirement already satisfied: cycler&gt;=0.10 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (0.10.0)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (1.3.1)
Requirement already satisfied: jupyterlab-pygments in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.1.2)
Requirement already satisfied: entrypoints&gt;=0.2.2 in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.3)
Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.8.4)
Requirement already satisfied: testpath in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.4.4)
Requirement already satisfied: nbclient&lt;0.6.0,&gt;=0.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.5.3)
Requirement already satisfied: bleach in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (3.3.0)
Requirement already satisfied: pandocfilters&gt;=1.4.1 in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (1.4.3)
Requirement already satisfied: defusedxml in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.7.1)
Requirement already satisfied: async-generator in /opt/anaconda3/lib/python3.8/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (1.10)
Requirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.8/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (1.5.1)
Requirement already satisfied: webencodings in /opt/anaconda3/lib/python3.8/site-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.5.1)
Requirement already satisfied: pytz&gt;=2017.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas-&gt;fastbook) (2021.1)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (2.1.0)
Requirement already satisfied: joblib&gt;=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.1)
Requirement already satisfied: mpmath&gt;=0.19 in /opt/anaconda3/lib/python3.8/site-packages (from sympy-&gt;torch&lt;2.2,&gt;=1.10-&gt;fastai&gt;=2.6-&gt;fastbook) (1.2.1)
Requirement already satisfied: safetensors&gt;=0.3.1 in /opt/anaconda3/lib/python3.8/site-packages (from transformers-&gt;fastbook) (0.4.0)
Requirement already satisfied: tokenizers&lt;0.15,&gt;=0.14 in /opt/anaconda3/lib/python3.8/site-packages (from transformers-&gt;fastbook) (0.14.1)
Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.8/site-packages (from transformers-&gt;fastbook) (2021.4.4)</code></pre>
</div>
</div>
<p>Then we’ll download a our sample from MNIST containing images of 3s and 7s, and use the .ls method to put these items into a special fastai list that also displays the amount of items in the list.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> untar_data(URLs.MNIST_SAMPLE)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>Path.BASE_PATH <span class="op">=</span> path</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>threes <span class="op">=</span> (path<span class="op">/</span><span class="st">'train'</span><span class="op">/</span><span class="st">'3'</span>).ls().<span class="bu">sorted</span>()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>sevens <span class="op">=</span> (path<span class="op">/</span><span class="st">'train'</span><span class="op">/</span><span class="st">'7'</span>).ls().<span class="bu">sorted</span>()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>path.ls()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>(#3) [Path('valid'),Path('labels.csv'),Path('train')]</code></pre>
</div>
</div>
<p>Why don’t we take a look at one of them. Oh yeah that’s a 3 all right.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>im3_path <span class="op">=</span> threes[<span class="dv">1</span>]</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>im3 <span class="op">=</span> Image.<span class="bu">open</span>(im3_path)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>im3</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<p><img src="index_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Then we’ll turn the image into a 2-d array where the values represent the darkness of pixel. We then put this into put this into a dataframe with some condotional formatting to visualize that pixel’s darkness value…for some reason. It’s cool I guess</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>im3_t <span class="op">=</span> tensor(im3)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(im3_t[<span class="dv">4</span>:<span class="dv">15</span>,<span class="dv">4</span>:<span class="dv">22</span>])</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>df.style.set_properties(<span class="op">**</span>{<span class="st">'font-size'</span>:<span class="st">'6pt'</span>}).background_gradient(<span class="st">'Greys'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<style type="text/css">
#T_31d93_row0_col0,#T_31d93_row0_col1,#T_31d93_row0_col2,#T_31d93_row0_col3,#T_31d93_row0_col4,#T_31d93_row0_col5,#T_31d93_row0_col6,#T_31d93_row0_col7,#T_31d93_row0_col8,#T_31d93_row0_col9,#T_31d93_row0_col10,#T_31d93_row0_col11,#T_31d93_row0_col12,#T_31d93_row0_col13,#T_31d93_row0_col14,#T_31d93_row0_col15,#T_31d93_row0_col16,#T_31d93_row0_col17,#T_31d93_row1_col0,#T_31d93_row1_col1,#T_31d93_row1_col2,#T_31d93_row1_col3,#T_31d93_row1_col4,#T_31d93_row1_col15,#T_31d93_row1_col16,#T_31d93_row1_col17,#T_31d93_row2_col0,#T_31d93_row2_col1,#T_31d93_row2_col2,#T_31d93_row2_col15,#T_31d93_row2_col16,#T_31d93_row2_col17,#T_31d93_row3_col0,#T_31d93_row3_col15,#T_31d93_row3_col16,#T_31d93_row3_col17,#T_31d93_row4_col0,#T_31d93_row4_col6,#T_31d93_row4_col7,#T_31d93_row4_col8,#T_31d93_row4_col9,#T_31d93_row4_col10,#T_31d93_row4_col15,#T_31d93_row4_col16,#T_31d93_row4_col17,#T_31d93_row5_col0,#T_31d93_row5_col5,#T_31d93_row5_col6,#T_31d93_row5_col7,#T_31d93_row5_col8,#T_31d93_row5_col9,#T_31d93_row5_col15,#T_31d93_row5_col16,#T_31d93_row5_col17,#T_31d93_row6_col0,#T_31d93_row6_col1,#T_31d93_row6_col2,#T_31d93_row6_col3,#T_31d93_row6_col4,#T_31d93_row6_col5,#T_31d93_row6_col6,#T_31d93_row6_col7,#T_31d93_row6_col8,#T_31d93_row6_col9,#T_31d93_row6_col14,#T_31d93_row6_col15,#T_31d93_row6_col16,#T_31d93_row6_col17,#T_31d93_row7_col0,#T_31d93_row7_col1,#T_31d93_row7_col2,#T_31d93_row7_col3,#T_31d93_row7_col4,#T_31d93_row7_col5,#T_31d93_row7_col6,#T_31d93_row7_col13,#T_31d93_row7_col14,#T_31d93_row7_col15,#T_31d93_row7_col16,#T_31d93_row7_col17,#T_31d93_row8_col0,#T_31d93_row8_col1,#T_31d93_row8_col2,#T_31d93_row8_col3,#T_31d93_row8_col4,#T_31d93_row8_col13,#T_31d93_row8_col14,#T_31d93_row8_col15,#T_31d93_row8_col16,#T_31d93_row8_col17,#T_31d93_row9_col0,#T_31d93_row9_col1,#T_31d93_row9_col2,#T_31d93_row9_col3,#T_31d93_row9_col4,#T_31d93_row9_col16,#T_31d93_row9_col17,#T_31d93_row10_col0,#T_31d93_row10_col1,#T_31d93_row10_col2,#T_31d93_row10_col3,#T_31d93_row10_col4,#T_31d93_row10_col5,#T_31d93_row10_col6,#T_31d93_row10_col17{
            font-size:  6pt;
            background-color:  #ffffff;
            color:  #000000;
        }#T_31d93_row1_col5{
            font-size:  6pt;
            background-color:  #efefef;
            color:  #000000;
        }#T_31d93_row1_col6,#T_31d93_row1_col13{
            font-size:  6pt;
            background-color:  #7c7c7c;
            color:  #000000;
        }#T_31d93_row1_col7{
            font-size:  6pt;
            background-color:  #4a4a4a;
            color:  #f1f1f1;
        }#T_31d93_row1_col8,#T_31d93_row1_col9,#T_31d93_row1_col10,#T_31d93_row2_col5,#T_31d93_row2_col6,#T_31d93_row2_col7,#T_31d93_row2_col11,#T_31d93_row2_col12,#T_31d93_row2_col13,#T_31d93_row3_col4,#T_31d93_row3_col12,#T_31d93_row3_col13,#T_31d93_row4_col1,#T_31d93_row4_col2,#T_31d93_row4_col3,#T_31d93_row4_col12,#T_31d93_row4_col13,#T_31d93_row5_col12,#T_31d93_row6_col11,#T_31d93_row9_col11,#T_31d93_row10_col11,#T_31d93_row10_col12,#T_31d93_row10_col13,#T_31d93_row10_col14,#T_31d93_row10_col15,#T_31d93_row10_col16{
            font-size:  6pt;
            background-color:  #000000;
            color:  #f1f1f1;
        }#T_31d93_row1_col11{
            font-size:  6pt;
            background-color:  #606060;
            color:  #f1f1f1;
        }#T_31d93_row1_col12{
            font-size:  6pt;
            background-color:  #4d4d4d;
            color:  #f1f1f1;
        }#T_31d93_row1_col14{
            font-size:  6pt;
            background-color:  #bbbbbb;
            color:  #000000;
        }#T_31d93_row2_col3{
            font-size:  6pt;
            background-color:  #e4e4e4;
            color:  #000000;
        }#T_31d93_row2_col4,#T_31d93_row8_col6{
            font-size:  6pt;
            background-color:  #6b6b6b;
            color:  #000000;
        }#T_31d93_row2_col8,#T_31d93_row2_col14,#T_31d93_row3_col14{
            font-size:  6pt;
            background-color:  #171717;
            color:  #f1f1f1;
        }#T_31d93_row2_col9,#T_31d93_row3_col11{
            font-size:  6pt;
            background-color:  #4b4b4b;
            color:  #f1f1f1;
        }#T_31d93_row2_col10,#T_31d93_row7_col10,#T_31d93_row8_col8,#T_31d93_row8_col10,#T_31d93_row9_col8,#T_31d93_row9_col10{
            font-size:  6pt;
            background-color:  #010101;
            color:  #f1f1f1;
        }#T_31d93_row3_col1{
            font-size:  6pt;
            background-color:  #272727;
            color:  #f1f1f1;
        }#T_31d93_row3_col2{
            font-size:  6pt;
            background-color:  #0a0a0a;
            color:  #f1f1f1;
        }#T_31d93_row3_col3{
            font-size:  6pt;
            background-color:  #050505;
            color:  #f1f1f1;
        }#T_31d93_row3_col5{
            font-size:  6pt;
            background-color:  #333333;
            color:  #f1f1f1;
        }#T_31d93_row3_col6{
            font-size:  6pt;
            background-color:  #e6e6e6;
            color:  #000000;
        }#T_31d93_row3_col7,#T_31d93_row3_col10{
            font-size:  6pt;
            background-color:  #fafafa;
            color:  #000000;
        }#T_31d93_row3_col8{
            font-size:  6pt;
            background-color:  #fbfbfb;
            color:  #000000;
        }#T_31d93_row3_col9{
            font-size:  6pt;
            background-color:  #fdfdfd;
            color:  #000000;
        }#T_31d93_row4_col4{
            font-size:  6pt;
            background-color:  #1b1b1b;
            color:  #f1f1f1;
        }#T_31d93_row4_col5{
            font-size:  6pt;
            background-color:  #e0e0e0;
            color:  #000000;
        }#T_31d93_row4_col11{
            font-size:  6pt;
            background-color:  #4e4e4e;
            color:  #f1f1f1;
        }#T_31d93_row4_col14{
            font-size:  6pt;
            background-color:  #767676;
            color:  #000000;
        }#T_31d93_row5_col1{
            font-size:  6pt;
            background-color:  #fcfcfc;
            color:  #000000;
        }#T_31d93_row5_col2,#T_31d93_row5_col3{
            font-size:  6pt;
            background-color:  #f6f6f6;
            color:  #000000;
        }#T_31d93_row5_col4,#T_31d93_row7_col7{
            font-size:  6pt;
            background-color:  #f8f8f8;
            color:  #000000;
        }#T_31d93_row5_col10,#T_31d93_row10_col7{
            font-size:  6pt;
            background-color:  #e8e8e8;
            color:  #000000;
        }#T_31d93_row5_col11{
            font-size:  6pt;
            background-color:  #222222;
            color:  #f1f1f1;
        }#T_31d93_row5_col13,#T_31d93_row6_col12{
            font-size:  6pt;
            background-color:  #090909;
            color:  #f1f1f1;
        }#T_31d93_row5_col14{
            font-size:  6pt;
            background-color:  #d0d0d0;
            color:  #000000;
        }#T_31d93_row6_col10,#T_31d93_row7_col11,#T_31d93_row9_col6{
            font-size:  6pt;
            background-color:  #060606;
            color:  #f1f1f1;
        }#T_31d93_row6_col13{
            font-size:  6pt;
            background-color:  #979797;
            color:  #000000;
        }#T_31d93_row7_col8{
            font-size:  6pt;
            background-color:  #b6b6b6;
            color:  #000000;
        }#T_31d93_row7_col9{
            font-size:  6pt;
            background-color:  #252525;
            color:  #f1f1f1;
        }#T_31d93_row7_col12{
            font-size:  6pt;
            background-color:  #999999;
            color:  #000000;
        }#T_31d93_row8_col5{
            font-size:  6pt;
            background-color:  #f9f9f9;
            color:  #000000;
        }#T_31d93_row8_col7{
            font-size:  6pt;
            background-color:  #101010;
            color:  #f1f1f1;
        }#T_31d93_row8_col9,#T_31d93_row9_col9{
            font-size:  6pt;
            background-color:  #020202;
            color:  #f1f1f1;
        }#T_31d93_row8_col11{
            font-size:  6pt;
            background-color:  #545454;
            color:  #f1f1f1;
        }#T_31d93_row8_col12{
            font-size:  6pt;
            background-color:  #f1f1f1;
            color:  #000000;
        }#T_31d93_row9_col5{
            font-size:  6pt;
            background-color:  #f7f7f7;
            color:  #000000;
        }#T_31d93_row9_col7{
            font-size:  6pt;
            background-color:  #030303;
            color:  #f1f1f1;
        }#T_31d93_row9_col12{
            font-size:  6pt;
            background-color:  #181818;
            color:  #f1f1f1;
        }#T_31d93_row9_col13{
            font-size:  6pt;
            background-color:  #303030;
            color:  #f1f1f1;
        }#T_31d93_row9_col14{
            font-size:  6pt;
            background-color:  #a9a9a9;
            color:  #000000;
        }#T_31d93_row9_col15{
            font-size:  6pt;
            background-color:  #fefefe;
            color:  #000000;
        }#T_31d93_row10_col8,#T_31d93_row10_col9{
            font-size:  6pt;
            background-color:  #bababa;
            color:  #000000;
        }#T_31d93_row10_col10{
            font-size:  6pt;
            background-color:  #393939;
            color:  #f1f1f1;
        }</style>
<table id="T_31d93_" data-quarto-postprocess="true" class="table table-sm table-striped small">
<thead>
<tr class="header">
<th class="blank level0" data-quarto-table-cell-role="th"></th>
<th class="col_heading level0 col0" data-quarto-table-cell-role="th">0</th>
<th class="col_heading level0 col1" data-quarto-table-cell-role="th">1</th>
<th class="col_heading level0 col2" data-quarto-table-cell-role="th">2</th>
<th class="col_heading level0 col3" data-quarto-table-cell-role="th">3</th>
<th class="col_heading level0 col4" data-quarto-table-cell-role="th">4</th>
<th class="col_heading level0 col5" data-quarto-table-cell-role="th">5</th>
<th class="col_heading level0 col6" data-quarto-table-cell-role="th">6</th>
<th class="col_heading level0 col7" data-quarto-table-cell-role="th">7</th>
<th class="col_heading level0 col8" data-quarto-table-cell-role="th">8</th>
<th class="col_heading level0 col9" data-quarto-table-cell-role="th">9</th>
<th class="col_heading level0 col10" data-quarto-table-cell-role="th">10</th>
<th class="col_heading level0 col11" data-quarto-table-cell-role="th">11</th>
<th class="col_heading level0 col12" data-quarto-table-cell-role="th">12</th>
<th class="col_heading level0 col13" data-quarto-table-cell-role="th">13</th>
<th class="col_heading level0 col14" data-quarto-table-cell-role="th">14</th>
<th class="col_heading level0 col15" data-quarto-table-cell-role="th">15</th>
<th class="col_heading level0 col16" data-quarto-table-cell-role="th">16</th>
<th class="col_heading level0 col17" data-quarto-table-cell-role="th">17</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_31d93_level0_row0" class="row_heading level0 row0" data-quarto-table-cell-role="th">0</td>
<td id="T_31d93_row0_col0" class="data row0 col0">0</td>
<td id="T_31d93_row0_col1" class="data row0 col1">0</td>
<td id="T_31d93_row0_col2" class="data row0 col2">0</td>
<td id="T_31d93_row0_col3" class="data row0 col3">0</td>
<td id="T_31d93_row0_col4" class="data row0 col4">0</td>
<td id="T_31d93_row0_col5" class="data row0 col5">0</td>
<td id="T_31d93_row0_col6" class="data row0 col6">0</td>
<td id="T_31d93_row0_col7" class="data row0 col7">0</td>
<td id="T_31d93_row0_col8" class="data row0 col8">0</td>
<td id="T_31d93_row0_col9" class="data row0 col9">0</td>
<td id="T_31d93_row0_col10" class="data row0 col10">0</td>
<td id="T_31d93_row0_col11" class="data row0 col11">0</td>
<td id="T_31d93_row0_col12" class="data row0 col12">0</td>
<td id="T_31d93_row0_col13" class="data row0 col13">0</td>
<td id="T_31d93_row0_col14" class="data row0 col14">0</td>
<td id="T_31d93_row0_col15" class="data row0 col15">0</td>
<td id="T_31d93_row0_col16" class="data row0 col16">0</td>
<td id="T_31d93_row0_col17" class="data row0 col17">0</td>
</tr>
<tr class="even">
<td id="T_31d93_level0_row1" class="row_heading level0 row1" data-quarto-table-cell-role="th">1</td>
<td id="T_31d93_row1_col0" class="data row1 col0">0</td>
<td id="T_31d93_row1_col1" class="data row1 col1">0</td>
<td id="T_31d93_row1_col2" class="data row1 col2">0</td>
<td id="T_31d93_row1_col3" class="data row1 col3">0</td>
<td id="T_31d93_row1_col4" class="data row1 col4">0</td>
<td id="T_31d93_row1_col5" class="data row1 col5">29</td>
<td id="T_31d93_row1_col6" class="data row1 col6">150</td>
<td id="T_31d93_row1_col7" class="data row1 col7">195</td>
<td id="T_31d93_row1_col8" class="data row1 col8">254</td>
<td id="T_31d93_row1_col9" class="data row1 col9">255</td>
<td id="T_31d93_row1_col10" class="data row1 col10">254</td>
<td id="T_31d93_row1_col11" class="data row1 col11">176</td>
<td id="T_31d93_row1_col12" class="data row1 col12">193</td>
<td id="T_31d93_row1_col13" class="data row1 col13">150</td>
<td id="T_31d93_row1_col14" class="data row1 col14">96</td>
<td id="T_31d93_row1_col15" class="data row1 col15">0</td>
<td id="T_31d93_row1_col16" class="data row1 col16">0</td>
<td id="T_31d93_row1_col17" class="data row1 col17">0</td>
</tr>
<tr class="odd">
<td id="T_31d93_level0_row2" class="row_heading level0 row2" data-quarto-table-cell-role="th">2</td>
<td id="T_31d93_row2_col0" class="data row2 col0">0</td>
<td id="T_31d93_row2_col1" class="data row2 col1">0</td>
<td id="T_31d93_row2_col2" class="data row2 col2">0</td>
<td id="T_31d93_row2_col3" class="data row2 col3">48</td>
<td id="T_31d93_row2_col4" class="data row2 col4">166</td>
<td id="T_31d93_row2_col5" class="data row2 col5">224</td>
<td id="T_31d93_row2_col6" class="data row2 col6">253</td>
<td id="T_31d93_row2_col7" class="data row2 col7">253</td>
<td id="T_31d93_row2_col8" class="data row2 col8">234</td>
<td id="T_31d93_row2_col9" class="data row2 col9">196</td>
<td id="T_31d93_row2_col10" class="data row2 col10">253</td>
<td id="T_31d93_row2_col11" class="data row2 col11">253</td>
<td id="T_31d93_row2_col12" class="data row2 col12">253</td>
<td id="T_31d93_row2_col13" class="data row2 col13">253</td>
<td id="T_31d93_row2_col14" class="data row2 col14">233</td>
<td id="T_31d93_row2_col15" class="data row2 col15">0</td>
<td id="T_31d93_row2_col16" class="data row2 col16">0</td>
<td id="T_31d93_row2_col17" class="data row2 col17">0</td>
</tr>
<tr class="even">
<td id="T_31d93_level0_row3" class="row_heading level0 row3" data-quarto-table-cell-role="th">3</td>
<td id="T_31d93_row3_col0" class="data row3 col0">0</td>
<td id="T_31d93_row3_col1" class="data row3 col1">93</td>
<td id="T_31d93_row3_col2" class="data row3 col2">244</td>
<td id="T_31d93_row3_col3" class="data row3 col3">249</td>
<td id="T_31d93_row3_col4" class="data row3 col4">253</td>
<td id="T_31d93_row3_col5" class="data row3 col5">187</td>
<td id="T_31d93_row3_col6" class="data row3 col6">46</td>
<td id="T_31d93_row3_col7" class="data row3 col7">10</td>
<td id="T_31d93_row3_col8" class="data row3 col8">8</td>
<td id="T_31d93_row3_col9" class="data row3 col9">4</td>
<td id="T_31d93_row3_col10" class="data row3 col10">10</td>
<td id="T_31d93_row3_col11" class="data row3 col11">194</td>
<td id="T_31d93_row3_col12" class="data row3 col12">253</td>
<td id="T_31d93_row3_col13" class="data row3 col13">253</td>
<td id="T_31d93_row3_col14" class="data row3 col14">233</td>
<td id="T_31d93_row3_col15" class="data row3 col15">0</td>
<td id="T_31d93_row3_col16" class="data row3 col16">0</td>
<td id="T_31d93_row3_col17" class="data row3 col17">0</td>
</tr>
<tr class="odd">
<td id="T_31d93_level0_row4" class="row_heading level0 row4" data-quarto-table-cell-role="th">4</td>
<td id="T_31d93_row4_col0" class="data row4 col0">0</td>
<td id="T_31d93_row4_col1" class="data row4 col1">107</td>
<td id="T_31d93_row4_col2" class="data row4 col2">253</td>
<td id="T_31d93_row4_col3" class="data row4 col3">253</td>
<td id="T_31d93_row4_col4" class="data row4 col4">230</td>
<td id="T_31d93_row4_col5" class="data row4 col5">48</td>
<td id="T_31d93_row4_col6" class="data row4 col6">0</td>
<td id="T_31d93_row4_col7" class="data row4 col7">0</td>
<td id="T_31d93_row4_col8" class="data row4 col8">0</td>
<td id="T_31d93_row4_col9" class="data row4 col9">0</td>
<td id="T_31d93_row4_col10" class="data row4 col10">0</td>
<td id="T_31d93_row4_col11" class="data row4 col11">192</td>
<td id="T_31d93_row4_col12" class="data row4 col12">253</td>
<td id="T_31d93_row4_col13" class="data row4 col13">253</td>
<td id="T_31d93_row4_col14" class="data row4 col14">156</td>
<td id="T_31d93_row4_col15" class="data row4 col15">0</td>
<td id="T_31d93_row4_col16" class="data row4 col16">0</td>
<td id="T_31d93_row4_col17" class="data row4 col17">0</td>
</tr>
<tr class="even">
<td id="T_31d93_level0_row5" class="row_heading level0 row5" data-quarto-table-cell-role="th">5</td>
<td id="T_31d93_row5_col0" class="data row5 col0">0</td>
<td id="T_31d93_row5_col1" class="data row5 col1">3</td>
<td id="T_31d93_row5_col2" class="data row5 col2">20</td>
<td id="T_31d93_row5_col3" class="data row5 col3">20</td>
<td id="T_31d93_row5_col4" class="data row5 col4">15</td>
<td id="T_31d93_row5_col5" class="data row5 col5">0</td>
<td id="T_31d93_row5_col6" class="data row5 col6">0</td>
<td id="T_31d93_row5_col7" class="data row5 col7">0</td>
<td id="T_31d93_row5_col8" class="data row5 col8">0</td>
<td id="T_31d93_row5_col9" class="data row5 col9">0</td>
<td id="T_31d93_row5_col10" class="data row5 col10">43</td>
<td id="T_31d93_row5_col11" class="data row5 col11">224</td>
<td id="T_31d93_row5_col12" class="data row5 col12">253</td>
<td id="T_31d93_row5_col13" class="data row5 col13">245</td>
<td id="T_31d93_row5_col14" class="data row5 col14">74</td>
<td id="T_31d93_row5_col15" class="data row5 col15">0</td>
<td id="T_31d93_row5_col16" class="data row5 col16">0</td>
<td id="T_31d93_row5_col17" class="data row5 col17">0</td>
</tr>
<tr class="odd">
<td id="T_31d93_level0_row6" class="row_heading level0 row6" data-quarto-table-cell-role="th">6</td>
<td id="T_31d93_row6_col0" class="data row6 col0">0</td>
<td id="T_31d93_row6_col1" class="data row6 col1">0</td>
<td id="T_31d93_row6_col2" class="data row6 col2">0</td>
<td id="T_31d93_row6_col3" class="data row6 col3">0</td>
<td id="T_31d93_row6_col4" class="data row6 col4">0</td>
<td id="T_31d93_row6_col5" class="data row6 col5">0</td>
<td id="T_31d93_row6_col6" class="data row6 col6">0</td>
<td id="T_31d93_row6_col7" class="data row6 col7">0</td>
<td id="T_31d93_row6_col8" class="data row6 col8">0</td>
<td id="T_31d93_row6_col9" class="data row6 col9">0</td>
<td id="T_31d93_row6_col10" class="data row6 col10">249</td>
<td id="T_31d93_row6_col11" class="data row6 col11">253</td>
<td id="T_31d93_row6_col12" class="data row6 col12">245</td>
<td id="T_31d93_row6_col13" class="data row6 col13">126</td>
<td id="T_31d93_row6_col14" class="data row6 col14">0</td>
<td id="T_31d93_row6_col15" class="data row6 col15">0</td>
<td id="T_31d93_row6_col16" class="data row6 col16">0</td>
<td id="T_31d93_row6_col17" class="data row6 col17">0</td>
</tr>
<tr class="even">
<td id="T_31d93_level0_row7" class="row_heading level0 row7" data-quarto-table-cell-role="th">7</td>
<td id="T_31d93_row7_col0" class="data row7 col0">0</td>
<td id="T_31d93_row7_col1" class="data row7 col1">0</td>
<td id="T_31d93_row7_col2" class="data row7 col2">0</td>
<td id="T_31d93_row7_col3" class="data row7 col3">0</td>
<td id="T_31d93_row7_col4" class="data row7 col4">0</td>
<td id="T_31d93_row7_col5" class="data row7 col5">0</td>
<td id="T_31d93_row7_col6" class="data row7 col6">0</td>
<td id="T_31d93_row7_col7" class="data row7 col7">14</td>
<td id="T_31d93_row7_col8" class="data row7 col8">101</td>
<td id="T_31d93_row7_col9" class="data row7 col9">223</td>
<td id="T_31d93_row7_col10" class="data row7 col10">253</td>
<td id="T_31d93_row7_col11" class="data row7 col11">248</td>
<td id="T_31d93_row7_col12" class="data row7 col12">124</td>
<td id="T_31d93_row7_col13" class="data row7 col13">0</td>
<td id="T_31d93_row7_col14" class="data row7 col14">0</td>
<td id="T_31d93_row7_col15" class="data row7 col15">0</td>
<td id="T_31d93_row7_col16" class="data row7 col16">0</td>
<td id="T_31d93_row7_col17" class="data row7 col17">0</td>
</tr>
<tr class="odd">
<td id="T_31d93_level0_row8" class="row_heading level0 row8" data-quarto-table-cell-role="th">8</td>
<td id="T_31d93_row8_col0" class="data row8 col0">0</td>
<td id="T_31d93_row8_col1" class="data row8 col1">0</td>
<td id="T_31d93_row8_col2" class="data row8 col2">0</td>
<td id="T_31d93_row8_col3" class="data row8 col3">0</td>
<td id="T_31d93_row8_col4" class="data row8 col4">0</td>
<td id="T_31d93_row8_col5" class="data row8 col5">11</td>
<td id="T_31d93_row8_col6" class="data row8 col6">166</td>
<td id="T_31d93_row8_col7" class="data row8 col7">239</td>
<td id="T_31d93_row8_col8" class="data row8 col8">253</td>
<td id="T_31d93_row8_col9" class="data row8 col9">253</td>
<td id="T_31d93_row8_col10" class="data row8 col10">253</td>
<td id="T_31d93_row8_col11" class="data row8 col11">187</td>
<td id="T_31d93_row8_col12" class="data row8 col12">30</td>
<td id="T_31d93_row8_col13" class="data row8 col13">0</td>
<td id="T_31d93_row8_col14" class="data row8 col14">0</td>
<td id="T_31d93_row8_col15" class="data row8 col15">0</td>
<td id="T_31d93_row8_col16" class="data row8 col16">0</td>
<td id="T_31d93_row8_col17" class="data row8 col17">0</td>
</tr>
<tr class="even">
<td id="T_31d93_level0_row9" class="row_heading level0 row9" data-quarto-table-cell-role="th">9</td>
<td id="T_31d93_row9_col0" class="data row9 col0">0</td>
<td id="T_31d93_row9_col1" class="data row9 col1">0</td>
<td id="T_31d93_row9_col2" class="data row9 col2">0</td>
<td id="T_31d93_row9_col3" class="data row9 col3">0</td>
<td id="T_31d93_row9_col4" class="data row9 col4">0</td>
<td id="T_31d93_row9_col5" class="data row9 col5">16</td>
<td id="T_31d93_row9_col6" class="data row9 col6">248</td>
<td id="T_31d93_row9_col7" class="data row9 col7">250</td>
<td id="T_31d93_row9_col8" class="data row9 col8">253</td>
<td id="T_31d93_row9_col9" class="data row9 col9">253</td>
<td id="T_31d93_row9_col10" class="data row9 col10">253</td>
<td id="T_31d93_row9_col11" class="data row9 col11">253</td>
<td id="T_31d93_row9_col12" class="data row9 col12">232</td>
<td id="T_31d93_row9_col13" class="data row9 col13">213</td>
<td id="T_31d93_row9_col14" class="data row9 col14">111</td>
<td id="T_31d93_row9_col15" class="data row9 col15">2</td>
<td id="T_31d93_row9_col16" class="data row9 col16">0</td>
<td id="T_31d93_row9_col17" class="data row9 col17">0</td>
</tr>
<tr class="odd">
<td id="T_31d93_level0_row10" class="row_heading level0 row10" data-quarto-table-cell-role="th">10</td>
<td id="T_31d93_row10_col0" class="data row10 col0">0</td>
<td id="T_31d93_row10_col1" class="data row10 col1">0</td>
<td id="T_31d93_row10_col2" class="data row10 col2">0</td>
<td id="T_31d93_row10_col3" class="data row10 col3">0</td>
<td id="T_31d93_row10_col4" class="data row10 col4">0</td>
<td id="T_31d93_row10_col5" class="data row10 col5">0</td>
<td id="T_31d93_row10_col6" class="data row10 col6">0</td>
<td id="T_31d93_row10_col7" class="data row10 col7">43</td>
<td id="T_31d93_row10_col8" class="data row10 col8">98</td>
<td id="T_31d93_row10_col9" class="data row10 col9">98</td>
<td id="T_31d93_row10_col10" class="data row10 col10">208</td>
<td id="T_31d93_row10_col11" class="data row10 col11">253</td>
<td id="T_31d93_row10_col12" class="data row10 col12">253</td>
<td id="T_31d93_row10_col13" class="data row10 col13">253</td>
<td id="T_31d93_row10_col14" class="data row10 col14">253</td>
<td id="T_31d93_row10_col15" class="data row10 col15">187</td>
<td id="T_31d93_row10_col16" class="data row10 col16">22</td>
<td id="T_31d93_row10_col17" class="data row10 col17">0</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Question: How might a system that actually classifies 3s and 7s work? Well now we have a 28x28 grid of pixel values that visualizes a given digit. Perhaps we could create some grid that would represent a ‘perfect’ 3 and 7. A 7 would have a perfectly horizontal top line, maybe in row 2 or 3, that pivoted into a perfect slant as well. A 3 would have a couple of perfectly bulbous curves stacked ontop of one another. The values would be completely dark (with a value of 255) where the number is and 0 elsewhere.</p>
<p>We could then go cell by cell and find take the difference between the values that we’re seeing in the digit we’re trying to classify and our ‘perfect’ example, (and maybe even square those differences so that they’re all positive, and larger differences are given more weight than smaller ones) and then sum them up to get a sense of how much difference there is between any given digit and our standardized example.</p>
<p>Infact this is exactly what we’re going to do. We’ll create our representation of a ‘perfect’ 3 and 7 by averaging the values of each cell in our 28x28g grid for all of our 3s and 7s. We do this by using list comprehension to create a list of 2-D tensors that are all our examples of 3s or 7s. After that, we’ll use the torch.stack method to put this into a tensor itself, along the 0th dimension, which will orient each cell in the 28x28 grid with the corresponding cell on other pictures.</p>
<p>You can imagine this as printing a picture of each 3/7 on a piece of paper, and stacking all those papers ontop of each other. We then use the .mean method to find the mean across the dimension we pass in as a parameter - 0, which means across all of our different examples of 3s/7s. This is like shining a bright light on our stack of papers from above and observing the general shape that appears through the opacity.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>seven_tensors <span class="op">=</span> [tensor(Image.<span class="bu">open</span>(o)) <span class="cf">for</span> o <span class="kw">in</span> sevens]</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>three_tensors <span class="op">=</span> [tensor(Image.<span class="bu">open</span>(o)) <span class="cf">for</span> o <span class="kw">in</span> threes]</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>stacked_sevens <span class="op">=</span> torch.stack(seven_tensors).<span class="bu">float</span>()<span class="op">/</span><span class="dv">255</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>stacked_threes <span class="op">=</span> torch.stack(three_tensors).<span class="bu">float</span>()<span class="op">/</span><span class="dv">255</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>mean3 <span class="op">=</span> stacked_threes.mean(<span class="dv">0</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>show_image(mean3)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Yeah that looks like a pretty average 3.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>mean7 <span class="op">=</span> stacked_sevens.mean(<span class="dv">0</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>show_image(mean7)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>And this looks like a pretty average 7. Good for us that this average 7 is actually perfect for us.</p>
<p>Now we’ll take the difference between the values in each cell/pixel for a given example to find how much it differs from our expectation of what a 3 or 7 should be. If we average these differences then it would offer us an idea of how different our digit is from our idea of a 3 or 7, and thus which one it is more likely to be. But just using the differences won’t be good enough since it might be positive or negative (the digit might be darker than our average for any particular pixel or vice versa).</p>
<p>Therefore if we use the absolute value of differences or square the differences before finding the average (and the taking the square root again to undo the square), it would make all our values positive. This is called the L1 and L2 norm respectively.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>mean7 <span class="op">=</span> stacked_sevens.mean(<span class="dv">0</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>show_image(mean7)<span class="op">;</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>a_3 <span class="op">=</span> stacked_threes[<span class="dv">1</span>]</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>dist_3_abs <span class="op">=</span> (a_3 <span class="op">-</span> mean3).<span class="bu">abs</span>().mean()</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>dist_3_sqr <span class="op">=</span> ((a_3 <span class="op">-</span> mean3)<span class="op">**</span><span class="dv">2</span>).mean().sqrt()</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>dist_3_abs,dist_3_sqr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>(tensor(0.1114), tensor(0.2021))</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Doing this we find the difference (l1 and l2 norm) of our digit to a 3</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>dist_7_abs <span class="op">=</span> (a_3 <span class="op">-</span> mean7).<span class="bu">abs</span>().mean()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>dist_7_sqr <span class="op">=</span> ((a_3 <span class="op">-</span> mean7)<span class="op">**</span><span class="dv">2</span>).mean().sqrt()</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>dist_7_abs,dist_7_sqr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(tensor(0.1586), tensor(0.3021))</code></pre>
</div>
</div>
<p>And the l1 and l2 norm of our digit to a 7. Since the norms are smaller for the 3, the interpretation is that it is less ‘different’ to our idea of a 3, thus more likely to be that digit, which is what we’ll classify it as.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>F.l1_loss(a_3.<span class="bu">float</span>(),mean7), F.mse_loss(a_3,mean7).sqrt()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>(tensor(0.1586), tensor(0.3021))</code></pre>
</div>
</div>
<p>Protip: Do this a lot simpler with the l1_loss and mse_loss functions inside torch.nn.functional. These functions are usually used to optimize the way we generate predictions for a given sample, which we’ll cover soon when going over gradient descent.</p>
<p>In this case, we’re using the loss a little differently, as it is directly informing our prediction. Either way, is our measurement of the average difference between our prediction, and the actual value. In this case, our ‘prediction’ of a 3/7 is the average we’ve calculated, and finding the difference (l1/l2 norm) between this and the digit being classified (our actual value) is the loss, where the category with the smallest loss is the one classify our digit as.</p>
<p>Now lets try and get a sense of how good this idea is. We’ll take a set of our data specifically for the purpose of evaluating the performance of our model (the validation set) and make a bunch of predictions for the validation set. Since we know what the correct classification for the digits in the validation set is, we can just calculate the percentage we classify corectly. We’ll create our validation set here:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>valid_3_tens <span class="op">=</span> torch.stack([tensor(Image.<span class="bu">open</span>(o)) </span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">for</span> o <span class="kw">in</span> (path<span class="op">/</span><span class="st">'valid'</span><span class="op">/</span><span class="st">'3'</span>).ls()])</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>valid_3_tens <span class="op">=</span> valid_3_tens.<span class="bu">float</span>()<span class="op">/</span><span class="dv">255</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>valid_7_tens <span class="op">=</span> torch.stack([tensor(Image.<span class="bu">open</span>(o)) </span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">for</span> o <span class="kw">in</span> (path<span class="op">/</span><span class="st">'valid'</span><span class="op">/</span><span class="st">'7'</span>).ls()])</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>valid_7_tens <span class="op">=</span> valid_7_tens.<span class="bu">float</span>()<span class="op">/</span><span class="dv">255</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>valid_3_tens.shape,valid_7_tens.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))</code></pre>
</div>
</div>
<p>We’ll also create a helper function to calculate the L1 norm of an image as compared to another image. This case, we are passing in the digit image being classified and our average 3. If you have a question about what the (-1,-2) argument in the mean() call is for, then I have just the thing for you. The thing for you being the answer to your question.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mnist_distance(a,b): <span class="cf">return</span> (a<span class="op">-</span>b).<span class="bu">abs</span>().mean((<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">2</span>))</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>mnist_distance(a_3, mean3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>tensor(0.1114)</code></pre>
</div>
</div>
<p>Notice we can pass in an entire tensor of images and get back a tensor of L1 norms. To do this, PyTorch uses ‘broadcasting’. This expands the tensor with a smaller rank to have the same size as the larger one, and then performs the operation on the tensors corresponding elements when they are the same size.</p>
<p>For this application, we have a rank-3 tensor (our list of 1000 or so rank-2 tensors which are our 28x28 images), and a rank-2 tensor, which is our 28x28 average digit. Broadcasting will create 1000 copies (in theory - in practice it doesn’t actually literally allocate memory for 1000 copies) of this rank-2 tensor, and subtract it from each of our 1000 images to be classified. Then we call abs to make all of the values in our rank-3 tensor positive. Now, for each of the 1000 images in our set, we want to find the average difference for a given pixel compared to our ‘average’ 3/7.</p>
<p>This is essentially summing all 784 (28 * 28) pixel difference values and dividing it by 784. PyTorch lets us do this without loops by specifying the axes to take the mean on. By negative indexing with (-1,-2), we are saying take the mean on the last and second to last axes of this tensor, which are our rows and columns of pixels for each training image, leaving us with a rank-1 tensor of size 1000. Compare this to how we created our average image of a 3/7 by calling .mean(axis=0), which took the mean across all our images, leaving a rank-2 tensor of size [28,28], which is almost a complementary operation.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>valid_3_dist <span class="op">=</span> mnist_distance(valid_3_tens, mean3)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>valid_3_dist, valid_3_dist.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>(tensor([0.1634, 0.1145, 0.1363,  ..., 0.1105, 0.1111, 0.1640]),
 torch.Size([1010]))</code></pre>
</div>
</div>
<p>Okay, after that wall of text our next step is a little easier to understand. If our difference (l1 norm) for a 3 is larger than it is for a 7, it is ‘farther away’ from what a 3 should be than a 7. Thus, we will just calculate the l1 norms for 3 &amp; 7, and provide an output based on what is lower.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_3(x): <span class="cf">return</span> mnist_distance(x,mean3) <span class="op">&lt;</span> mnist_distance(x,mean7)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll use broadcasting to apply this method to our validation set tensor. As it did in our mnist_distance(a,b) definition, it will expand the tensor with our average 3 and average 7 to the size of our x argument, which is our validation set, get the difference, absolute value, then average, and then perform an element-wise comparison on the values returned from both calls to mnist_distance()</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>is_3(valid_3_tens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>tensor([True, True, True,  ..., True, True, True])</code></pre>
</div>
</div>
<p>Then we can easily measure our accuracy by calling is_3 on the list of 3s in our validation set, where each of these should values should be True. We can use the mean() method here after converting the elements to float since Trues will convert to 1.0, while False will be 0.</p>
<p>Our accuracy for 7s is then calculated by using the same method on our list of 7s (where any True value pushing the mean above 0 is actually a 7 being misclassified as a 3), and then subtract that from 1.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>accuracy_3s <span class="op">=</span>      is_3(valid_3_tens).<span class="bu">float</span>() .mean()</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>accuracy_7s <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> is_3(valid_7_tens).<span class="bu">float</span>()).mean()</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>accuracy_3s,accuracy_7s,(accuracy_3s<span class="op">+</span>accuracy_7s)<span class="op">/</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>(tensor(0.9168), tensor(0.9854), tensor(0.9511))</code></pre>
</div>
</div>
<p>WOW!!! That was actually a lot. But it didn’t really feel like artificial intelligence or machine learning or whatever fancy buzzword you want to call it now. All those calculations felt pretty straightforward, fixed, and mechanical. How can we put the intelligence in artifical intelligence?? Well you know what they say: you can’t spell ‘Machine LearSGDning’ without SGD!</p>
<p>So the basic loop for machine learning is 1. initialize weights 2. Predict 3. Calculate loss (a measurement of how far away your prediction is from the its actual label - basically how wrong your model is. We want to minimize this) 4. Calculate gradient and modify the weights based on this. The gradient is the rate of change of a function relative to one of the function’s parameters for a given value of that parameter. We apply this to our loss function, relative to the parameters of our model. Basically, how does our loss function change when we increment/decrement one of our model’s weights. If the gradient is positive, it means that increasing that weight will increase the lose (and thus make our model more inaccurate). A negative gradient means that increasing that weight will decrease the loss (and thus make our model more accurate). The inverse is also true, where decreasing a weight with a positive gradient will decrease the loss. As minimizing loss is our goal, we want to subtract this gradient from our weight, so that we move in the opposite direction of what will increase it. The magnitude of the gradient also tells us how large a change in the loss will result from a change in the weight. Therefore, we will make changes to the weight that are relative to the size of the gradient, which will be some fraction of the gradient based on our learning rate. 5. Repeat steps 2 - 4 until we stop (until the accuracy of our model is at a certain point, or after a predetermined amount of iterations)</p>
<p>Now lets apply this all to our digit classification. Instead of having an ‘average’ image of a 3 and 7 that we will compare our images to, lets try and create a mathematical formula that will tell us an image is a 3 or a 7. The quantitative data that we have to work with are the pixel intensity values of the image, so we will work with those. We have 784 pretty obvious numbers to use for any given image, so many we can find a weight for each of those pixels that will return a high value if a number is a 3 or a 7, and something low otherwise. We’ll use gradient descent to automate the creation of this equation.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>train_x <span class="op">=</span> torch.cat([stacked_threes, stacked_sevens]).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>train_y <span class="op">=</span> tensor([<span class="dv">1</span>]<span class="op">*</span><span class="bu">len</span>(threes) <span class="op">+</span> [<span class="dv">0</span>]<span class="op">*</span><span class="bu">len</span>(sevens)).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>train_x.shape,train_y.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>(torch.Size([12396, 784]), torch.Size([12396, 1]))</code></pre>
</div>
</div>
<p>We’ll put our pictures into a rank-2 tensor, with an element for each tensor of pixel values. Since our classification functions’s weights will be a 1-d tensor, we’ll format our picture tensors in 1 dimension as well with shape (784,1) rather than a 2-d one with shape (28,28). We’ll reshape our labels as well, so that it’s also a rank 2 tensor, with an element for each of our images that is a tensor (with just a single value in it) containing our images label. A will will represent that the image is a 3, and a 0 will represent it being a 7.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>dset <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(train_x,train_y))</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>valid_x <span class="op">=</span> torch.cat([valid_3_tens, valid_7_tens]).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>valid_y <span class="op">=</span> tensor([<span class="dv">1</span>]<span class="op">*</span><span class="bu">len</span>(valid_3_tens) <span class="op">+</span> [<span class="dv">0</span>]<span class="op">*</span><span class="bu">len</span>(valid_7_tens)).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>valid_dset <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(valid_x,valid_y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll use the zip method to create a list of corresponding (training data, label) pairs and do the same thing for our validation set</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_params(size, std<span class="op">=</span><span class="fl">1.0</span>): <span class="cf">return</span> (torch.randn(size)<span class="op">*</span>std).requires_grad_()</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> init_params((<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>,<span class="dv">1</span>))</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> init_params(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we’ll write a helper method that will just return a tensor of randomized values in the shape that we give it, which we’ll pass in the shape of one of our pieces of data (a tensor of size [784,1] representing all the pixel values of a given image) to it as an argument. We’ll also add in our bias. You might be familiar with the general linear equation y = mx+b. The b is our bias, and will just be a constant value that we add.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>(train_x[<span class="dv">0</span>]<span class="op">*</span>weights.T).<span class="bu">sum</span>() <span class="op">+</span> bias</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>tensor([-6.2330], grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<p>Now our prediction is given by taking the dot-product of one of our data points, with our weights (making sure to use the transpose of our weights so that their are properly oriented for matrix multiplication) and adding the bias.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear1(xb): <span class="cf">return</span> xb<span class="op">@</span>weights <span class="op">+</span> bias</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> linear1(train_x)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>preds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>tensor([[ -6.2330],
        [-10.6388],
        [-20.8865],
        ...,
        [-15.9176],
        [ -1.6866],
        [-11.3568]], grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<p>This is just the prediciton for one of our data points. In order to make predictions for our set of images, we will matrix multiply the matrix that they’re contained in, with the matrix our weights are contained in.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>corrects <span class="op">=</span> (preds<span class="op">&gt;</span><span class="fl">0.0</span>).<span class="bu">float</span>() <span class="op">==</span> train_y</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>corrects</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>corrects.<span class="bu">float</span>().mean().item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>0.5379961133003235</code></pre>
</div>
</div>
<p>Since our outputs from the matmul are random numbers, and our labels are either 1 or 0, then we’ll translate all of our outputs into a boolean by comparing it to 0.0. Then we’ll convert it to a 1 or 0 by calling .float(). Now, we can compare it to our known labels with “== train_y”. Now by converting these booleans to 1/0 again, and taking the mean of this tensor, it will give us the percentage of our predictions that were correct.</p>
<p>This is good at seeing well our model is doing, but it actually isn’t a good loss function for optimizing the weights in our predictive model. This is because this accuracy function will only change when a prediction that was previously incorrect or correct switches to the other category, which would probably require a large change in our models weights. Thus, the gradient will be 0 most of the time which won’t be of use to us.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mnist_loss(predictions, targets):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.where(targets<span class="op">==</span><span class="dv">1</span>, <span class="dv">1</span><span class="op">-</span>predictions, predictions).mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Instead we’ll define a new loss function for ourselves. The way we’re predicting if a number is a 3 or 7 is if the output of our model is a 0, or higher. We will treat our predictions as probabilities that a certain number is a 3 or 7. If it is higher than .5, the model thinks it is more likely to be a 3 than a 7, and if less than .5, than it thinks it more likely to be a 7. With this, we will take our tensor of probabilities for all of our images, and use the .where() method which will use the first argument as a condition, and create a new tensor of the same size, filing in the values at the corresponding indices with the 2nd argument if the condition is true, otherwise with the 3rd argument (basically b[i] if a[i] else c[i]).</p>
<p>With this, we’ll be able to create a tensor of how accurate our probabilities were. If an image is a 3, then the value populated is 1 - prediction, which should be closer to 0 the higher the prediction, which is how sure we are it is a 3. If it is a 7, then it will also be closer to 0, the lower the prediction (which is equal to unsure we are it is a 3 aka how sure we are it is 7). Basically, if we are right, it is inversley proportional to how confident we were in that prediction, and if we were wrong, it will be directly proportional to how confident we were in that wrong answer.</p>
<p>WAIT! You have be thinking to yourself, probabilities should be between 1 and 0, but the output from our model can be of any size. It may be much greater than 1, or even negative. In order to get the output of our model into a number between 0 and 1, we’ll use something called the sigmoid function in order to resize our number into someting in this range. It’s defined below (but we’ll be using the one in the PyTorch library as it’s better optimized).</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x): <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>torch.exp(<span class="op">-</span>x))</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mnist_loss(predictions, targets):</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> predictions.sigmoid()</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.where(targets<span class="op">==</span><span class="dv">1</span>, <span class="dv">1</span><span class="op">-</span>predictions, predictions).mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll rewrite our loss function to transform the output of our model with the sigmoid function to get it in the range of 0 to 1.</p>
<p>So again, why did we do all of this when we originally had a metric that was literally our accuracy of the model? The accuracy metric was for human understanding, while this loss function is for the machine to learn from. It is easy for us to interpret the percentage of our predictions that are accurate, but it lacked a meaningful gradient because it was flat for much of the values of our models weights. So in the end, these functions have different attributes, and they serve different purposes.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> init_params((<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>,<span class="dv">1</span>))</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>dl <span class="op">=</span> DataLoader(dset, batch_size<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>xb,yb <span class="op">=</span> first(dl)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>xb.shape,yb.shape</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>valid_dl <span class="op">=</span> DataLoader(valid_dset, batch_size<span class="op">=</span><span class="dv">256</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Additionally, in our example we used our gradients to find out how we should adjust our parameters based on how they were performing across the entire dataset. In practice, this becomes very computationally demanding, and so we’ll use a practice called mini-batching in order to select subsets of our data that are small enough as to enable us to train a model efficiently, but large enough so that they’re still representative of our dataset. Choosing an effective mini batch size is an essential and important practice in itself.</p>
<p>Note - we’ll use stochastic gradient descent for the rest of the notebook, because we’re calculating the gradients for the loss and using it for updating our parameters after each data point, which is different from truly mini-batching or batch gradient descent where we calculate the gradient over the entire batch, take the average of those gradients, and that average to update our parameters.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_grad(xb, yb, model):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> model(xb)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> mnist_loss(preds, yb)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="co">#weights.grad.zero_()</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="co">#bias.grad.zero_();</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Putting this all together, we’ll define calc_grad() which will create a prediction for a set of data based on our model, calculate the loss based on our defined loss function of mnist_loss, and calculate the gradient of the loss function.</p>
<p>Note: loss.backward adds the gradients of loss to any stored gradients, so we’ll have to reset the gradients after each call. The _() at the end of the method denotes that the object is modified in place.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_epoch(model, lr, params):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> xb,yb <span class="kw">in</span> dl:</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>        calc_grad(xb, yb, model)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> params:</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>            p.data <span class="op">-=</span> p.grad<span class="op">*</span>lr</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>            p.grad.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now lets put this gradient calculation into our larger epoch (a pass through the data set or all our batches) training step. We’ll calculate the gradient, and for each one of our model’s parameters, update that weight (or the bias constant) based on the calculated gradient for that parameter and our learning rate. Then reset the gradient as discussed above.</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_accuracy(xb, yb):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> xb.sigmoid()</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> (preds<span class="op">&gt;</span><span class="fl">0.5</span>) <span class="op">==</span> yb</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> correct.<span class="bu">float</span>().mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s create a function using the sigmoid to facilitate the process of checking the accuracy of our predictions</p>
<p>Since the sigmoid will transform our output to a value between 0 and 1 then our threshold for where we consider ourselves more confident in a number being a 3 or a 7 is the halfway at 0.5. As the name implies, this will get the accuracy of the particular batch we’re working with.</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> validate_epoch(model):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    accs <span class="op">=</span> [batch_accuracy(model(xb), yb) <span class="cf">for</span> xb,yb <span class="kw">in</span> valid_dl]</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">round</span>(torch.stack(accs).mean().item(), <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we’ll write a helper method to get the accuracy of all of the batches in our validation set contained in our ‘valid_dl’ DataLoader by leveraging a call to batch_accuracy for all of our batches, and then taking the mean of those returned accuracies.</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> weights,bias</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>train_epoch(linear1, lr, params)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>validate_epoch(linear1)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    train_epoch(linear1, lr, params)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(validate_epoch(linear1), end<span class="op">=</span><span class="st">' '</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.8568 0.9095 0.9295 0.9398 0.9466 0.9545 0.9569 0.9628 0.9647 0.9661 0.9671 0.9681 0.9725 0.9725 0.9725 0.973 0.9735 0.974 0.974 0.9749 </code></pre>
</div>
</div>
<p>With all of these parts - the loss function, our gradient calculation method, the epoch training method, and our methods to determine the accuracy of a batch, and the entire validation set, we can train our model, and see how much our model is improving because of it.</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>linear_model <span class="op">=</span> nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>,<span class="dv">1</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>w,b <span class="op">=</span> linear_model.parameters()</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>w.shape,b.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>(torch.Size([1, 784]), torch.Size([1]))</code></pre>
</div>
</div>
<p>We’ll use the Linear module from Pytorch’s nn module to take care of our the functionality we defined in linear1. We’ll set the parameters with the .parameters method</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BasicOptim:</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,params,lr): <span class="va">self</span>.params,<span class="va">self</span>.lr <span class="op">=</span> <span class="bu">list</span>(params),lr</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params: p.data <span class="op">-=</span> p.grad.data <span class="op">*</span> <span class="va">self</span>.lr</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> zero_grad(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params: p.grad <span class="op">=</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s also create a BasicOptim class that will take care of the optimizing functionality that we’ve created methods for, which is the part where we update our model weights using gradient descent.</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> BasicOptim(linear_model.parameters(), lr)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_epoch(model):</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> xb,yb <span class="kw">in</span> dl:</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>        calc_grad(xb, yb, model)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>        opt.step()</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>        opt.zero_grad()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s use the Linear module and our optimizer class to simply the code for training our model. The dl comes from a variable defined earlier in this notebook where we put our dataset into a DataLoader.</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(model, epochs):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>        train_epoch(model)</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(validate_epoch(model), end<span class="op">=</span><span class="st">' '</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And we’ll also create a method that will run this train_epoch() method for a predetermined amount of epochs.</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>linear_model <span class="op">=</span> nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>,<span class="dv">1</span>)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> SGD(linear_model.parameters(), lr)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>train_model(linear_model, <span class="dv">20</span>)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders(dl, valid_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.4932 0.8706 0.8276 0.9101 0.9331 0.9458 0.9551 0.9629 0.9658 0.9673 0.9687 0.9712 0.9741 0.9751 0.9761 0.9761 0.9775 0.978 0.9785 0.9785 </code></pre>
</div>
</div>
<p>But this was actually all a prank because we have more fastai classes that will handle this functionality that we just defined for us. SGD will replace the BasicOptim class, and we are putting our training and validation dataloader into a DataLoaders class, so that we can use the Learner class’s .fit method, which will replace our train_model() method.</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>,<span class="dv">1</span>), opt_func<span class="op">=</span>SGD,</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>                loss_func<span class="op">=</span>mnist_loss, metrics<span class="op">=</span>batch_accuracy)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>learn.fit(<span class="dv">10</span>, lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">batch_accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.636884</td>
<td>0.503483</td>
<td>0.495584</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.525323</td>
<td>0.197635</td>
<td>0.830716</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.193055</td>
<td>0.175610</td>
<td>0.842493</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.084568</td>
<td>0.105104</td>
<td>0.911678</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.044592</td>
<td>0.077108</td>
<td>0.933268</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.028991</td>
<td>0.061910</td>
<td>0.947007</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.022591</td>
<td>0.052414</td>
<td>0.954858</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>7</td>
<td>0.019752</td>
<td>0.046097</td>
<td>0.962218</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>8</td>
<td>0.018318</td>
<td>0.041659</td>
<td>0.966143</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>9</td>
<td>0.017461</td>
<td>0.038389</td>
<td>0.968106</td>
<td>00:00</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>To use this Learner class, we must instanstiate it with our DataLoaders containing the training and validation set, the model (using the Linear class), our optimizing function, which is going to have the parameters of our model passed to it, along with the learning rate when we call .fit, along with our loss function and any metrics.</p>
<p>Then we call .fit with the number of epochs and learning rate as arguments</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_net(xb): </span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> xb<span class="op">@</span>w1 <span class="op">+</span> b1</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.0</span>))</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>w2 <span class="op">+</span> b2</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> init_params((<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>,<span class="dv">30</span>))</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> init_params(<span class="dv">30</span>)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> init_params((<span class="dv">30</span>,<span class="dv">1</span>))</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> init_params(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s put the linear function we just created a model for using gradient descent into an actual neural net. This will be comprised of two linear functions with a ReLU (rectified linear unit) in the middle. This is our nonlinear component that gives this composition even more flexibility in the problems it can solve. The ReLU is simply taking the max of the output, and 0 (turning any negative number we have to 0).</p>
<p>We initalize our first and second set of weights to be of size (784, 30) and (30,1) respectively. In order to have the proper size matrices for multiplication - we need to have the number of rows in the weight matrix be equal to the the number of columns in the input. In this case, we have a column for each of the image’s 784 pixels. The number of columns of our weight matrix will be equal to the amount of outputs we want to feed in as inputs to the next layer - which we will choose to be 30 a little bit arbitrarily. Now, since we only have two layers, the output of this next weight matrix should be a single number that will be used for our prediction, so this second weight matrix will only have 1 column to produce that singular output.</p>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>simple_net <span class="op">=</span> nn.Sequential(</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>,<span class="dv">30</span>),</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">30</span>,<span class="dv">1</span>)</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using PyTorch, we can build out the same simple neural network with this code. The nn.Sequential model will call each of these layers in turn. We have our 2 Linear models that we used previously, sandwiching our ReLU layer.</p>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, simple_net, opt_func<span class="op">=</span>SGD,</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>                loss_func<span class="op">=</span>mnist_loss, metrics<span class="op">=</span>batch_accuracy)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>learn.fit(<span class="dv">40</span>, <span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">batch_accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.316185</td>
<td>0.399123</td>
<td>0.512758</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.146678</td>
<td>0.228968</td>
<td>0.805692</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.080991</td>
<td>0.114859</td>
<td>0.915604</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.053001</td>
<td>0.077410</td>
<td>0.939647</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.040057</td>
<td>0.060447</td>
<td>0.955839</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.033485</td>
<td>0.050950</td>
<td>0.963690</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.029739</td>
<td>0.044957</td>
<td>0.966634</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>7</td>
<td>0.027323</td>
<td>0.040858</td>
<td>0.969578</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>8</td>
<td>0.025593</td>
<td>0.037880</td>
<td>0.970559</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>9</td>
<td>0.024256</td>
<td>0.035617</td>
<td>0.972031</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>10</td>
<td>0.023172</td>
<td>0.033827</td>
<td>0.973013</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>11</td>
<td>0.022266</td>
<td>0.032365</td>
<td>0.973994</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>12</td>
<td>0.021493</td>
<td>0.031136</td>
<td>0.974975</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>13</td>
<td>0.020823</td>
<td>0.030082</td>
<td>0.975957</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>14</td>
<td>0.020234</td>
<td>0.029161</td>
<td>0.975957</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>15</td>
<td>0.019711</td>
<td>0.028346</td>
<td>0.975957</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>16</td>
<td>0.019243</td>
<td>0.027618</td>
<td>0.976938</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>17</td>
<td>0.018820</td>
<td>0.026962</td>
<td>0.978410</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>18</td>
<td>0.018435</td>
<td>0.026369</td>
<td>0.978901</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>19</td>
<td>0.018083</td>
<td>0.025831</td>
<td>0.979392</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>20</td>
<td>0.017759</td>
<td>0.025340</td>
<td>0.979392</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>21</td>
<td>0.017459</td>
<td>0.024891</td>
<td>0.979882</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>22</td>
<td>0.017179</td>
<td>0.024480</td>
<td>0.980373</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>23</td>
<td>0.016918</td>
<td>0.024103</td>
<td>0.980373</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>24</td>
<td>0.016674</td>
<td>0.023755</td>
<td>0.980373</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>25</td>
<td>0.016444</td>
<td>0.023435</td>
<td>0.980373</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>26</td>
<td>0.016227</td>
<td>0.023138</td>
<td>0.980864</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>27</td>
<td>0.016021</td>
<td>0.022864</td>
<td>0.980864</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>28</td>
<td>0.015826</td>
<td>0.022609</td>
<td>0.981354</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>29</td>
<td>0.015641</td>
<td>0.022372</td>
<td>0.981845</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>30</td>
<td>0.015465</td>
<td>0.022152</td>
<td>0.981354</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>31</td>
<td>0.015298</td>
<td>0.021946</td>
<td>0.981354</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>32</td>
<td>0.015138</td>
<td>0.021753</td>
<td>0.981354</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>33</td>
<td>0.014985</td>
<td>0.021573</td>
<td>0.981354</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>34</td>
<td>0.014838</td>
<td>0.021404</td>
<td>0.981845</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>35</td>
<td>0.014698</td>
<td>0.021244</td>
<td>0.981845</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>36</td>
<td>0.014563</td>
<td>0.021094</td>
<td>0.981845</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>37</td>
<td>0.014434</td>
<td>0.020952</td>
<td>0.982336</td>
<td>00:00</td>
</tr>
<tr class="odd">
<td>38</td>
<td>0.014310</td>
<td>0.020818</td>
<td>0.982336</td>
<td>00:00</td>
</tr>
<tr class="even">
<td>39</td>
<td>0.014190</td>
<td>0.020691</td>
<td>0.982826</td>
<td>00:00</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>After putting this new neural network model into our Learner and fitting it for a number of epochs, we can see the performance from our batch_accuracy metric doing better than just our simple single layered linear model.</p>
<p>BRUH. Wow we just basically implemented our first machine learning model and have gone over the foundational aspects that underlie every machine learning model with using some sort of optimization function (usually stochastic gradient descent) with respect to our loss function to tweak and optimize the parameters of our model. We’ve seen that once we have our data, a learning rate, and the architecture of our neural network, it’s actually basically just a formality to implement a deep learning model at that point and can be done in a few lines of code. The real work with deep learning comes before all that, in doing the necessary work to properly prepare all of those components, and determine what optimal hyperparameters are.</p>
<p>Let’s cap off this lesson with a celebration: going over some jargon that will be necessary to continue our understanding of this subject. As you’ve just seen, none of this is actually particularly complicated. And at this point, we have the luxury of having very powerful machines do all of the manual computation for us. Thus, in order for academics and practioners in this field to make themselves feel better about themselves, they introduced a lot of complicated sounding jargon to make what they were talking about more mystical and technical than it really is. Here’s a breakdown:</p>
<p>Activations - The numbers calculated by our layers. The number that we put into the sigmoid to give us our probability that a certain image was a 3 or 7 was an activation.<br>
Parameters - Numbers that we use to calculate these activation values. They are the weights of our model. In this excercise, they were the 784 values that we multiplied by each corresponding pixel value in a given image by, as well as our bias.</p>
<p>Tensors - regularly shaped arrays. They have a rank, which is equal to the number of axes/dimensions they have. For example:</p>
<p>Rank-0 Tensor: Scalar (a single value) Rank-1 Tensor: Vector (a list of scalars) Rank-2 Tensor: Matrix (a list of vectors aka a list of a list of scalars) Thus, a Rank-3 Tensor would be a list of matrices, a rank-4 would be matrix of matrices (wtf trippy) and so on and so forth.</p>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>