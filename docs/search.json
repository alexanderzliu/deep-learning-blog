[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Practical Deep Learning for Coders: Tabular Modeling\n\n\n\n\n\n\n\nDeep Learning\n\n\nTabular Data\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nAlex Liu\n\n\n\n\n\n\n  \n\n\n\n\nReflections: Starting Off Right\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2023\n\n\nAlexander Liu\n\n\n\n\n\n\n  \n\n\n\n\nPractical Deep Learning for Coders: NLP Intro Notes\n\n\n\n\n\n\n\nfast.ai\n\n\nNeural Networks\n\n\nNatural Language Processing\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nAlex Liu\n\n\n\n\n\n\n  \n\n\n\n\nPractical Deep Learning for Coders: Neural Net Foundations w/ MNIST Image Recognition\n\n\n\n\n\n\n\nfast.ai\n\n\nNeural Networks\n\n\nGradient Descent\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nAlex Liu\n\n\n\n\n\n\n  \n\n\n\n\nHugging Face NLP Course Notes: Transform Models Overview\n\n\n\n\n\n\n\nhappy\n\n\nbappy\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nAlex Liu\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nhappy\n\n\nbappy\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nAlex Liu\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog ^_^\n\n\n\n\n\n\n\nBubu\n\n\nDudu\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2023\n\n\nAlexander Liu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fast.ai Lesson 3/index.html",
    "href": "posts/fast.ai Lesson 3/index.html",
    "title": "Practical Deep Learning for Coders: Lesson 3 Notes",
    "section": "",
    "text": "Okay let’s go finally learning about what everyone want’s to know about when learning about AI and Machine Learning: stochastic gradient descent.\nThe first two lesson’s of this course had us familiarize ourselves with what a deep learning application looks like without really getting into any of the theory or inner workings. We used the fastai library that provides a layer of abstraction for the techniques used in fine-tuning and training models (as it should). Now we get into components of machine learning that so many other MOOCs start with right off the bat - loss functions and gradient descent. It was only inevitable.\nAnyways, let’s get on with the notebook. The example we’re working through is an image classification model that can classify any image as a 3 or a 7. Or you could just go on r/truerateme for that.\nAlright let’s import all our standard fastbook and fastai libraries.\n\nimport sys\n!{sys.executable} -m pip install fastbook\n\n%matplotlib inline\n\nimport fastbook\nfastbook.setup_book()\nfrom fastai.vision.all import *\nfrom fastbook import *\nmatplotlib.rc('image', cmap='Greys')\n\nRequirement already satisfied: fastbook in /opt/anaconda3/lib/python3.8/site-packages (0.0.29)\nRequirement already satisfied: fastai&gt;=2.6 in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (2.7.13)\nRequirement already satisfied: datasets in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (2.14.5)\nRequirement already satisfied: ipywidgets&lt;8 in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (7.6.3)\nRequirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (2.25.1)\nRequirement already satisfied: pip in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (21.0.1)\nRequirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (0.1.99)\nRequirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (1.2.4)\nRequirement already satisfied: transformers in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (4.34.0)\nRequirement already satisfied: graphviz in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (0.20.1)\nRequirement already satisfied: packaging in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (20.9)\nRequirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.24.1)\nRequirement already satisfied: spacy&lt;4 in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (3.7.2)\nRequirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (5.4.1)\nRequirement already satisfied: pillow&gt;=9.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (10.1.0)\nRequirement already satisfied: fastcore&lt;1.6,&gt;=1.5.29 in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.5.29)\nRequirement already satisfied: fastdownload&lt;2,&gt;=0.0.5 in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.0.7)\nRequirement already satisfied: torch&lt;2.2,&gt;=1.10 in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (2.1.0)\nRequirement already satisfied: torchvision&gt;=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.16.0)\nRequirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (3.3.4)\nRequirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.6.2)\nRequirement already satisfied: fastprogress&gt;=0.2.4 in /opt/anaconda3/lib/python3.8/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.0.3)\nRequirement already satisfied: ipykernel&gt;=4.5.1 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets&lt;8-&gt;fastbook) (5.3.4)\nRequirement already satisfied: traitlets&gt;=4.3.1 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets&lt;8-&gt;fastbook) (5.0.5)\nRequirement already satisfied: widgetsnbextension~=3.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets&lt;8-&gt;fastbook) (3.5.1)\nRequirement already satisfied: ipython&gt;=4.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets&lt;8-&gt;fastbook) (7.22.0)\nRequirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets&lt;8-&gt;fastbook) (1.0.0)\nRequirement already satisfied: nbformat&gt;=4.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets&lt;8-&gt;fastbook) (5.1.3)\nRequirement already satisfied: tornado&gt;=4.2 in /opt/anaconda3/lib/python3.8/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;8-&gt;fastbook) (6.1)\nRequirement already satisfied: appnope in /opt/anaconda3/lib/python3.8/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;8-&gt;fastbook) (0.1.2)\nRequirement already satisfied: jupyter-client in /opt/anaconda3/lib/python3.8/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;8-&gt;fastbook) (6.1.12)\nRequirement already satisfied: pygments in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (2.8.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (3.0.17)\nRequirement already satisfied: decorator in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (5.0.6)\nRequirement already satisfied: setuptools&gt;=18.5 in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (52.0.0.post20210125)\nRequirement already satisfied: backcall in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.2.0)\nRequirement already satisfied: pexpect&gt;4.3 in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (4.8.0)\nRequirement already satisfied: pickleshare in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.7.5)\nRequirement already satisfied: jedi&gt;=0.16 in /opt/anaconda3/lib/python3.8/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.17.2)\nRequirement already satisfied: parso&lt;0.8.0,&gt;=0.7.0 in /opt/anaconda3/lib/python3.8/site-packages (from jedi&gt;=0.16-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.7.0)\nRequirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /opt/anaconda3/lib/python3.8/site-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets&lt;8-&gt;fastbook) (3.2.0)\nRequirement already satisfied: jupyter-core in /opt/anaconda3/lib/python3.8/site-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets&lt;8-&gt;fastbook) (4.7.1)\nRequirement already satisfied: ipython-genutils in /opt/anaconda3/lib/python3.8/site-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.2.0)\nRequirement already satisfied: pyrsistent&gt;=0.14.0 in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.17.3)\nRequirement already satisfied: six&gt;=1.11.0 in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&lt;8-&gt;fastbook) (1.15.0)\nRequirement already satisfied: attrs&gt;=17.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&lt;8-&gt;fastbook) (20.3.0)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /opt/anaconda3/lib/python3.8/site-packages (from pexpect&gt;4.3-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.2.5)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.8)\nRequirement already satisfied: weasel&lt;0.4.0,&gt;=0.1.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.3.3)\nRequirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (6.4.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.4.2)\nRequirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.5)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.10)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (4.66.1)\nRequirement already satisfied: wasabi&lt;1.2.0,&gt;=0.9.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.1.2)\nRequirement already satisfied: thinc&lt;8.3.0,&gt;=8.1.8 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.2.1)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.10)\nRequirement already satisfied: typer&lt;0.10.0,&gt;=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.9.0)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.11 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.12)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.4.8)\nRequirement already satisfied: numpy&gt;=1.15.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.20.1)\nRequirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.11.3)\nRequirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.3.0)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.9)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging-&gt;fastbook) (2.4.7)\nRequirement already satisfied: annotated-types&gt;=0.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.6.0)\nRequirement already satisfied: typing-extensions&gt;=4.6.1 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (4.8.0)\nRequirement already satisfied: pydantic-core==2.10.1 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.10.1)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests-&gt;fastbook) (2.10)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests-&gt;fastbook) (2020.12.5)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests-&gt;fastbook) (1.26.4)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests-&gt;fastbook) (4.0.0)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /opt/anaconda3/lib/python3.8/site-packages (from thinc&lt;8.3.0,&gt;=8.1.8-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.7.11)\nRequirement already satisfied: confection&lt;1.0.0,&gt;=0.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from thinc&lt;8.3.0,&gt;=8.1.8-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.1.3)\nRequirement already satisfied: filelock in /opt/anaconda3/lib/python3.8/site-packages (from torch&lt;2.2,&gt;=1.10-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.12)\nRequirement already satisfied: sympy in /opt/anaconda3/lib/python3.8/site-packages (from torch&lt;2.2,&gt;=1.10-&gt;fastai&gt;=2.6-&gt;fastbook) (1.8)\nRequirement already satisfied: fsspec in /opt/anaconda3/lib/python3.8/site-packages (from torch&lt;2.2,&gt;=1.10-&gt;fastai&gt;=2.6-&gt;fastbook) (2023.6.0)\nRequirement already satisfied: networkx in /opt/anaconda3/lib/python3.8/site-packages (from torch&lt;2.2,&gt;=1.10-&gt;fastai&gt;=2.6-&gt;fastbook) (2.5)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from typer&lt;0.10.0,&gt;=0.3.0-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (7.1.2)\nRequirement already satisfied: cloudpathlib&lt;0.17.0,&gt;=0.7.0 in /opt/anaconda3/lib/python3.8/site-packages (from weasel&lt;0.4.0,&gt;=0.1.0-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.16.0)\nRequirement already satisfied: notebook&gt;=4.4.1 in /opt/anaconda3/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (6.3.0)\nRequirement already satisfied: prometheus-client in /opt/anaconda3/lib/python3.8/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.10.1)\nRequirement already satisfied: argon2-cffi in /opt/anaconda3/lib/python3.8/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (20.1.0)\nRequirement already satisfied: terminado&gt;=0.8.3 in /opt/anaconda3/lib/python3.8/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.9.4)\nRequirement already satisfied: nbconvert in /opt/anaconda3/lib/python3.8/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (6.0.7)\nRequirement already satisfied: pyzmq&gt;=17 in /opt/anaconda3/lib/python3.8/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (20.0.0)\nRequirement already satisfied: Send2Trash&gt;=1.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (1.5.0)\nRequirement already satisfied: python-dateutil&gt;=2.1 in /opt/anaconda3/lib/python3.8/site-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;8-&gt;fastbook) (2.8.1)\nRequirement already satisfied: cffi&gt;=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from argon2-cffi-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (1.14.5)\nRequirement already satisfied: pycparser in /opt/anaconda3/lib/python3.8/site-packages (from cffi&gt;=1.0.0-&gt;argon2-cffi-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (2.20)\nRequirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.8/site-packages (from datasets-&gt;fastbook) (0.70.15)\nRequirement already satisfied: xxhash in /opt/anaconda3/lib/python3.8/site-packages (from datasets-&gt;fastbook) (3.4.1)\nRequirement already satisfied: dill&lt;0.3.8,&gt;=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from datasets-&gt;fastbook) (0.3.7)\nRequirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.8/site-packages (from datasets-&gt;fastbook) (3.8.4)\nRequirement already satisfied: pyarrow&gt;=8.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from datasets-&gt;fastbook) (13.0.0)\nRequirement already satisfied: huggingface-hub&lt;1.0.0,&gt;=0.14.0 in /opt/anaconda3/lib/python3.8/site-packages (from datasets-&gt;fastbook) (0.17.3)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (6.0.4)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (4.0.2)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.3.1)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.3.3)\nRequirement already satisfied: charset-normalizer&lt;4.0,&gt;=2.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (3.0.1)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.8.2)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.1.1)\nRequirement already satisfied: cycler&gt;=0.10 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (0.10.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (1.3.1)\nRequirement already satisfied: jupyterlab-pygments in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.1.2)\nRequirement already satisfied: entrypoints&gt;=0.2.2 in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.3)\nRequirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.8.4)\nRequirement already satisfied: testpath in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.4.4)\nRequirement already satisfied: nbclient&lt;0.6.0,&gt;=0.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.5.3)\nRequirement already satisfied: bleach in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (3.3.0)\nRequirement already satisfied: pandocfilters&gt;=1.4.1 in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (1.4.3)\nRequirement already satisfied: defusedxml in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.7.1)\nRequirement already satisfied: async-generator in /opt/anaconda3/lib/python3.8/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (1.10)\nRequirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.8/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (1.5.1)\nRequirement already satisfied: webencodings in /opt/anaconda3/lib/python3.8/site-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.5.1)\nRequirement already satisfied: pytz&gt;=2017.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas-&gt;fastbook) (2021.1)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (2.1.0)\nRequirement already satisfied: joblib&gt;=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.1)\nRequirement already satisfied: mpmath&gt;=0.19 in /opt/anaconda3/lib/python3.8/site-packages (from sympy-&gt;torch&lt;2.2,&gt;=1.10-&gt;fastai&gt;=2.6-&gt;fastbook) (1.2.1)\nRequirement already satisfied: safetensors&gt;=0.3.1 in /opt/anaconda3/lib/python3.8/site-packages (from transformers-&gt;fastbook) (0.4.0)\nRequirement already satisfied: tokenizers&lt;0.15,&gt;=0.14 in /opt/anaconda3/lib/python3.8/site-packages (from transformers-&gt;fastbook) (0.14.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.8/site-packages (from transformers-&gt;fastbook) (2021.4.4)\n\n\nThen we’ll download a our sample from MNIST containing images of 3s and 7s, and use the .ls method to put these items into a special fastai list that also displays the amount of items in the list.\n\npath = untar_data(URLs.MNIST_SAMPLE)\nPath.BASE_PATH = path\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\npath.ls()\n\n(#3) [Path('valid'),Path('labels.csv'),Path('train')]\n\n\nWhy don’t we take a look at one of them. Oh yeah that’s a 3 all right.\n\nim3_path = threes[1]\nim3 = Image.open(im3_path)\nim3\n\n\n\n\nThen we’ll turn the image into a 2-d array where the values represent the darkness of pixel. We then put this into put this into a dataframe with some condotional formatting to visualize that pixel’s darkness value…for some reason. It’s cool I guess\n\nim3_t = tensor(im3)\ndf = pd.DataFrame(im3_t[4:15,4:22])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n29\n150\n195\n254\n255\n254\n176\n193\n150\n96\n0\n0\n0\n\n\n2\n0\n0\n0\n48\n166\n224\n253\n253\n234\n196\n253\n253\n253\n253\n233\n0\n0\n0\n\n\n3\n0\n93\n244\n249\n253\n187\n46\n10\n8\n4\n10\n194\n253\n253\n233\n0\n0\n0\n\n\n4\n0\n107\n253\n253\n230\n48\n0\n0\n0\n0\n0\n192\n253\n253\n156\n0\n0\n0\n\n\n5\n0\n3\n20\n20\n15\n0\n0\n0\n0\n0\n43\n224\n253\n245\n74\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n249\n253\n245\n126\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n14\n101\n223\n253\n248\n124\n0\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n0\n0\n11\n166\n239\n253\n253\n253\n187\n30\n0\n0\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n16\n248\n250\n253\n253\n253\n253\n232\n213\n111\n2\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n43\n98\n98\n208\n253\n253\n253\n253\n187\n22\n0\n\n\n\n\n\nQuestion: How might a system that actually classifies 3s and 7s work? Well now we have a 28x28 grid of pixel values that visualizes a given digit. Perhaps we could create some grid that would represent a ‘perfect’ 3 and 7. A 7 would have a perfectly horizontal top line, maybe in row 2 or 3, that pivoted into a perfect slant as well. A 3 would have a couple of perfectly bulbous curves stacked ontop of one another. The values would be completely dark (with a value of 255) where the number is and 0 elsewhere.\nWe could then go cell by cell and find take the difference between the values that we’re seeing in the digit we’re trying to classify and our ‘perfect’ example, (and maybe even square those differences so that they’re all positive, and larger differences are given more weight than smaller ones) and then sum them up to get a sense of how much difference there is between any given digit and our standardized example.\nInfact this is exactly what we’re going to do. We’ll create our representation of a ‘perfect’ 3 and 7 by averaging the values of each cell in our 28x28g grid for all of our 3s and 7s. We do this by using list comprehension to create a list of 2-D tensors that are all our examples of 3s or 7s. After that, we’ll use the torch.stack method to put this into a tensor itself, along the 0th dimension, which will orient each cell in the 28x28 grid with the corresponding cell on other pictures.\nYou can imagine this as printing a picture of each 3/7 on a piece of paper, and stacking all those papers ontop of each other. We then use the .mean method to find the mean across the dimension we pass in as a parameter - 0, which means across all of our different examples of 3s/7s. This is like shining a bright light on our stack of papers from above and observing the general shape that appears through the opacity.\n\nseven_tensors = [tensor(Image.open(o)) for o in sevens]\nthree_tensors = [tensor(Image.open(o)) for o in threes]\nstacked_sevens = torch.stack(seven_tensors).float()/255\nstacked_threes = torch.stack(three_tensors).float()/255\nmean3 = stacked_threes.mean(0)\nshow_image(mean3);\n\n\n\n\nYeah that looks like a pretty average 3.\n\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\n\n\n\n\nAnd this looks like a pretty average 7. Good for us that this average 7 is actually perfect for us.\nNow we’ll take the difference between the values in each cell/pixel for a given example to find how much it differs from our expectation of what a 3 or 7 should be. If we average these differences then it would offer us an idea of how different our digit is from our idea of a 3 or 7, and thus which one it is more likely to be. But just using the differences won’t be good enough since it might be positive or negative (the digit might be darker than our average for any particular pixel or vice versa).\nTherefore if we use the absolute value of differences or square the differences before finding the average (and the taking the square root again to undo the square), it would make all our values positive. This is called the L1 and L2 norm respectively.\n\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\na_3 = stacked_threes[1]\ndist_3_abs = (a_3 - mean3).abs().mean()\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\ndist_3_abs,dist_3_sqr\n\n(tensor(0.1114), tensor(0.2021))\n\n\n\n\n\nDoing this we find the difference (l1 and l2 norm) of our digit to a 3\n\ndist_7_abs = (a_3 - mean7).abs().mean()\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\ndist_7_abs,dist_7_sqr\n\n(tensor(0.1586), tensor(0.3021))\n\n\nAnd the l1 and l2 norm of our digit to a 7. Since the norms are smaller for the 3, the interpretation is that it is less ‘different’ to our idea of a 3, thus more likely to be that digit, which is what we’ll classify it as.\n\nF.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()\n\n(tensor(0.1586), tensor(0.3021))\n\n\nProtip: Do this a lot simpler with the l1_loss and mse_loss functions inside torch.nn.functional. These functions are usually used to optimize the way we generate predictions for a given sample, which we’ll cover soon when going over gradient descent.\nIn this case, we’re using the loss a little differently, as it is directly informing our prediction. Either way, is our measurement of the average difference between our prediction, and the actual value. In this case, our ‘prediction’ of a 3/7 is the average we’ve calculated, and finding the difference (l1/l2 norm) between this and the digit being classified (our actual value) is the loss, where the category with the smallest loss is the one classify our digit as.\nNow lets try and get a sense of how good this idea is. We’ll take a set of our data specifically for the purpose of evaluating the performance of our model (the validation set) and make a bunch of predictions for the validation set. Since we know what the correct classification for the digits in the validation set is, we can just calculate the percentage we classify corectly. We’ll create our validation set here:\n\nvalid_3_tens = torch.stack([tensor(Image.open(o)) \n                            for o in (path/'valid'/'3').ls()])\nvalid_3_tens = valid_3_tens.float()/255\nvalid_7_tens = torch.stack([tensor(Image.open(o)) \n                            for o in (path/'valid'/'7').ls()])\nvalid_7_tens = valid_7_tens.float()/255\nvalid_3_tens.shape,valid_7_tens.shape\n\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n\n\nWe’ll also create a helper function to calculate the L1 norm of an image as compared to another image. This case, we are passing in the digit image being classified and our average 3. If you have a question about what the (-1,-2) argument in the mean() call is for, then I have just the thing for you. The thing for you being the answer to your question.\n\ndef mnist_distance(a,b): return (a-b).abs().mean((-1,-2))\nmnist_distance(a_3, mean3)\n\ntensor(0.1114)\n\n\nNotice we can pass in an entire tensor of images and get back a tensor of L1 norms. To do this, PyTorch uses ‘broadcasting’. This expands the tensor with a smaller rank to have the same size as the larger one, and then performs the operation on the tensors corresponding elements when they are the same size.\nFor this application, we have a rank-3 tensor (our list of 1000 or so rank-2 tensors which are our 28x28 images), and a rank-2 tensor, which is our 28x28 average digit. Broadcasting will create 1000 copies (in theory - in practice it doesn’t actually literally allocate memory for 1000 copies) of this rank-2 tensor, and subtract it from each of our 1000 images to be classified. Then we call abs to make all of the values in our rank-3 tensor positive. Now, for each of the 1000 images in our set, we want to find the average difference for a given pixel compared to our ‘average’ 3/7.\nThis is essentially summing all 784 (28 * 28) pixel difference values and dividing it by 784. PyTorch lets us do this without loops by specifying the axes to take the mean on. By negative indexing with (-1,-2), we are saying take the mean on the last and second to last axes of this tensor, which are our rows and columns of pixels for each training image, leaving us with a rank-1 tensor of size 1000. Compare this to how we created our average image of a 3/7 by calling .mean(axis=0), which took the mean across all our images, leaving a rank-2 tensor of size [28,28], which is almost a complementary operation.\n\nvalid_3_dist = mnist_distance(valid_3_tens, mean3)\nvalid_3_dist, valid_3_dist.shape\n\n(tensor([0.1634, 0.1145, 0.1363,  ..., 0.1105, 0.1111, 0.1640]),\n torch.Size([1010]))\n\n\nOkay, after that wall of text our next step is a little easier to understand. If our difference (l1 norm) for a 3 is larger than it is for a 7, it is ‘farther away’ from what a 3 should be than a 7. Thus, we will just calculate the l1 norms for 3 & 7, and provide an output based on what is lower.\n\ndef is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7)\n\nWe’ll use broadcasting to apply this method to our validation set tensor. As it did in our mnist_distance(a,b) definition, it will expand the tensor with our average 3 and average 7 to the size of our x argument, which is our validation set, get the difference, absolute value, then average, and then perform an element-wise comparison on the values returned from both calls to mnist_distance()\n\nis_3(valid_3_tens)\n\ntensor([True, True, True,  ..., True, True, True])\n\n\nThen we can easily measure our accuracy by calling is_3 on the list of 3s in our validation set, where each of these should values should be True. We can use the mean() method here after converting the elements to float since Trues will convert to 1.0, while False will be 0.\nOur accuracy for 7s is then calculated by using the same method on our list of 7s (where any True value pushing the mean above 0 is actually a 7 being misclassified as a 3), and then subtract that from 1.\n\naccuracy_3s =      is_3(valid_3_tens).float() .mean()\naccuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n\naccuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2\n\n(tensor(0.9168), tensor(0.9854), tensor(0.9511))\n\n\nWOW!!! That was actually a lot. But it didn’t really feel like artificial intelligence or machine learning or whatever fancy buzzword you want to call it now. All those calculations felt pretty straightforward, fixed, and mechanical. How can we put the intelligence in artifical intelligence?? Well you know what they say: you can’t spell ‘Machine LearSGDning’ without SGD!\nSo the basic loop for machine learning is 1. initialize weights 2. Predict 3. Calculate loss (a measurement of how far away your prediction is from the its actual label - basically how wrong your model is. We want to minimize this) 4. Calculate gradient and modify the weights based on this. The gradient is the rate of change of a function relative to one of the function’s parameters for a given value of that parameter. We apply this to our loss function, relative to the parameters of our model. Basically, how does our loss function change when we increment/decrement one of our model’s weights. If the gradient is positive, it means that increasing that weight will increase the lose (and thus make our model more inaccurate). A negative gradient means that increasing that weight will decrease the loss (and thus make our model more accurate). The inverse is also true, where decreasing a weight with a positive gradient will decrease the loss. As minimizing loss is our goal, we want to subtract this gradient from our weight, so that we move in the opposite direction of what will increase it. The magnitude of the gradient also tells us how large a change in the loss will result from a change in the weight. Therefore, we will make changes to the weight that are relative to the size of the gradient, which will be some fraction of the gradient based on our learning rate. 5. Repeat steps 2 - 4 until we stop (until the accuracy of our model is at a certain point, or after a predetermined amount of iterations)\nNow lets apply this all to our digit classification. Instead of having an ‘average’ image of a 3 and 7 that we will compare our images to, lets try and create a mathematical formula that will tell us an image is a 3 or a 7. The quantitative data that we have to work with are the pixel intensity values of the image, so we will work with those. We have 784 pretty obvious numbers to use for any given image, so many we can find a weight for each of those pixels that will return a high value if a number is a 3 or a 7, and something low otherwise. We’ll use gradient descent to automate the creation of this equation.\n\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\ntrain_x.shape,train_y.shape\n\n(torch.Size([12396, 784]), torch.Size([12396, 1]))\n\n\nWe’ll put our pictures into a rank-2 tensor, with an element for each tensor of pixel values. Since our classification functions’s weights will be a 1-d tensor, we’ll format our picture tensors in 1 dimension as well with shape (784,1) rather than a 2-d one with shape (28,28). We’ll reshape our labels as well, so that it’s also a rank 2 tensor, with an element for each of our images that is a tensor (with just a single value in it) containing our images label. A will will represent that the image is a 3, and a 0 will represent it being a 7.\n\ndset = list(zip(train_x,train_y))\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nvalid_dset = list(zip(valid_x,valid_y))\n\nWe’ll use the zip method to create a list of corresponding (training data, label) pairs and do the same thing for our validation set\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\nweights = init_params((28*28,1))\nbias = init_params(1)\n\nNow we’ll write a helper method that will just return a tensor of randomized values in the shape that we give it, which we’ll pass in the shape of one of our pieces of data (a tensor of size [784,1] representing all the pixel values of a given image) to it as an argument. We’ll also add in our bias. You might be familiar with the general linear equation y = mx+b. The b is our bias, and will just be a constant value that we add.\n\n(train_x[0]*weights.T).sum() + bias\n\ntensor([-6.2330], grad_fn=&lt;AddBackward0&gt;)\n\n\nNow our prediction is given by taking the dot-product of one of our data points, with our weights (making sure to use the transpose of our weights so that their are properly oriented for matrix multiplication) and adding the bias.\n\ndef linear1(xb): return xb@weights + bias\npreds = linear1(train_x)\npreds\n\ntensor([[ -6.2330],\n        [-10.6388],\n        [-20.8865],\n        ...,\n        [-15.9176],\n        [ -1.6866],\n        [-11.3568]], grad_fn=&lt;AddBackward0&gt;)\n\n\nThis is just the prediciton for one of our data points. In order to make predictions for our set of images, we will matrix multiply the matrix that they’re contained in, with the matrix our weights are contained in.\n\ncorrects = (preds&gt;0.0).float() == train_y\ncorrects\ncorrects.float().mean().item()\n\n0.5379961133003235\n\n\nSince our outputs from the matmul are random numbers, and our labels are either 1 or 0, then we’ll translate all of our outputs into a boolean by comparing it to 0.0. Then we’ll convert it to a 1 or 0 by calling .float(). Now, we can compare it to our known labels with “== train_y”. Now by converting these booleans to 1/0 again, and taking the mean of this tensor, it will give us the percentage of our predictions that were correct.\nThis is good at seeing well our model is doing, but it actually isn’t a good loss function for optimizing the weights in our predictive model. This is because this accuracy function will only change when a prediction that was previously incorrect or correct switches to the other category, which would probably require a large change in our models weights. Thus, the gradient will be 0 most of the time which won’t be of use to us.\n\ndef mnist_loss(predictions, targets):\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\nInstead we’ll define a new loss function for ourselves. The way we’re predicting if a number is a 3 or 7 is if the output of our model is a 0, or higher. We will treat our predictions as probabilities that a certain number is a 3 or 7. If it is higher than .5, the model thinks it is more likely to be a 3 than a 7, and if less than .5, than it thinks it more likely to be a 7. With this, we will take our tensor of probabilities for all of our images, and use the .where() method which will use the first argument as a condition, and create a new tensor of the same size, filing in the values at the corresponding indices with the 2nd argument if the condition is true, otherwise with the 3rd argument (basically b[i] if a[i] else c[i]).\nWith this, we’ll be able to create a tensor of how accurate our probabilities were. If an image is a 3, then the value populated is 1 - prediction, which should be closer to 0 the higher the prediction, which is how sure we are it is a 3. If it is a 7, then it will also be closer to 0, the lower the prediction (which is equal to unsure we are it is a 3 aka how sure we are it is 7). Basically, if we are right, it is inversley proportional to how confident we were in that prediction, and if we were wrong, it will be directly proportional to how confident we were in that wrong answer.\nWAIT! You have be thinking to yourself, probabilities should be between 1 and 0, but the output from our model can be of any size. It may be much greater than 1, or even negative. In order to get the output of our model into a number between 0 and 1, we’ll use something called the sigmoid function in order to resize our number into someting in this range. It’s defined below (but we’ll be using the one in the PyTorch library as it’s better optimized).\n\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\ndef mnist_loss(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\nWe’ll rewrite our loss function to transform the output of our model with the sigmoid function to get it in the range of 0 to 1.\nSo again, why did we do all of this when we originally had a metric that was literally our accuracy of the model? The accuracy metric was for human understanding, while this loss function is for the machine to learn from. It is easy for us to interpret the percentage of our predictions that are accurate, but it lacked a meaningful gradient because it was flat for much of the values of our models weights. So in the end, these functions have different attributes, and they serve different purposes.\n\nweights = init_params((28*28,1))\nbias = init_params(1)\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape,yb.shape\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\nAdditionally, in our example we used our gradients to find out how we should adjust our parameters based on how they were performing across the entire dataset. In practice, this becomes very computationally demanding, and so we’ll use a practice called mini-batching in order to select subsets of our data that are small enough as to enable us to train a model efficiently, but large enough so that they’re still representative of our dataset. Choosing an effective mini batch size is an essential and important practice in itself.\nNote - we’ll use stochastic gradient descent for the rest of the notebook, because we’re calculating the gradients for the loss and using it for updating our parameters after each data point, which is different from truly mini-batching or batch gradient descent where we calculate the gradient over the entire batch, take the average of those gradients, and that average to update our parameters.\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\n#weights.grad.zero_()\n#bias.grad.zero_();\n\nPutting this all together, we’ll define calc_grad() which will create a prediction for a set of data based on our model, calculate the loss based on our defined loss function of mnist_loss, and calculate the gradient of the loss function.\nNote: loss.backward adds the gradients of loss to any stored gradients, so we’ll have to reset the gradients after each call. The _() at the end of the method denotes that the object is modified in place.\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\nNow lets put this gradient calculation into our larger epoch (a pass through the data set or all our batches) training step. We’ll calculate the gradient, and for each one of our model’s parameters, update that weight (or the bias constant) based on the calculated gradient for that parameter and our learning rate. Then reset the gradient as discussed above.\n\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds&gt;0.5) == yb\n    return correct.float().mean()\n\nNow let’s create a function using the sigmoid to facilitate the process of checking the accuracy of our predictions\nSince the sigmoid will transform our output to a value between 0 and 1 then our threshold for where we consider ourselves more confident in a number being a 3 or a 7 is the halfway at 0.5. As the name implies, this will get the accuracy of the particular batch we’re working with.\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\nNow we’ll write a helper method to get the accuracy of all of the batches in our validation set contained in our ‘valid_dl’ DataLoader by leveraging a call to batch_accuracy for all of our batches, and then taking the mean of those returned accuracies.\n\nlr = 1.\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n\n0.8568 0.9095 0.9295 0.9398 0.9466 0.9545 0.9569 0.9628 0.9647 0.9661 0.9671 0.9681 0.9725 0.9725 0.9725 0.973 0.9735 0.974 0.974 0.9749 \n\n\nWith all of these parts - the loss function, our gradient calculation method, the epoch training method, and our methods to determine the accuracy of a batch, and the entire validation set, we can train our model, and see how much our model is improving because of it.\n\nlinear_model = nn.Linear(28*28,1)\nw,b = linear_model.parameters()\nw.shape,b.shape\n\n(torch.Size([1, 784]), torch.Size([1]))\n\n\nWe’ll use the Linear module from Pytorch’s nn module to take care of our the functionality we defined in linear1. We’ll set the parameters with the .parameters method\n\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\nLet’s also create a BasicOptim class that will take care of the optimizing functionality that we’ve created methods for, which is the part where we update our model weights using gradient descent.\n\nopt = BasicOptim(linear_model.parameters(), lr)\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\nNow let’s use the Linear module and our optimizer class to simply the code for training our model. The dl comes from a variable defined earlier in this notebook where we put our dataset into a DataLoader.\n\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\nAnd we’ll also create a method that will run this train_epoch() method for a predetermined amount of epochs.\n\nlinear_model = nn.Linear(28*28,1)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\ndls = DataLoaders(dl, valid_dl)\n\n0.4932 0.8706 0.8276 0.9101 0.9331 0.9458 0.9551 0.9629 0.9658 0.9673 0.9687 0.9712 0.9741 0.9751 0.9761 0.9761 0.9775 0.978 0.9785 0.9785 \n\n\nBut this was actually all a prank because we have more fastai classes that will handle this functionality that we just defined for us. SGD will replace the BasicOptim class, and we are putting our training and validation dataloader into a DataLoaders class, so that we can use the Learner class’s .fit method, which will replace our train_model() method.\n\nlearn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nlearn.fit(10, lr=lr)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.636884\n0.503483\n0.495584\n00:00\n\n\n1\n0.525323\n0.197635\n0.830716\n00:00\n\n\n2\n0.193055\n0.175610\n0.842493\n00:00\n\n\n3\n0.084568\n0.105104\n0.911678\n00:00\n\n\n4\n0.044592\n0.077108\n0.933268\n00:00\n\n\n5\n0.028991\n0.061910\n0.947007\n00:00\n\n\n6\n0.022591\n0.052414\n0.954858\n00:00\n\n\n7\n0.019752\n0.046097\n0.962218\n00:00\n\n\n8\n0.018318\n0.041659\n0.966143\n00:00\n\n\n9\n0.017461\n0.038389\n0.968106\n00:00\n\n\n\n\n\nTo use this Learner class, we must instanstiate it with our DataLoaders containing the training and validation set, the model (using the Linear class), our optimizing function, which is going to have the parameters of our model passed to it, along with the learning rate when we call .fit, along with our loss function and any metrics.\nThen we call .fit with the number of epochs and learning rate as arguments\n\ndef simple_net(xb): \n    res = xb@w1 + b1\n    res = res.max(tensor(0.0))\n    res = res@w2 + b2\n    return res\n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n\nLet’s put the linear function we just created a model for using gradient descent into an actual neural net. This will be comprised of two linear functions with a ReLU (rectified linear unit) in the middle. This is our nonlinear component that gives this composition even more flexibility in the problems it can solve. The ReLU is simply taking the max of the output, and 0 (turning any negative number we have to 0).\nWe initalize our first and second set of weights to be of size (784, 30) and (30,1) respectively. In order to have the proper size matrices for multiplication - we need to have the number of rows in the weight matrix be equal to the the number of columns in the input. In this case, we have a column for each of the image’s 784 pixels. The number of columns of our weight matrix will be equal to the amount of outputs we want to feed in as inputs to the next layer - which we will choose to be 30 a little bit arbitrarily. Now, since we only have two layers, the output of this next weight matrix should be a single number that will be used for our prediction, so this second weight matrix will only have 1 column to produce that singular output.\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n\nUsing PyTorch, we can build out the same simple neural network with this code. The nn.Sequential model will call each of these layers in turn. We have our 2 Linear models that we used previously, sandwiching our ReLU layer.\n\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nlearn.fit(40, 0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.316185\n0.399123\n0.512758\n00:00\n\n\n1\n0.146678\n0.228968\n0.805692\n00:00\n\n\n2\n0.080991\n0.114859\n0.915604\n00:00\n\n\n3\n0.053001\n0.077410\n0.939647\n00:00\n\n\n4\n0.040057\n0.060447\n0.955839\n00:00\n\n\n5\n0.033485\n0.050950\n0.963690\n00:00\n\n\n6\n0.029739\n0.044957\n0.966634\n00:00\n\n\n7\n0.027323\n0.040858\n0.969578\n00:00\n\n\n8\n0.025593\n0.037880\n0.970559\n00:00\n\n\n9\n0.024256\n0.035617\n0.972031\n00:00\n\n\n10\n0.023172\n0.033827\n0.973013\n00:00\n\n\n11\n0.022266\n0.032365\n0.973994\n00:00\n\n\n12\n0.021493\n0.031136\n0.974975\n00:00\n\n\n13\n0.020823\n0.030082\n0.975957\n00:00\n\n\n14\n0.020234\n0.029161\n0.975957\n00:00\n\n\n15\n0.019711\n0.028346\n0.975957\n00:00\n\n\n16\n0.019243\n0.027618\n0.976938\n00:00\n\n\n17\n0.018820\n0.026962\n0.978410\n00:00\n\n\n18\n0.018435\n0.026369\n0.978901\n00:00\n\n\n19\n0.018083\n0.025831\n0.979392\n00:00\n\n\n20\n0.017759\n0.025340\n0.979392\n00:00\n\n\n21\n0.017459\n0.024891\n0.979882\n00:00\n\n\n22\n0.017179\n0.024480\n0.980373\n00:00\n\n\n23\n0.016918\n0.024103\n0.980373\n00:00\n\n\n24\n0.016674\n0.023755\n0.980373\n00:00\n\n\n25\n0.016444\n0.023435\n0.980373\n00:00\n\n\n26\n0.016227\n0.023138\n0.980864\n00:00\n\n\n27\n0.016021\n0.022864\n0.980864\n00:00\n\n\n28\n0.015826\n0.022609\n0.981354\n00:00\n\n\n29\n0.015641\n0.022372\n0.981845\n00:00\n\n\n30\n0.015465\n0.022152\n0.981354\n00:00\n\n\n31\n0.015298\n0.021946\n0.981354\n00:00\n\n\n32\n0.015138\n0.021753\n0.981354\n00:00\n\n\n33\n0.014985\n0.021573\n0.981354\n00:00\n\n\n34\n0.014838\n0.021404\n0.981845\n00:00\n\n\n35\n0.014698\n0.021244\n0.981845\n00:00\n\n\n36\n0.014563\n0.021094\n0.981845\n00:00\n\n\n37\n0.014434\n0.020952\n0.982336\n00:00\n\n\n38\n0.014310\n0.020818\n0.982336\n00:00\n\n\n39\n0.014190\n0.020691\n0.982826\n00:00\n\n\n\n\n\nAfter putting this new neural network model into our Learner and fitting it for a number of epochs, we can see the performance from our batch_accuracy metric doing better than just our simple single layered linear model.\nBRUH. Wow we just basically implemented our first machine learning model and have gone over the foundational aspects that underlie every machine learning model with using some sort of optimization function (usually stochastic gradient descent) with respect to our loss function to tweak and optimize the parameters of our model. We’ve seen that once we have our data, a learning rate, and the architecture of our neural network, it’s actually basically just a formality to implement a deep learning model at that point and can be done in a few lines of code. The real work with deep learning comes before all that, in doing the necessary work to properly prepare all of those components, and determine what optimal hyperparameters are.\nLet’s cap off this lesson with a celebration: going over some jargon that will be necessary to continue our understanding of this subject. As you’ve just seen, none of this is actually particularly complicated. And at this point, we have the luxury of having very powerful machines do all of the manual computation for us. Thus, in order for academics and practioners in this field to make themselves feel better about themselves, they introduced a lot of complicated sounding jargon to make what they were talking about more mystical and technical than it really is. Here’s a breakdown:\nActivations - The numbers calculated by our layers. The number that we put into the sigmoid to give us our probability that a certain image was a 3 or 7 was an activation.\nParameters - Numbers that we use to calculate these activation values. They are the weights of our model. In this excercise, they were the 784 values that we multiplied by each corresponding pixel value in a given image by, as well as our bias.\nTensors - regularly shaped arrays. They have a rank, which is equal to the number of axes/dimensions they have. For example:\nRank-0 Tensor: Scalar (a single value) Rank-1 Tensor: Vector (a list of scalars) Rank-2 Tensor: Matrix (a list of vectors aka a list of a list of scalars) Thus, a Rank-3 Tensor would be a list of matrices, a rank-4 would be matrix of matrices (wtf trippy) and so on and so forth."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog ^_^",
    "section": "",
    "text": "This is the first post in my Deep Learning blog. Enjoy these pictures of adorable bears in love. How long can the snippet of a blog be."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\nprint([i for i in 'hi bebi'])\nprint('hello bebi')\n\n['h', 'i', ' ', 'b', 'e', 'b', 'i']\nhello bebi"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Reflections 1/index.html",
    "href": "posts/Reflections 1/index.html",
    "title": "Reflections: Starting Off Right",
    "section": "",
    "text": "As I’m creating this blog to document my adventures in learning deep learning I realize that now I have the perfect medium (haha get IT?) to share my thoughts on whatever I want, and there’s literally nothing anybody can do about it. So let me start facting:\nEPIC QUOTE ALERT: “If you don’t have time to do it right, when will you have time to do it over?”\nI am a fan of progression, of move towards a goal. Or at least the illusion of it. Maybe even at the expense of any positive outcomes that would result from actually working towards that goal. I used to play RuneScape, so an arbitrary number ticking upwards towards another completely arbitrary threshold appealed to me. For learning, this manifests as a eagerness to move onto the next lesson after I’ve ‘finished’ going through all the material, but in no way having absorbed it all. I’m well aware of this now, my goal isn’t to watch a bunch of lectures, or scroll through a bunch of code cells, but to actual learn deeply. So this blog is my way of slowing down while going through these lessons (focusing on fast.ai’s Practical Deep Learning for Coders atm)to better digest the material by translating it into my own thoughts and the way that I undertand it. It’ll also guarantee that I have something to show for along the way. This is my way of doing things right.\nI figured it would be a good excercise in communication to write posts that weren’t strictly associated with a particular deep learning lesson. It’s actually a problem for me - I take so long at work writing emails that I know people won’t even look twice at. I think if I just got better at starting and completing a thought through writing in a timely and straightforward manner, I could literally multiply my productive. I’ve rewritten the last sentence like 4 times and you probably didn’t even read it, so this seems like a productive goal to work towards.\nHonestly, I thought it would be a cool to have a kind of humorous iteration of deep learning lessons, but aside from occasionally being mildly sarcastic being funny is way harder than I thought it would be. I’m literally too focused on gradients and layers and loss functions and all this other crap I don’t have the mental capacity to dedicate to wit. Anyway, that’s all the writing I have to bless you with for now. Stay tuned for more profound quotes - like this one:\n“bebi” - my gf"
  },
  {
    "objectID": "posts/HuggingFace NLP Course/index.html",
    "href": "posts/HuggingFace NLP Course/index.html",
    "title": "Hugging Face NLP Course Notes",
    "section": "",
    "text": "Label text with your own provided set of labels without fine tuning the model on any of your own data\nclassifier = pipeline(\"zero-shot-classification\")\nclassifier(\n    \"This is a course about the Transformers library\",\n    candidate_labels=[\"education\", \"politics\", \"business\"],\n)\n\n{'sequence': 'This is a course about the Transformers library',\n 'labels': ['education', 'business', 'politics'],\n 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}"
  },
  {
    "objectID": "posts/HuggingFace NLP Course/index.html#nlp-tasks",
    "href": "posts/HuggingFace NLP Course/index.html#nlp-tasks",
    "title": "Hugging Face NLP Course Notes",
    "section": "",
    "text": "Label text with your own provided set of labels without fine tuning the model on any of your own data\nclassifier = pipeline(\"zero-shot-classification\")\nclassifier(\n    \"This is a course about the Transformers library\",\n    candidate_labels=[\"education\", \"politics\", \"business\"],\n)\n\n{'sequence': 'This is a course about the Transformers library',\n 'labels': ['education', 'business', 'politics'],\n 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}"
  },
  {
    "objectID": "posts/HuggingFace NLP Course 1/index.html",
    "href": "posts/HuggingFace NLP Course 1/index.html",
    "title": "Hugging Face NLP Course Notes: Transform Models Overview",
    "section": "",
    "text": "Label text with your own provided set of labels without fine tuning the model on any of your own data\n```{python}\nclassifier = pipeline(\"zero-shot-classification\")\nclassifier(\n    \"This is a course about the Transformers library\",\n    candidate_labels=[\"education\", \"politics\", \"business\"],\n)\n\n{'sequence': 'This is a course about the Transformers library',\n 'labels': ['education', 'business', 'politics'],\n 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}\n```\n\n\n\nAuto-complete subsequent words based on a provided prompt\n```{python}\ngenerator = pipeline(\"text-generation\")\ngenerator(\"In this course, we will teach you how to\")\n\n[{'generated_text': 'In this course, we will teach you how to understand and use '\n                    'data flow and data interchange when handling user data. We '\n                    'will be working with one or more of the most commonly used '\n                    'data flows — data flows of various types, as seen by the '\n                    'HTTP'}]\n```\n\n\n\nFill in the blanks of a given text\n```{python}\nunmasker = pipeline(\"fill-mask\")\nunmasker(\"This course will teach you all about &lt;mask&gt; models.\", top_k=2)\nCopied\n[{'sequence': 'This course will teach you all about mathematical models.',\n  'score': 0.19619831442832947,\n  'token': 30412,\n  'token_str': ' mathematical'},\n {'sequence': 'This course will teach you all about computational models.',\n  'score': 0.04052725434303284,\n  'token': 38163,\n  'token_str': ' computational'}]\n```\n\n\n\nFind which parts of an input text are certain entities like persons, locations, organizations. Different models can tag inputs with different entity labels, like for parts of speech.\n```{python}\nner = pipeline(\"ner\", grouped_entities=True)\nner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\nCopied\n[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, \n {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, \n {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n]\n```\n\n\n\nAnswer a question based on given context\n```{python}\nquestion_answerer = pipeline(\"question-answering\")\nquestion_answerer(\n    question=\"Where do I work?\",\n    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n)\n{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}\n```\n\n\n\nSummarize a given input - turn it into a smaller portion of text, while still retaining all of the important information\n```{python}\nsummarizer = pipeline(\"summarization\")\nsummarizer(\n    \"\"\"\n    America has changed dramatically during recent years. Not only has the number of \n    graduates in traditional engineering disciplines such as mechanical, civil, \n    electrical, chemical, and aeronautical engineering declined, but in most of \n    the premier American universities engineering curricula now concentrate on \n    and encourage largely the study of engineering science. As a result, there \n    are declining offerings in engineering subjects dealing with infrastructure, \n    the environment, and related issues, and greater concentration on high \n    technology subjects, largely supporting increasingly complex scientific \n    developments. While the latter is important, it should not be at the expense \n    of more traditional engineering.\n\n    Rapidly developing economies such as China and India, as well as other \n    industrial countries in Europe and Asia, continue to encourage and advance \n    the teaching of engineering. Both China and India, respectively, graduate \n    six and eight times as many traditional engineers as does the United States. \n    Other industrial countries at minimum maintain their output, while America \n    suffers an increasingly serious decline in the number of engineering graduates \n    and a lack of well-educated engineers.\n\"\"\"\n)\n[{'summary_text': ' America has changed dramatically during recent years . The '\n                  'number of engineering graduates in the U.S. has declined in '\n                  'traditional engineering disciplines such as mechanical, civil '\n                  ', electrical, chemical, and aeronautical engineering . Rapidly '\n                  'developing economies such as China and India, as well as other '\n                  'industrial countries in Europe and Asia, continue to encourage '\n                  'and advance engineering .'}]\n```\n\n\n\nConvert text from one language into another\n```{python}\ntranslator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\ntranslator(\"Ce cours est produit par Hugging Face.\")\n[{'translation_text': 'This course is produced by Hugging Face.'}]\n```"
  },
  {
    "objectID": "posts/HuggingFace NLP Course 1/index.html#what-nlp-tasks-can-transforms-do",
    "href": "posts/HuggingFace NLP Course 1/index.html#what-nlp-tasks-can-transforms-do",
    "title": "Hugging Face NLP Course Notes: Transform Models Overview",
    "section": "",
    "text": "Label text with your own provided set of labels without fine tuning the model on any of your own data\n```{python}\nclassifier = pipeline(\"zero-shot-classification\")\nclassifier(\n    \"This is a course about the Transformers library\",\n    candidate_labels=[\"education\", \"politics\", \"business\"],\n)\n\n{'sequence': 'This is a course about the Transformers library',\n 'labels': ['education', 'business', 'politics'],\n 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}\n```\n\n\n\nAuto-complete subsequent words based on a provided prompt\n```{python}\ngenerator = pipeline(\"text-generation\")\ngenerator(\"In this course, we will teach you how to\")\n\n[{'generated_text': 'In this course, we will teach you how to understand and use '\n                    'data flow and data interchange when handling user data. We '\n                    'will be working with one or more of the most commonly used '\n                    'data flows — data flows of various types, as seen by the '\n                    'HTTP'}]\n```\n\n\n\nFill in the blanks of a given text\n```{python}\nunmasker = pipeline(\"fill-mask\")\nunmasker(\"This course will teach you all about &lt;mask&gt; models.\", top_k=2)\nCopied\n[{'sequence': 'This course will teach you all about mathematical models.',\n  'score': 0.19619831442832947,\n  'token': 30412,\n  'token_str': ' mathematical'},\n {'sequence': 'This course will teach you all about computational models.',\n  'score': 0.04052725434303284,\n  'token': 38163,\n  'token_str': ' computational'}]\n```\n\n\n\nFind which parts of an input text are certain entities like persons, locations, organizations. Different models can tag inputs with different entity labels, like for parts of speech.\n```{python}\nner = pipeline(\"ner\", grouped_entities=True)\nner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\nCopied\n[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, \n {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, \n {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n]\n```\n\n\n\nAnswer a question based on given context\n```{python}\nquestion_answerer = pipeline(\"question-answering\")\nquestion_answerer(\n    question=\"Where do I work?\",\n    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n)\n{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}\n```\n\n\n\nSummarize a given input - turn it into a smaller portion of text, while still retaining all of the important information\n```{python}\nsummarizer = pipeline(\"summarization\")\nsummarizer(\n    \"\"\"\n    America has changed dramatically during recent years. Not only has the number of \n    graduates in traditional engineering disciplines such as mechanical, civil, \n    electrical, chemical, and aeronautical engineering declined, but in most of \n    the premier American universities engineering curricula now concentrate on \n    and encourage largely the study of engineering science. As a result, there \n    are declining offerings in engineering subjects dealing with infrastructure, \n    the environment, and related issues, and greater concentration on high \n    technology subjects, largely supporting increasingly complex scientific \n    developments. While the latter is important, it should not be at the expense \n    of more traditional engineering.\n\n    Rapidly developing economies such as China and India, as well as other \n    industrial countries in Europe and Asia, continue to encourage and advance \n    the teaching of engineering. Both China and India, respectively, graduate \n    six and eight times as many traditional engineers as does the United States. \n    Other industrial countries at minimum maintain their output, while America \n    suffers an increasingly serious decline in the number of engineering graduates \n    and a lack of well-educated engineers.\n\"\"\"\n)\n[{'summary_text': ' America has changed dramatically during recent years . The '\n                  'number of engineering graduates in the U.S. has declined in '\n                  'traditional engineering disciplines such as mechanical, civil '\n                  ', electrical, chemical, and aeronautical engineering . Rapidly '\n                  'developing economies such as China and India, as well as other '\n                  'industrial countries in Europe and Asia, continue to encourage '\n                  'and advance engineering .'}]\n```\n\n\n\nConvert text from one language into another\n```{python}\ntranslator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\ntranslator(\"Ce cours est produit par Hugging Face.\")\n[{'translation_text': 'This course is produced by Hugging Face.'}]\n```"
  },
  {
    "objectID": "posts/HuggingFace NLP Course 1/index.html#transformers.-how-do-they-work",
    "href": "posts/HuggingFace NLP Course 1/index.html#transformers.-how-do-they-work",
    "title": "Hugging Face NLP Course Notes: Transform Models Overview",
    "section": "Transformers. How do they work?",
    "text": "Transformers. How do they work?\nTransformers are language models that have been trained on a massive amount of raw, unlabeled data through a particular unsupervised learning technique called “self-supervised learning”. This allows the model to build up a general understanding of the language it’s trained on, but it isn’t good at any particular task.\nAt this point, we can call this model a “general, pretrained transformer” (OMG THAT’S WHAT GPT STANDS FOR).\nWe can then use transfer learning to fine-tune our transformer on labeled data (supervised learning).\nThis practice of fine-tuning a pretained model to apply it towards specific use cases is preferable as the process of pre-training, that is starting from scratch and training a language model on a huge corpus text takes a massive amount of time & resources. So much so that the energy required for it has a very non-trivial carbon footprint. So yeah, I guess you could pretrain models yourself. IF YOU HATE THE EARTH."
  },
  {
    "objectID": "posts/HuggingFace NLP Course 1/index.html#very-general-architecture",
    "href": "posts/HuggingFace NLP Course 1/index.html#very-general-architecture",
    "title": "Hugging Face NLP Course Notes: Transform Models Overview",
    "section": "(Very) General Architecture",
    "text": "(Very) General Architecture\nThe Transformer model is comprised of two parts - an encoder and a decoder.\n\nEncoder\nThe encoder takes in the input and encodes it into a numerical representation (it’s features) that the model can understand.\nThese ‘feature vectors’ are sequences of numbers with one sequence for each word in the input. A word’s feature vectors are also influenced by the other words in the input. So in the case of inputs like “the horse” and “the grove”, ‘the’ would be represented as a different sequence of numbers in each case. This is essentially just how the meaning of words are determined by the context in which they are used.\nEncoders are also ‘bi-directional’, which means these feature vectors are influenced by words located before and after it in the text.\nThey’re usually trained through mask-filling tasks, so they build up a capability to understand words given the context of pre- and proceding words.\nTasks that are centered around understanding an input, like classification or named entity recognition can be done with just the encoder.\nThe decoder takes the features - our encoded representation, and possibly other inputs to generate an output.\nUnlike encoders, they are uni-directional. The only have access to the words located either before or after a given word in a sequence. This makes sense since if our task is to generated a new string of words based on a preceding sequence, it would be trivialized if we had knowledge of what the upcoming words actually were. This is more relevant in the training of these models. We don’t want the decoder to get into the habit of generating its predictions for future words, by just peeking ahead in the training data and seeing what those words actually are, since it won’t be able to do this when we actually try and use it to complete a sentence, since there will be nothing to peek ahead at.\nThese models are also auto-regressive, meaning sequential predictions build upon each other, using previous predictions as input. For example, if we ask it to predict the next 4 words in a sentence, it will predict word 1, use word 1 to predict word 2, use words 1 & 2 to predict word 3, and so on.\nThey’re trained through text generation tasks, so they build up the capaibility to predict words in sequence.\nThese are used for generative tasks like text generation.\n\n\nAttention Layers\nTransformers have a key feature called ‘attention layers’ that have enabled their impressive performance. At a very high level, when the model is dealing with the representation a.k.a trying to understand the words in an input, the attention layer tells it to pay attention to other specific words to better understand that particular word.\nAn example would be for translating from English, to a language with gendered nouns like Spanish. To translate “the bike” to Spanish, the model will look at the words ‘the’ and ‘bike’. However, when translating ‘the’, the model will also need to pay attention to the word ‘bike’, because that will tell it if it should be masculine or feminine i.e. determine if it’s ‘el’ or ‘la’.\n\n\nSequence-to-sequence models\nThey can be used in tandem for generative tasks that require understanding of an input, like translation or summarization."
  }
]