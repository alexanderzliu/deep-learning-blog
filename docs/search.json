[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/fast.ai/fast.ai Chapter 4 - MNIST/index.html",
    "href": "posts/fast.ai/fast.ai Chapter 4 - MNIST/index.html",
    "title": "Practical Deep Learning for Coders: Neural Net Foundations w/ MNIST Image Recognition",
    "section": "",
    "text": "Okay let’s go finally learning about what everyone want’s to know about when learning about AI and Machine Learning: stochastic gradient descent.\nThe first two lesson’s of this course had us familiarize ourselves with what a deep learning application looks like without really getting into any of the theory or inner workings. We used the fastai library that provides a layer of abstraction for the techniques used in fine-tuning and training models (as it should). Now we get into components of machine learning that so many other MOOCs start with right off the bat - loss functions and gradient descent. It was only inevitable.\nAnyways, let’s get on with the notebook. The example we’re working through is an image classification model that can classify any image as a 3 or a 7. Or you could just go on r/truerateme for that.\nAlright let’s import all our standard fastbook and fastai libraries.\n\nimport sys\n!{sys.executable} -m pip install fastbook\n\n%matplotlib inline\n\nimport fastbook\nfastbook.setup_book()\nfrom fastai.vision.all import *\nfrom fastbook import *\nmatplotlib.rc('image', cmap='Greys')\n\nRequirement already satisfied: fastbook in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (0.0.29)\nRequirement already satisfied: pip in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastbook) (23.3.1)\nRequirement already satisfied: packaging in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastbook) (23.2)\nRequirement already satisfied: fastai&gt;=2.6 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastbook) (2.7.13)\nRequirement already satisfied: graphviz in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastbook) (0.20.1)\nRequirement already satisfied: pandas in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastbook) (2.1.4)\nRequirement already satisfied: requests in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastbook) (2.31.0)\nRequirement already satisfied: transformers in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastbook) (4.35.2)\nRequirement already satisfied: datasets in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastbook) (2.15.0)\nRequirement already satisfied: ipywidgets&lt;8 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastbook) (7.8.1)\nRequirement already satisfied: sentencepiece in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastbook) (0.1.99)\nRequirement already satisfied: fastdownload&lt;2,&gt;=0.0.5 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.0.7)\nRequirement already satisfied: fastcore&lt;1.6,&gt;=1.5.29 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.5.29)\nRequirement already satisfied: torchvision&gt;=0.11 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.16.1)\nRequirement already satisfied: matplotlib in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastai&gt;=2.6-&gt;fastbook) (3.8.0)\nRequirement already satisfied: pyyaml in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastai&gt;=2.6-&gt;fastbook) (6.0.1)\nRequirement already satisfied: fastprogress&gt;=0.2.4 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.0.3)\nRequirement already satisfied: pillow&gt;=9.0.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastai&gt;=2.6-&gt;fastbook) (10.0.1)\nRequirement already satisfied: scikit-learn in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.2.2)\nRequirement already satisfied: scipy in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.11.4)\nRequirement already satisfied: spacy&lt;4 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastai&gt;=2.6-&gt;fastbook) (3.7.2)\nRequirement already satisfied: torch&lt;2.2,&gt;=1.10 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fastai&gt;=2.6-&gt;fastbook) (2.1.1)\nRequirement already satisfied: comm&gt;=0.1.3 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipywidgets&lt;8-&gt;fastbook) (0.1.4)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipywidgets&lt;8-&gt;fastbook) (0.2.0)\nRequirement already satisfied: traitlets&gt;=4.3.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipywidgets&lt;8-&gt;fastbook) (5.14.0)\nRequirement already satisfied: widgetsnbextension~=3.6.6 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipywidgets&lt;8-&gt;fastbook) (3.6.6)\nRequirement already satisfied: ipython&gt;=4.0.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipywidgets&lt;8-&gt;fastbook) (8.18.1)\nRequirement already satisfied: jupyterlab-widgets&lt;3,&gt;=1.0.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipywidgets&lt;8-&gt;fastbook) (1.1.7)\nRequirement already satisfied: numpy&gt;=1.17 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from datasets-&gt;fastbook) (1.26.2)\nRequirement already satisfied: pyarrow&gt;=8.0.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from datasets-&gt;fastbook) (14.0.1)\nRequirement already satisfied: pyarrow-hotfix in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from datasets-&gt;fastbook) (0.6)\nRequirement already satisfied: dill&lt;0.3.8,&gt;=0.3.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from datasets-&gt;fastbook) (0.3.7)\nRequirement already satisfied: tqdm&gt;=4.62.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from datasets-&gt;fastbook) (4.66.1)\nRequirement already satisfied: xxhash in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from datasets-&gt;fastbook) (3.4.1)\nRequirement already satisfied: multiprocess in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from datasets-&gt;fastbook) (0.70.15)\nRequirement already satisfied: fsspec&lt;=2023.10.0,&gt;=2023.1.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from fsspec[http]&lt;=2023.10.0,&gt;=2023.1.0-&gt;datasets-&gt;fastbook) (2023.10.0)\nRequirement already satisfied: aiohttp in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from datasets-&gt;fastbook) (3.9.1)\nRequirement already satisfied: huggingface-hub&gt;=0.18.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from datasets-&gt;fastbook) (0.19.4)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from requests-&gt;fastbook) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from requests-&gt;fastbook) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from requests-&gt;fastbook) (2.1.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from requests-&gt;fastbook) (2023.11.17)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from pandas-&gt;fastbook) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from pandas-&gt;fastbook) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from pandas-&gt;fastbook) (2023.3)\nRequirement already satisfied: filelock in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from transformers-&gt;fastbook) (3.13.1)\nRequirement already satisfied: regex!=2019.12.17 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from transformers-&gt;fastbook) (2023.10.3)\nRequirement already satisfied: tokenizers&lt;0.19,&gt;=0.14 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from transformers-&gt;fastbook) (0.15.0)\nRequirement already satisfied: safetensors&gt;=0.3.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from transformers-&gt;fastbook) (0.4.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (23.1.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (6.0.4)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.9.4)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.3.1)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from huggingface-hub&gt;=0.18.0-&gt;datasets-&gt;fastbook) (4.8.0)\nRequirement already satisfied: decorator in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (5.1.1)\nRequirement already satisfied: jedi&gt;=0.16 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.1.6)\nRequirement already satisfied: prompt-toolkit&lt;3.1.0,&gt;=3.0.41 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (3.0.41)\nRequirement already satisfied: pygments&gt;=2.4.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (2.17.2)\nRequirement already satisfied: stack-data in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.6.2)\nRequirement already satisfied: pexpect&gt;4.3 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (4.8.0)\nRequirement already satisfied: six&gt;=1.5 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;fastbook) (1.16.0)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.11 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.12)\nRequirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.5)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.10)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.8)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.9)\nRequirement already satisfied: thinc&lt;8.3.0,&gt;=8.1.8 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.2.1)\nRequirement already satisfied: wasabi&lt;1.2.0,&gt;=0.9.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.1.2)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.4.8)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.10)\nRequirement already satisfied: weasel&lt;0.4.0,&gt;=0.1.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.3.4)\nRequirement already satisfied: typer&lt;0.10.0,&gt;=0.3.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.9.0)\nRequirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (6.4.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.5.2)\nRequirement already satisfied: jinja2 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.1.2)\nRequirement already satisfied: setuptools in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (68.0.0)\nRequirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.3.0)\nRequirement already satisfied: sympy in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from torch&lt;2.2,&gt;=1.10-&gt;fastai&gt;=2.6-&gt;fastbook) (1.12)\nRequirement already satisfied: networkx in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from torch&lt;2.2,&gt;=1.10-&gt;fastai&gt;=2.6-&gt;fastbook) (3.2.1)\nRequirement already satisfied: notebook&gt;=4.4.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (7.0.6)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (1.2.0)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (4.25.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (1.4.4)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.9)\nRequirement already satisfied: joblib&gt;=1.1.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (1.2.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (2.2.0)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jedi&gt;=0.16-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.8.3)\nRequirement already satisfied: jupyter-server&lt;3,&gt;=2.4.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (2.12.1)\nRequirement already satisfied: jupyterlab-server&lt;3,&gt;=2.22.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (2.25.2)\nRequirement already satisfied: jupyterlab&lt;5,&gt;=4.0.2 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (4.0.9)\nRequirement already satisfied: notebook-shim&lt;0.3,&gt;=0.2 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.2.3)\nRequirement already satisfied: tornado&gt;=6.2.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (6.3.3)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from pexpect&gt;4.3-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.7.0)\nRequirement already satisfied: wcwidth in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from prompt-toolkit&lt;3.1.0,&gt;=3.0.41-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.2.12)\nRequirement already satisfied: annotated-types&gt;=0.4.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.5 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.14.5)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from thinc&lt;8.3.0,&gt;=8.1.8-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.7.11)\nRequirement already satisfied: confection&lt;1.0.0,&gt;=0.0.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from thinc&lt;8.3.0,&gt;=8.1.8-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.1.4)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from typer&lt;0.10.0,&gt;=0.3.0-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.1.7)\nRequirement already satisfied: cloudpathlib&lt;0.17.0,&gt;=0.7.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from weasel&lt;0.4.0,&gt;=0.1.0-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.16.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jinja2-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.1.3)\nRequirement already satisfied: executing&gt;=1.2.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from stack-data-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (2.0.1)\nRequirement already satisfied: asttokens&gt;=2.1.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from stack-data-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (2.4.1)\nRequirement already satisfied: pure-eval in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from stack-data-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&lt;8-&gt;fastbook) (0.2.2)\nRequirement already satisfied: mpmath&gt;=0.19 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from sympy-&gt;torch&lt;2.2,&gt;=1.10-&gt;fastai&gt;=2.6-&gt;fastbook) (1.3.0)\nRequirement already satisfied: anyio&gt;=3.1.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (4.1.0)\nRequirement already satisfied: argon2-cffi in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (23.1.0)\nRequirement already satisfied: jupyter-client&gt;=7.4.4 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (8.6.0)\nRequirement already satisfied: jupyter-core!=5.0.*,&gt;=4.12 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (5.5.0)\nRequirement already satisfied: jupyter-events&gt;=0.9.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.9.0)\nRequirement already satisfied: jupyter-server-terminals in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.4.4)\nRequirement already satisfied: nbconvert&gt;=6.4.4 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (7.12.0)\nRequirement already satisfied: nbformat&gt;=5.3.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (5.9.2)\nRequirement already satisfied: overrides in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (7.4.0)\nRequirement already satisfied: prometheus-client in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.19.0)\nRequirement already satisfied: pyzmq&gt;=24 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (25.1.0)\nRequirement already satisfied: send2trash&gt;=1.8.2 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (1.8.2)\nRequirement already satisfied: terminado&gt;=0.8.3 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.18.0)\nRequirement already satisfied: websocket-client in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (1.7.0)\nRequirement already satisfied: async-lru&gt;=1.0.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyterlab&lt;5,&gt;=4.0.2-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (2.0.4)\nRequirement already satisfied: ipykernel in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyterlab&lt;5,&gt;=4.0.2-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (6.26.0)\nRequirement already satisfied: jupyter-lsp&gt;=2.0.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyterlab&lt;5,&gt;=4.0.2-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (2.2.1)\nRequirement already satisfied: babel&gt;=2.10 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (2.13.1)\nRequirement already satisfied: json5&gt;=0.9.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.9.14)\nRequirement already satisfied: jsonschema&gt;=4.18.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (4.20.0)\nRequirement already satisfied: sniffio&gt;=1.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from anyio&gt;=3.1.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (1.3.0)\nRequirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jsonschema&gt;=4.18.0-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (2023.11.2)\nRequirement already satisfied: referencing&gt;=0.28.4 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jsonschema&gt;=4.18.0-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.32.0)\nRequirement already satisfied: rpds-py&gt;=0.7.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jsonschema&gt;=4.18.0-&gt;jupyterlab-server&lt;3,&gt;=2.22.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.13.2)\nRequirement already satisfied: platformdirs&gt;=2.5 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-core!=5.0.*,&gt;=4.12-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (4.1.0)\nRequirement already satisfied: python-json-logger&gt;=2.0.4 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (2.0.7)\nRequirement already satisfied: rfc3339-validator in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.1.4)\nRequirement already satisfied: rfc3986-validator&gt;=0.1.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.1.1)\nRequirement already satisfied: beautifulsoup4 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (4.12.2)\nRequirement already satisfied: bleach!=5.0.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (6.1.0)\nRequirement already satisfied: defusedxml in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.7.1)\nRequirement already satisfied: jupyterlab-pygments in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.3.0)\nRequirement already satisfied: mistune&lt;4,&gt;=2.0.3 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (3.0.2)\nRequirement already satisfied: nbclient&gt;=0.5.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.9.0)\nRequirement already satisfied: pandocfilters&gt;=1.4.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (1.5.0)\nRequirement already satisfied: tinycss2 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (1.2.1)\nRequirement already satisfied: fastjsonschema in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from nbformat&gt;=5.3.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (2.19.0)\nRequirement already satisfied: argon2-cffi-bindings in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from argon2-cffi-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (21.2.0)\nRequirement already satisfied: appnope in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipykernel-&gt;jupyterlab&lt;5,&gt;=4.0.2-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.1.3)\nRequirement already satisfied: debugpy&gt;=1.6.5 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipykernel-&gt;jupyterlab&lt;5,&gt;=4.0.2-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (1.6.7)\nRequirement already satisfied: nest-asyncio in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipykernel-&gt;jupyterlab&lt;5,&gt;=4.0.2-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (1.5.8)\nRequirement already satisfied: psutil in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from ipykernel-&gt;jupyterlab&lt;5,&gt;=4.0.2-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (5.9.0)\nRequirement already satisfied: webencodings in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from bleach!=5.0.0-&gt;nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (0.5.1)\nRequirement already satisfied: fqdn in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (1.5.1)\nRequirement already satisfied: isoduration in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (20.11.0)\nRequirement already satisfied: jsonpointer&gt;1.13 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (2.4)\nRequirement already satisfied: uri-template in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (1.3.0)\nRequirement already satisfied: webcolors&gt;=1.11 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (1.13)\nRequirement already satisfied: cffi&gt;=1.0.1 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from argon2-cffi-bindings-&gt;argon2-cffi-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (1.16.0)\nRequirement already satisfied: soupsieve&gt;1.2 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from beautifulsoup4-&gt;nbconvert&gt;=6.4.4-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (2.5)\nRequirement already satisfied: pycparser in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from cffi&gt;=1.0.1-&gt;argon2-cffi-bindings-&gt;argon2-cffi-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (2.21)\nRequirement already satisfied: arrow&gt;=0.15.0 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from isoduration-&gt;jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (1.3.0)\nRequirement already satisfied: types-python-dateutil&gt;=2.8.10 in /Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages (from arrow&gt;=0.15.0-&gt;isoduration-&gt;jsonschema[format-nongpl]&gt;=4.18.0-&gt;jupyter-events&gt;=0.9.0-&gt;jupyter-server&lt;3,&gt;=2.4.0-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.6-&gt;ipywidgets&lt;8-&gt;fastbook) (2.8.19.14)\n\n\nThen we’ll download a our sample from MNIST containing images of 3s and 7s, and use the .ls method to put these items into a special fastai list that also displays the amount of items in the list.\n\npath = untar_data(URLs.MNIST_SAMPLE)\nPath.BASE_PATH = path\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\npath.ls()\n\n(#3) [Path('valid'),Path('labels.csv'),Path('train')]\n\n\nWhy don’t we take a look at one of them. Oh yeah that’s a 3 all right.\n\nim3_path = threes[1]\nim3 = Image.open(im3_path)\nim3\n\n\n\n\nThen we’ll turn the image into a 2-d array where the values represent the darkness of pixel. We then put this into put this into a dataframe with some condotional formatting to visualize that pixel’s darkness value…for some reason. It’s cool I guess\n\nim3_t = tensor(im3)\ndf = pd.DataFrame(im3_t[4:15,4:22])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n29\n150\n195\n254\n255\n254\n176\n193\n150\n96\n0\n0\n0\n\n\n2\n0\n0\n0\n48\n166\n224\n253\n253\n234\n196\n253\n253\n253\n253\n233\n0\n0\n0\n\n\n3\n0\n93\n244\n249\n253\n187\n46\n10\n8\n4\n10\n194\n253\n253\n233\n0\n0\n0\n\n\n4\n0\n107\n253\n253\n230\n48\n0\n0\n0\n0\n0\n192\n253\n253\n156\n0\n0\n0\n\n\n5\n0\n3\n20\n20\n15\n0\n0\n0\n0\n0\n43\n224\n253\n245\n74\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n249\n253\n245\n126\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n14\n101\n223\n253\n248\n124\n0\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n0\n0\n11\n166\n239\n253\n253\n253\n187\n30\n0\n0\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n16\n248\n250\n253\n253\n253\n253\n232\n213\n111\n2\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n43\n98\n98\n208\n253\n253\n253\n253\n187\n22\n0\n\n\n\n\n\nQuestion: How might a system that actually classifies 3s and 7s work? Well now we have a 28x28 grid of pixel values that visualizes a given digit. Perhaps we could create some grid that would represent a ‘perfect’ 3 and 7. A 7 would have a perfectly horizontal top line, maybe in row 2 or 3, that pivoted into a perfect slant as well. A 3 would have a couple of perfectly bulbous curves stacked ontop of one another. The values would be completely dark (with a value of 255) where the number is and 0 elsewhere.\nWe could then go cell by cell and find take the difference between the values that we’re seeing in the digit we’re trying to classify and our ‘perfect’ example, (and maybe even square those differences so that they’re all positive, and larger differences are given more weight than smaller ones) and then sum them up to get a sense of how much difference there is between any given digit and our standardized example.\nInfact this is exactly what we’re going to do. We’ll create our representation of a ‘perfect’ 3 and 7 by averaging the values of each cell in our 28x28g grid for all of our 3s and 7s. We do this by using list comprehension to create a list of 2-D tensors that are all our examples of 3s or 7s. After that, we’ll use the torch.stack method to put this into a tensor itself, along the 0th dimension, which will orient each cell in the 28x28 grid with the corresponding cell on other pictures.\nYou can imagine this as printing a picture of each 3/7 on a piece of paper, and stacking all those papers ontop of each other. We then use the .mean method to find the mean across the dimension we pass in as a parameter - 0, which means across all of our different examples of 3s/7s. This is like shining a bright light on our stack of papers from above and observing the general shape that appears through the opacity.\n\nseven_tensors = [tensor(Image.open(o)) for o in sevens]\nthree_tensors = [tensor(Image.open(o)) for o in threes]\nstacked_sevens = torch.stack(seven_tensors).float()/255\nstacked_threes = torch.stack(three_tensors).float()/255\nmean3 = stacked_threes.mean(0)\nshow_image(mean3);\n\n\n\n\nYeah that looks like a pretty average 3.\n\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\n\n\n\n\nAnd this looks like a pretty average 7. Good for us that this average 7 is actually perfect for us.\nNow we’ll take the difference between the values in each cell/pixel for a given example to find how much it differs from our expectation of what a 3 or 7 should be. If we average these differences then it would offer us an idea of how different our digit is from our idea of a 3 or 7, and thus which one it is more likely to be. But just using the differences won’t be good enough since it might be positive or negative (the digit might be darker than our average for any particular pixel or vice versa).\nTherefore if we use the absolute value of differences or square the differences before finding the average (and the taking the square root again to undo the square), it would make all our values positive. This is called the L1 and L2 norm respectively.\n\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\na_3 = stacked_threes[1]\ndist_3_abs = (a_3 - mean3).abs().mean()\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\ndist_3_abs,dist_3_sqr\n\n(tensor(0.1114), tensor(0.2021))\n\n\n\n\n\nDoing this we find the difference (l1 and l2 norm) of our digit to a 3\n\ndist_7_abs = (a_3 - mean7).abs().mean()\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\ndist_7_abs,dist_7_sqr\n\n(tensor(0.1586), tensor(0.3021))\n\n\nAnd the l1 and l2 norm of our digit to a 7. Since the norms are smaller for the 3, the interpretation is that it is less ‘different’ to our idea of a 3, thus more likely to be that digit, which is what we’ll classify it as.\n\nF.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()\n\n(tensor(0.1586), tensor(0.3021))\n\n\nProtip: Do this a lot simpler with the l1_loss and mse_loss functions inside torch.nn.functional. These functions are usually used to optimize the way we generate predictions for a given sample, which we’ll cover soon when going over gradient descent.\nIn this case, we’re using the loss a little differently, as it is directly informing our prediction. Either way, is our measurement of the average difference between our prediction, and the actual value. In this case, our ‘prediction’ of a 3/7 is the average we’ve calculated, and finding the difference (l1/l2 norm) between this and the digit being classified (our actual value) is the loss, where the category with the smallest loss is the one classify our digit as.\nNow lets try and get a sense of how good this idea is. We’ll take a set of our data specifically for the purpose of evaluating the performance of our model (the validation set) and make a bunch of predictions for the validation set. Since we know what the correct classification for the digits in the validation set is, we can just calculate the percentage we classify corectly. We’ll create our validation set here:\n\nvalid_3_tens = torch.stack([tensor(Image.open(o)) \n                            for o in (path/'valid'/'3').ls()])\nvalid_3_tens = valid_3_tens.float()/255\nvalid_7_tens = torch.stack([tensor(Image.open(o)) \n                            for o in (path/'valid'/'7').ls()])\nvalid_7_tens = valid_7_tens.float()/255\nvalid_3_tens.shape,valid_7_tens.shape\n\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n\n\nWe’ll also create a helper function to calculate the L1 norm of an image as compared to another image. This case, we are passing in the digit image being classified and our average 3. If you have a question about what the (-1,-2) argument in the mean() call is for, then I have just the thing for you. The thing for you being the answer to your question.\n\ndef mnist_distance(a,b): return (a-b).abs().mean((-1,-2))\nmnist_distance(a_3, mean3)\n\ntensor(0.1114)\n\n\nNotice we can pass in an entire tensor of images and get back a tensor of L1 norms. To do this, PyTorch uses ‘broadcasting’. This expands the tensor with a smaller rank to have the same size as the larger one, and then performs the operation on the tensors corresponding elements when they are the same size.\nFor this application, we have a rank-3 tensor (our list of 1000 or so rank-2 tensors which are our 28x28 images), and a rank-2 tensor, which is our 28x28 average digit. Broadcasting will create 1000 copies (in theory - in practice it doesn’t actually literally allocate memory for 1000 copies) of this rank-2 tensor, and subtract it from each of our 1000 images to be classified. Then we call abs to make all of the values in our rank-3 tensor positive. Now, for each of the 1000 images in our set, we want to find the average difference for a given pixel compared to our ‘average’ 3/7.\nThis is essentially summing all 784 (28 * 28) pixel difference values and dividing it by 784. PyTorch lets us do this without loops by specifying the axes to take the mean on. By negative indexing with (-1,-2), we are saying take the mean on the last and second to last axes of this tensor, which are our rows and columns of pixels for each training image, leaving us with a rank-1 tensor of size 1000. Compare this to how we created our average image of a 3/7 by calling .mean(axis=0), which took the mean across all our images, leaving a rank-2 tensor of size [28,28], which is almost a complementary operation.\n\nvalid_3_dist = mnist_distance(valid_3_tens, mean3)\nvalid_3_dist, valid_3_dist.shape\n\n(tensor([0.1634, 0.1145, 0.1363,  ..., 0.1105, 0.1111, 0.1640]),\n torch.Size([1010]))\n\n\nOkay, after that wall of text our next step is a little easier to understand. If our difference (l1 norm) for a 3 is larger than it is for a 7, it is ‘farther away’ from what a 3 should be than a 7. Thus, we will just calculate the l1 norms for 3 & 7, and provide an output based on what is lower.\n\ndef is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7)\n\nWe’ll use broadcasting to apply this method to our validation set tensor. As it did in our mnist_distance(a,b) definition, it will expand the tensor with our average 3 and average 7 to the size of our x argument, which is our validation set, get the difference, absolute value, then average, and then perform an element-wise comparison on the values returned from both calls to mnist_distance()\n\nis_3(valid_3_tens)\n\ntensor([True, True, True,  ..., True, True, True])\n\n\nThen we can easily measure our accuracy by calling is_3 on the list of 3s in our validation set, where each of these should values should be True. We can use the mean() method here after converting the elements to float since Trues will convert to 1.0, while False will be 0.\nOur accuracy for 7s is then calculated by using the same method on our list of 7s (where any True value pushing the mean above 0 is actually a 7 being misclassified as a 3), and then subtract that from 1.\n\naccuracy_3s =      is_3(valid_3_tens).float() .mean()\naccuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n\naccuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2\n\n(tensor(0.9168), tensor(0.9854), tensor(0.9511))\n\n\nWOW!!! That was actually a lot. But it didn’t really feel like artificial intelligence or machine learning or whatever fancy buzzword you want to call it now. All those calculations felt pretty straightforward, fixed, and mechanical. How can we put the intelligence in artifical intelligence?? Well you know what they say: you can’t spell ‘Machine LearSGDning’ without SGD!\nSo the basic loop for machine learning is 1. initialize weights 2. Predict 3. Calculate loss (a measurement of how far away your prediction is from the its actual label - basically how wrong your model is. We want to minimize this) 4. Calculate gradient and modify the weights based on this. The gradient is the rate of change of a function relative to one of the function’s parameters for a given value of that parameter. We apply this to our loss function, relative to the parameters of our model. Basically, how does our loss function change when we increment/decrement one of our model’s weights. If the gradient is positive, it means that increasing that weight will increase the lose (and thus make our model more inaccurate). A negative gradient means that increasing that weight will decrease the loss (and thus make our model more accurate). The inverse is also true, where decreasing a weight with a positive gradient will decrease the loss. As minimizing loss is our goal, we want to subtract this gradient from our weight, so that we move in the opposite direction of what will increase it. The magnitude of the gradient also tells us how large a change in the loss will result from a change in the weight. Therefore, we will make changes to the weight that are relative to the size of the gradient, which will be some fraction of the gradient based on our learning rate. 5. Repeat steps 2 - 4 until we stop (until the accuracy of our model is at a certain point, or after a predetermined amount of iterations)\nNow lets apply this all to our digit classification. Instead of having an ‘average’ image of a 3 and 7 that we will compare our images to, lets try and create a mathematical formula that will tell us an image is a 3 or a 7. The quantitative data that we have to work with are the pixel intensity values of the image, so we will work with those. We have 784 pretty obvious numbers to use for any given image, so many we can find a weight for each of those pixels that will return a high value if a number is a 3 or a 7, and something low otherwise. We’ll use gradient descent to automate the creation of this equation.\n\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\ntrain_x.shape,train_y.shape\n\n(torch.Size([12396, 784]), torch.Size([12396, 1]))\n\n\nWe’ll put our pictures into a rank-2 tensor, with an element for each tensor of pixel values. Since our classification functions’s weights will be a 1-d tensor, we’ll format our picture tensors in 1 dimension as well with shape (784,1) rather than a 2-d one with shape (28,28). We’ll reshape our labels as well, so that it’s also a rank 2 tensor, with an element for each of our images that is a tensor (with just a single value in it) containing our images label. A will will represent that the image is a 3, and a 0 will represent it being a 7.\n\ndset = list(zip(train_x,train_y))\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nvalid_dset = list(zip(valid_x,valid_y))\n\nWe’ll use the zip method to create a list of corresponding (training data, label) pairs and do the same thing for our validation set\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\nweights = init_params((28*28,1))\nbias = init_params(1)\n\nNow we’ll write a helper method that will just return a tensor of randomized values in the shape that we give it, which we’ll pass in the shape of one of our pieces of data (a tensor of size [784,1] representing all the pixel values of a given image) to it as an argument. We’ll also add in our bias. You might be familiar with the general linear equation y = mx+b. The b is our bias, and will just be a constant value that we add.\n\n(train_x[0]*weights.T).sum() + bias\n\ntensor([-6.2330], grad_fn=&lt;AddBackward0&gt;)\n\n\nNow our prediction is given by taking the dot-product of one of our data points, with our weights (making sure to use the transpose of our weights so that their are properly oriented for matrix multiplication) and adding the bias.\n\ndef linear1(xb): return xb@weights + bias\npreds = linear1(train_x)\npreds\n\ntensor([[ -6.2330],\n        [-10.6388],\n        [-20.8865],\n        ...,\n        [-15.9176],\n        [ -1.6866],\n        [-11.3568]], grad_fn=&lt;AddBackward0&gt;)\n\n\nThis is just the prediciton for one of our data points. In order to make predictions for our set of images, we will matrix multiply the matrix that they’re contained in, with the matrix our weights are contained in.\n\ncorrects = (preds&gt;0.0).float() == train_y\ncorrects\ncorrects.float().mean().item()\n\n0.5379961133003235\n\n\nSince our outputs from the matmul are random numbers, and our labels are either 1 or 0, then we’ll translate all of our outputs into a boolean by comparing it to 0.0. Then we’ll convert it to a 1 or 0 by calling .float(). Now, we can compare it to our known labels with “== train_y”. Now by converting these booleans to 1/0 again, and taking the mean of this tensor, it will give us the percentage of our predictions that were correct.\nThis is good at seeing well our model is doing, but it actually isn’t a good loss function for optimizing the weights in our predictive model. This is because this accuracy function will only change when a prediction that was previously incorrect or correct switches to the other category, which would probably require a large change in our models weights. Thus, the gradient will be 0 most of the time which won’t be of use to us.\n\ndef mnist_loss(predictions, targets):\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\nInstead we’ll define a new loss function for ourselves. The way we’re predicting if a number is a 3 or 7 is if the output of our model is a 0, or higher. We will treat our predictions as probabilities that a certain number is a 3 or 7. If it is higher than .5, the model thinks it is more likely to be a 3 than a 7, and if less than .5, than it thinks it more likely to be a 7. With this, we will take our tensor of probabilities for all of our images, and use the .where() method which will use the first argument as a condition, and create a new tensor of the same size, filing in the values at the corresponding indices with the 2nd argument if the condition is true, otherwise with the 3rd argument (basically b[i] if a[i] else c[i]).\nWith this, we’ll be able to create a tensor of how accurate our probabilities were. If an image is a 3, then the value populated is 1 - prediction, which should be closer to 0 the higher the prediction, which is how sure we are it is a 3. If it is a 7, then it will also be closer to 0, the lower the prediction (which is equal to unsure we are it is a 3 aka how sure we are it is 7). Basically, if we are right, it is inversley proportional to how confident we were in that prediction, and if we were wrong, it will be directly proportional to how confident we were in that wrong answer.\nWAIT! You have be thinking to yourself, probabilities should be between 1 and 0, but the output from our model can be of any size. It may be much greater than 1, or even negative. In order to get the output of our model into a number between 0 and 1, we’ll use something called the sigmoid function in order to resize our number into someting in this range. It’s defined below (but we’ll be using the one in the PyTorch library as it’s better optimized).\n\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\ndef mnist_loss(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\nWe’ll rewrite our loss function to transform the output of our model with the sigmoid function to get it in the range of 0 to 1.\nSo again, why did we do all of this when we originally had a metric that was literally our accuracy of the model? The accuracy metric was for human understanding, while this loss function is for the machine to learn from. It is easy for us to interpret the percentage of our predictions that are accurate, but it lacked a meaningful gradient because it was flat for much of the values of our models weights. So in the end, these functions have different attributes, and they serve different purposes.\n\nweights = init_params((28*28,1))\nbias = init_params(1)\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape,yb.shape\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\nAdditionally, in our example we used our gradients to find out how we should adjust our parameters based on how they were performing across the entire dataset. In practice, this becomes very computationally demanding, and so we’ll use a practice called mini-batching in order to select subsets of our data that are small enough as to enable us to train a model efficiently, but large enough so that they’re still representative of our dataset. Choosing an effective mini batch size is an essential and important practice in itself.\nNote - we’ll use stochastic gradient descent for the rest of the notebook, because we’re calculating the gradients for the loss and using it for updating our parameters after each data point, which is different from truly mini-batching or batch gradient descent where we calculate the gradient over the entire batch, take the average of those gradients, and that average to update our parameters.\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\n#weights.grad.zero_()\n#bias.grad.zero_();\n\nPutting this all together, we’ll define calc_grad() which will create a prediction for a set of data based on our model, calculate the loss based on our defined loss function of mnist_loss, and calculate the gradient of the loss function.\nNote: loss.backward adds the gradients of loss to any stored gradients, so we’ll have to reset the gradients after each call. The _() at the end of the method denotes that the object is modified in place.\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\nNow lets put this gradient calculation into our larger epoch (a pass through the data set or all our batches) training step. We’ll calculate the gradient, and for each one of our model’s parameters, update that weight (or the bias constant) based on the calculated gradient for that parameter and our learning rate. Then reset the gradient as discussed above.\n\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds&gt;0.5) == yb\n    return correct.float().mean()\n\nNow let’s create a function using the sigmoid to facilitate the process of checking the accuracy of our predictions\nSince the sigmoid will transform our output to a value between 0 and 1 then our threshold for where we consider ourselves more confident in a number being a 3 or a 7 is the halfway at 0.5. As the name implies, this will get the accuracy of the particular batch we’re working with.\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\nNow we’ll write a helper method to get the accuracy of all of the batches in our validation set contained in our ‘valid_dl’ DataLoader by leveraging a call to batch_accuracy for all of our batches, and then taking the mean of those returned accuracies.\n\nlr = 1.\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n\n0.8568 0.9095 0.9295 0.9398 0.9466 0.9545 0.9569 0.9628 0.9647 0.9661 0.9671 0.9681 0.9725 0.9725 0.9725 0.973 0.9735 0.974 0.974 0.9749 \n\n\nWith all of these parts - the loss function, our gradient calculation method, the epoch training method, and our methods to determine the accuracy of a batch, and the entire validation set, we can train our model, and see how much our model is improving because of it.\n\nlinear_model = nn.Linear(28*28,1)\nw,b = linear_model.parameters()\nw.shape,b.shape\n\n(torch.Size([1, 784]), torch.Size([1]))\n\n\nWe’ll use the Linear module from Pytorch’s nn module to take care of our the functionality we defined in linear1. We’ll set the parameters with the .parameters method\n\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\nLet’s also create a BasicOptim class that will take care of the optimizing functionality that we’ve created methods for, which is the part where we update our model weights using gradient descent.\n\nopt = BasicOptim(linear_model.parameters(), lr)\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\nNow let’s use the Linear module and our optimizer class to simply the code for training our model. The dl comes from a variable defined earlier in this notebook where we put our dataset into a DataLoader.\n\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\nAnd we’ll also create a method that will run this train_epoch() method for a predetermined amount of epochs.\n\nlinear_model = nn.Linear(28*28,1)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\ndls = DataLoaders(dl, valid_dl)\n\n0.4932 0.8706 0.8276 0.9101 0.9331 0.9458 0.9551 0.9629 0.9658 0.9673 0.9687 0.9712 0.9741 0.9751 0.9761 0.9761 0.9775 0.978 0.9785 0.9785 \n\n\nBut this was actually all a prank because we have more fastai classes that will handle this functionality that we just defined for us. SGD will replace the BasicOptim class, and we are putting our training and validation dataloader into a DataLoaders class, so that we can use the Learner class’s .fit method, which will replace our train_model() method.\n\nlearn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nlearn.fit(10, lr=lr)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.636884\n0.503483\n0.495584\n00:00\n\n\n1\n0.525323\n0.197636\n0.830716\n00:00\n\n\n2\n0.193055\n0.175610\n0.842493\n00:00\n\n\n3\n0.084568\n0.105104\n0.911678\n00:00\n\n\n4\n0.044592\n0.077108\n0.933268\n00:00\n\n\n5\n0.028991\n0.061910\n0.947007\n00:00\n\n\n6\n0.022591\n0.052414\n0.954858\n00:00\n\n\n7\n0.019752\n0.046097\n0.962218\n00:00\n\n\n8\n0.018318\n0.041659\n0.966143\n00:00\n\n\n9\n0.017461\n0.038389\n0.968106\n00:00\n\n\n\n\n\nTo use this Learner class, we must instanstiate it with our DataLoaders containing the training and validation set, the model (using the Linear class), our optimizing function, which is going to have the parameters of our model passed to it, along with the learning rate when we call .fit, along with our loss function and any metrics.\nThen we call .fit with the number of epochs and learning rate as arguments\n\ndef simple_net(xb): \n    res = xb@w1 + b1\n    res = res.max(tensor(0.0))\n    res = res@w2 + b2\n    return res\n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n\nLet’s put the linear function we just created a model for using gradient descent into an actual neural net. This will be comprised of two linear functions with a ReLU (rectified linear unit) in the middle. This is our nonlinear component that gives this composition even more flexibility in the problems it can solve. The ReLU is simply taking the max of the output, and 0 (turning any negative number we have to 0).\nWe initalize our first and second set of weights to be of size (784, 30) and (30,1) respectively. In order to have the proper size matrices for multiplication - we need to have the number of rows in the weight matrix be equal to the the number of columns in the input. In this case, we have a column for each of the image’s 784 pixels. The number of columns of our weight matrix will be equal to the amount of outputs we want to feed in as inputs to the next layer - which we will choose to be 30 a little bit arbitrarily. Now, since we only have two layers, the output of this next weight matrix should be a single number that will be used for our prediction, so this second weight matrix will only have 1 column to produce that singular output.\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n\nUsing PyTorch, we can build out the same simple neural network with this code. The nn.Sequential model will call each of these layers in turn. We have our 2 Linear models that we used previously, sandwiching our ReLU layer.\n\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nlearn.fit(40, 0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.316185\n0.399123\n0.512758\n00:00\n\n\n1\n0.146678\n0.228968\n0.805692\n00:00\n\n\n2\n0.080991\n0.114859\n0.915604\n00:00\n\n\n3\n0.053001\n0.077410\n0.939647\n00:00\n\n\n4\n0.040057\n0.060447\n0.955839\n00:00\n\n\n5\n0.033485\n0.050950\n0.963690\n00:00\n\n\n6\n0.029739\n0.044957\n0.966634\n00:00\n\n\n7\n0.027323\n0.040858\n0.969578\n00:00\n\n\n8\n0.025593\n0.037880\n0.970559\n00:00\n\n\n9\n0.024256\n0.035617\n0.972031\n00:00\n\n\n10\n0.023172\n0.033827\n0.973013\n00:00\n\n\n11\n0.022266\n0.032365\n0.973994\n00:00\n\n\n12\n0.021493\n0.031136\n0.974975\n00:00\n\n\n13\n0.020823\n0.030082\n0.975957\n00:00\n\n\n14\n0.020234\n0.029161\n0.975957\n00:00\n\n\n15\n0.019711\n0.028346\n0.975957\n00:00\n\n\n16\n0.019243\n0.027618\n0.976938\n00:00\n\n\n17\n0.018820\n0.026962\n0.978410\n00:00\n\n\n18\n0.018435\n0.026369\n0.978901\n00:00\n\n\n19\n0.018083\n0.025831\n0.979392\n00:00\n\n\n20\n0.017759\n0.025340\n0.979392\n00:00\n\n\n21\n0.017459\n0.024891\n0.979882\n00:00\n\n\n22\n0.017179\n0.024480\n0.980373\n00:00\n\n\n23\n0.016918\n0.024103\n0.980373\n00:00\n\n\n24\n0.016674\n0.023755\n0.980373\n00:00\n\n\n25\n0.016444\n0.023435\n0.980373\n00:00\n\n\n26\n0.016227\n0.023138\n0.980864\n00:00\n\n\n27\n0.016021\n0.022864\n0.980864\n00:00\n\n\n28\n0.015826\n0.022609\n0.981354\n00:00\n\n\n29\n0.015641\n0.022372\n0.981845\n00:00\n\n\n30\n0.015465\n0.022152\n0.981354\n00:00\n\n\n31\n0.015298\n0.021946\n0.981354\n00:00\n\n\n32\n0.015138\n0.021753\n0.981354\n00:00\n\n\n33\n0.014985\n0.021573\n0.981354\n00:00\n\n\n34\n0.014838\n0.021404\n0.981845\n00:00\n\n\n35\n0.014698\n0.021244\n0.981845\n00:00\n\n\n36\n0.014563\n0.021094\n0.981845\n00:00\n\n\n37\n0.014434\n0.020952\n0.982336\n00:00\n\n\n38\n0.014310\n0.020818\n0.982336\n00:00\n\n\n39\n0.014190\n0.020691\n0.982826\n00:00\n\n\n\n\n\nAfter putting this new neural network model into our Learner and fitting it for a number of epochs, we can see the performance from our batch_accuracy metric doing better than just our simple single layered linear model.\nBRUH. Wow we just basically implemented our first machine learning model and have gone over the foundational aspects that underlie every machine learning model with using some sort of optimization function (usually stochastic gradient descent) with respect to our loss function to tweak and optimize the parameters of our model. We’ve seen that once we have our data, a learning rate, and the architecture of our neural network, it’s actually basically just a formality to implement a deep learning model at that point and can be done in a few lines of code. The real work with deep learning comes before all that, in doing the necessary work to properly prepare all of those components, and determine what optimal hyperparameters are.\nLet’s cap off this lesson with a celebration: going over some jargon that will be necessary to continue our understanding of this subject. As you’ve just seen, none of this is actually particularly complicated. And at this point, we have the luxury of having very powerful machines do all of the manual computation for us. Thus, in order for academics and practioners in this field to make themselves feel better about themselves, they introduced a lot of complicated sounding jargon to make what they were talking about more mystical and technical than it really is. Here’s a breakdown:\nActivations - The numbers calculated by our layers. The number that we put into the sigmoid to give us our probability that a certain image was a 3 or 7 was an activation.\nParameters - Numbers that we use to calculate these activation values. They are the weights of our model. In this excercise, they were the 784 values that we multiplied by each corresponding pixel value in a given image by, as well as our bias.\nTensors - regularly shaped arrays. They have a rank, which is equal to the number of axes/dimensions they have. For example:\nRank-0 Tensor: Scalar (a single value) Rank-1 Tensor: Vector (a list of scalars) Rank-2 Tensor: Matrix (a list of vectors aka a list of a list of scalars) Thus, a Rank-3 Tensor would be a list of matrices, a rank-4 would be matrix of matrices (wtf trippy) and so on and so forth."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to a collection of machine learning excercises and explanations, compiled through my self-study of the subject. If you are reading this you’re a potential employer or my girlfriend. Either way, your support is appreciated."
  },
  {
    "objectID": "posts/Hugging Face NLP/HuggingFace NLP 2/index.html",
    "href": "posts/Hugging Face NLP/HuggingFace NLP 2/index.html",
    "title": "Hugging Face NLP II: Tokenization & Batching",
    "section": "",
    "text": "import torch\nfrom torch import Tensor as tensor\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\nfrom transformers import pipeline\n\n\nclassifier = pipeline(\"sentiment-analysis\")\nclassifier(\n    [\n        \"I've been waiting for a HuggingFace course my whole life.\",\n        \"I hate this so much!\",\n    ]\n)\n\nNo model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'multiprocessing.resource_tracker'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n {'label': 'NEGATIVE', 'score': 0.9994558691978455}]\nWe’ll need to convert our input text to something our transformer model can use. To do this we’ll use a tokenizer.\nThe ‘tokenizer’ will tokenize words, that is split them up into chunks like words, subwords or punctuation. Then it’ll map each of these tokens to an integer, and add some additional some into the integerized, tokenized string, like an start and stop indicator.\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nraw_inputs = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"I hate this so much!\",\n]\ninputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\nprint(inputs)\n\n{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n          2607,  2026,  2878,  2166,  1012,   102],\n        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\nWe can see the list of integers that represent the tokens that comprise our inputs (one tensor of integers for each sentence).\nfrom transformers import AutoModel\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModel.from_pretrained(checkpoint)\n\noutputs = model(**inputs)\nprint(outputs.last_hidden_state.shape)\nOur output has three dimensions 1. Batch size - the number of sequences processed at a time 2. Sequence length - Length of the numerical representation of the sequence i.e. how many tokens our input ends up being 3. Hidden size - The vector dimension of each input token. Basically, we’ll use a vector with elements equal to our hidden size to represent our tokens numerically, which will allow the model to do mathematical operations on it, thus ‘work’ with natural language.\nfrom transformers import AutoModelForSequenceClassification\nimport torch\n\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\noutputs = model(**inputs)\npredictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\nprint(predictions)\nUsing a specialized model for sequence classificaiton, give us an 2x2 output, one for each sequence, and for each of our sentiment categories (positive and negative).\nWe’ll need to put the logits (raw, unnormalized output of last layer) through a softmax layer to turn them into a probability of our input falling into the output categories we have, but after we do that, we can see the chance of our input having negative or positive sentiment.\nfrom transformers import BertConfig, BertModel\n\n# Building the config\nconfig = BertConfig()\n\n# Building the model from the config\nmodel = BertModel(config)\nWe can use a Configuration object which contains a set of predetermined hyperparameters to instantiate our Model object. After that it will be initatilized with random weights.\nAt this point, we would train our model. This would require a non-trivial amount of resources, so we can instead use a pretrained model with the from_pretrained() method, passing in our checkpoint of choice.\nfrom transformers import BertModel\n\nmodel = BertModel.from_pretrained(\"bert-base-cased\")\n\nmodel.save_pretrained(\"directory_on_my_computer\")\nWe’ll save the model, passing in the directory we want to save it in. This will put two files in this directory, a .json, with the model’s architecture (the hyperparameters it’s configured with). We’ll also get a .bin that contains the weights of the model."
  },
  {
    "objectID": "posts/Hugging Face NLP/HuggingFace NLP 2/index.html#tokenizers",
    "href": "posts/Hugging Face NLP/HuggingFace NLP 2/index.html#tokenizers",
    "title": "Hugging Face NLP II: Tokenization & Batching",
    "section": "Tokenizers",
    "text": "Tokenizers\nWe need to convert our input string to numbers for the model to understand it. This is done by converting our input into tokens, which are partitions of our input that we have a few ways of implementing. These tokens each have a numerical ID, which will each be represented by our ‘hidden size’-sized vector for our model to ingest.\n3 of the most popular approaches to tokenization are word, subword, and character based.\nWord - each word is itself a token. The input is split on either spaces or punctuation. Since each word is a token, our vocabulary (the total set of tokens we have IDs for) must be large to account for all of the different words that might be in our input. Since each of these will then be represented by a vector that might be hundreds of numbers long, the model’s representation of an input could becoming overwhelmingly large quickly. Additionally, we’ll have an “UNKNOWN” token ID, assigned to every word that isn’t in our vocabulary. So we might also lose a lot of information if our vocabulary isn’t sufficiently comprehensive.\nCharacter - All characters (like letters) in a language become a token. Usually results in a smaller necessary vocabulary (although character-based languages like Chinese can still have large vocabularies). Issues arise since characters hold less semantic information than words. Additionally, this approach translates to longer token ID strings compared to word-based tokenization, introducing limitations to the length of sequences that can be processed.\nSubword - A balance between character and word based tokenization. Complex words can be split into meaningful components as necessary, like how the -s or -es suffix indicates plurality in English, while common words can be efficiently represented with a single token ID. Because words are split up, we also have a special token indicating the start of new words.\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n#tokenizer.save_pretrained(\"directory_on_my_computer\")\n\n\n\n\n\n\n\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'multiprocessing.resource_tracker'\n\n\n\n\n\n\n\n\nUsing tokenizers is essentially the same as using other pretrained models. We’ll load the one we want based on chosen checkpoint. This will load the algorithm (character/subword/word) used by the tokenizer, and it’s vocabulary (the dictionary of tokens and their asscociated IDs)\nSaving a tokenizer is also identical to models.\n\ntokenizer(\"Using a Transformer network is simple\")\n\nWe see the IDs of the tokens in our input string (we’ll talk about the other outputs - token_type_ids/attention_mask later).\nThis translation from text to numbers is called ‘encoding’ and consists of 2 parts:\n\nTokenization - Splitting up our input string into tokens based on our tokenization algorithm\nConversion - Taking those tokens and creating a sequence of those token’s IDs as defined by our vocabulary.\n\nAs a note: To use a model, we’ll need to tokenize any input string in the same way that was used in the initial training of that model.\n\ndecoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n\nThis process can also be reversed through decoding, translating our token ID sequence back to the string it was derived from."
  },
  {
    "objectID": "posts/Hugging Face NLP/HuggingFace NLP 2/index.html#handling-longer-inputs-with-batching",
    "href": "posts/Hugging Face NLP/HuggingFace NLP 2/index.html#handling-longer-inputs-with-batching",
    "title": "Hugging Face NLP II: Tokenization & Batching",
    "section": "Handling Longer Inputs with Batching",
    "text": "Handling Longer Inputs with Batching\nOur models expect a tensor of IDs as input. Since tensors are rectangular, we need to find a way to account for different lengthed sequences.\nWe’ll do this by padding. We pad the end of the shorter sequence with a token ID that we can find using tokenizer.pad_token_id until it’s the same length as the longest sequence.\nThis will disrupt our attention mechanism, since these padding tokens are improperly contributing to the context in which the other words are interpreted by the model.\nTo solve this problem, we’ll pass in an attention_mask argument during our inference. This is another tensor of the same shape as our input sequence, comprising of 1s and 0s. Token IDs that should be attended to by our model will have a 1 in the corresponding spot in our attention mask. Token IDs that should be ignored have a 0 (all our padding tokens will have a 0).\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n\nseq1= \"I've been waiting for a HuggingFace course my whole life.\"\nseq2= \"I hate this so much!\"\n\n#seqs = [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\",]\n\n#ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(seqs))\n\nids1 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(seq1))\nids2 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(seq2))\n\npad_id = tokenizer.pad_token_id\n\nbatched_ids = [\n[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],\n[1045, 5223, 2023, 2061, 2172, 999, pad_id, pad_id, pad_id, pad_id, pad_id, pad_id, pad_id, pad_id]\n]\n\nmask1 = [1 if i != 0 else 0 for i in batched_ids[0]]\nmask2 = [1 if i != 0 else 0 for i in batched_ids[1]]\n\nattention_mask = [mask1, mask2]\n\noutputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\noutput1 = model(torch.tensor([ids1]))\noutput2 = model(torch.tensor([ids2]))\nprint(f'W/ batching and attention maskign:{outputs}')\nprint(f'Passing in those sequences separately: {output1} and {output2}')\n#ids\n\nW/ batching and attention maskign:SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789],\n        [ 3.1931, -2.6685]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\nPassing in those sequences separately: SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None) and SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1931, -2.6685]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\nWe can see that the output of our model is identical when we properly pad and use the attention mask.\nWe might run into another problem where are sequences are too long for the model. In this case we either have to switch to a model that accepts longer input sequences or truncate (cut off) our sequence.\n\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\ntokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\noutput = model(**tokens)\n\nWe can just call the tokenizer directly on our input sequence by passing it in as a argument to our model, along with some other parameters that handle behavior related to truncation, padding, and the type of tensor returned."
  },
  {
    "objectID": "posts/Hugging Face NLP/HuggingFace NLP Course 1/index1.html",
    "href": "posts/Hugging Face NLP/HuggingFace NLP Course 1/index1.html",
    "title": "Hugging Face NLP I: Transformer Architecture Overview",
    "section": "",
    "text": "Label text with your own provided set of labels without fine tuning the model on any of your own data\n```{python}\nclassifier = pipeline(\"zero-shot-classification\")\nclassifier(\n    \"This is a course about the Transformers library\",\n    candidate_labels=[\"education\", \"politics\", \"business\"],\n)\n\n{'sequence': 'This is a course about the Transformers library',\n 'labels': ['education', 'business', 'politics'],\n 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}\n```\n\n\n\nAuto-complete subsequent words based on a provided prompt\n```{python}\ngenerator = pipeline(\"text-generation\")\ngenerator(\"In this course, we will teach you how to\")\n\n[{'generated_text': 'In this course, we will teach you how to understand and use '\n                    'data flow and data interchange when handling user data. We '\n                    'will be working with one or more of the most commonly used '\n                    'data flows — data flows of various types, as seen by the '\n                    'HTTP'}]\n```\n\n\n\nFill in the blanks of a given text\n```{python}\nunmasker = pipeline(\"fill-mask\")\nunmasker(\"This course will teach you all about &lt;mask&gt; models.\", top_k=2)\nCopied\n[{'sequence': 'This course will teach you all about mathematical models.',\n  'score': 0.19619831442832947,\n  'token': 30412,\n  'token_str': ' mathematical'},\n {'sequence': 'This course will teach you all about computational models.',\n  'score': 0.04052725434303284,\n  'token': 38163,\n  'token_str': ' computational'}]\n```\n\n\n\nFind which parts of an input text are certain entities like persons, locations, organizations. Different models can tag inputs with different entity labels, like for parts of speech.\n```{python}\nner = pipeline(\"ner\", grouped_entities=True)\nner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\nCopied\n[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, \n {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, \n {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n]\n```\n\n\n\nAnswer a question based on given context\n```{python}\nquestion_answerer = pipeline(\"question-answering\")\nquestion_answerer(\n    question=\"Where do I work?\",\n    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n)\n{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}\n```\n\n\n\nSummarize a given input - turn it into a smaller portion of text, while still retaining all of the important information\n```{python}\nsummarizer = pipeline(\"summarization\")\nsummarizer(\n    \"\"\"\n    America has changed dramatically during recent years. Not only has the number of \n    graduates in traditional engineering disciplines such as mechanical, civil, \n    electrical, chemical, and aeronautical engineering declined, but in most of \n    the premier American universities engineering curricula now concentrate on \n    and encourage largely the study of engineering science. As a result, there \n    are declining offerings in engineering subjects dealing with infrastructure, \n    the environment, and related issues, and greater concentration on high \n    technology subjects, largely supporting increasingly complex scientific \n    developments. While the latter is important, it should not be at the expense \n    of more traditional engineering.\n\n    Rapidly developing economies such as China and India, as well as other \n    industrial countries in Europe and Asia, continue to encourage and advance \n    the teaching of engineering. Both China and India, respectively, graduate \n    six and eight times as many traditional engineers as does the United States. \n    Other industrial countries at minimum maintain their output, while America \n    suffers an increasingly serious decline in the number of engineering graduates \n    and a lack of well-educated engineers.\n\"\"\"\n)\n[{'summary_text': ' America has changed dramatically during recent years . The '\n                  'number of engineering graduates in the U.S. has declined in '\n                  'traditional engineering disciplines such as mechanical, civil '\n                  ', electrical, chemical, and aeronautical engineering . Rapidly '\n                  'developing economies such as China and India, as well as other '\n                  'industrial countries in Europe and Asia, continue to encourage '\n                  'and advance engineering .'}]\n```\n\n\n\nConvert text from one language into another\n```{python}\ntranslator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\ntranslator(\"Ce cours est produit par Hugging Face.\")\n[{'translation_text': 'This course is produced by Hugging Face.'}]\n```"
  },
  {
    "objectID": "posts/Hugging Face NLP/HuggingFace NLP Course 1/index1.html#what-nlp-tasks-can-transforms-do",
    "href": "posts/Hugging Face NLP/HuggingFace NLP Course 1/index1.html#what-nlp-tasks-can-transforms-do",
    "title": "Hugging Face NLP I: Transformer Architecture Overview",
    "section": "",
    "text": "Label text with your own provided set of labels without fine tuning the model on any of your own data\n```{python}\nclassifier = pipeline(\"zero-shot-classification\")\nclassifier(\n    \"This is a course about the Transformers library\",\n    candidate_labels=[\"education\", \"politics\", \"business\"],\n)\n\n{'sequence': 'This is a course about the Transformers library',\n 'labels': ['education', 'business', 'politics'],\n 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}\n```\n\n\n\nAuto-complete subsequent words based on a provided prompt\n```{python}\ngenerator = pipeline(\"text-generation\")\ngenerator(\"In this course, we will teach you how to\")\n\n[{'generated_text': 'In this course, we will teach you how to understand and use '\n                    'data flow and data interchange when handling user data. We '\n                    'will be working with one or more of the most commonly used '\n                    'data flows — data flows of various types, as seen by the '\n                    'HTTP'}]\n```\n\n\n\nFill in the blanks of a given text\n```{python}\nunmasker = pipeline(\"fill-mask\")\nunmasker(\"This course will teach you all about &lt;mask&gt; models.\", top_k=2)\nCopied\n[{'sequence': 'This course will teach you all about mathematical models.',\n  'score': 0.19619831442832947,\n  'token': 30412,\n  'token_str': ' mathematical'},\n {'sequence': 'This course will teach you all about computational models.',\n  'score': 0.04052725434303284,\n  'token': 38163,\n  'token_str': ' computational'}]\n```\n\n\n\nFind which parts of an input text are certain entities like persons, locations, organizations. Different models can tag inputs with different entity labels, like for parts of speech.\n```{python}\nner = pipeline(\"ner\", grouped_entities=True)\nner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\nCopied\n[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, \n {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, \n {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n]\n```\n\n\n\nAnswer a question based on given context\n```{python}\nquestion_answerer = pipeline(\"question-answering\")\nquestion_answerer(\n    question=\"Where do I work?\",\n    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n)\n{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}\n```\n\n\n\nSummarize a given input - turn it into a smaller portion of text, while still retaining all of the important information\n```{python}\nsummarizer = pipeline(\"summarization\")\nsummarizer(\n    \"\"\"\n    America has changed dramatically during recent years. Not only has the number of \n    graduates in traditional engineering disciplines such as mechanical, civil, \n    electrical, chemical, and aeronautical engineering declined, but in most of \n    the premier American universities engineering curricula now concentrate on \n    and encourage largely the study of engineering science. As a result, there \n    are declining offerings in engineering subjects dealing with infrastructure, \n    the environment, and related issues, and greater concentration on high \n    technology subjects, largely supporting increasingly complex scientific \n    developments. While the latter is important, it should not be at the expense \n    of more traditional engineering.\n\n    Rapidly developing economies such as China and India, as well as other \n    industrial countries in Europe and Asia, continue to encourage and advance \n    the teaching of engineering. Both China and India, respectively, graduate \n    six and eight times as many traditional engineers as does the United States. \n    Other industrial countries at minimum maintain their output, while America \n    suffers an increasingly serious decline in the number of engineering graduates \n    and a lack of well-educated engineers.\n\"\"\"\n)\n[{'summary_text': ' America has changed dramatically during recent years . The '\n                  'number of engineering graduates in the U.S. has declined in '\n                  'traditional engineering disciplines such as mechanical, civil '\n                  ', electrical, chemical, and aeronautical engineering . Rapidly '\n                  'developing economies such as China and India, as well as other '\n                  'industrial countries in Europe and Asia, continue to encourage '\n                  'and advance engineering .'}]\n```\n\n\n\nConvert text from one language into another\n```{python}\ntranslator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\ntranslator(\"Ce cours est produit par Hugging Face.\")\n[{'translation_text': 'This course is produced by Hugging Face.'}]\n```"
  },
  {
    "objectID": "posts/Hugging Face NLP/HuggingFace NLP Course 1/index1.html#transformers.-how-do-they-work",
    "href": "posts/Hugging Face NLP/HuggingFace NLP Course 1/index1.html#transformers.-how-do-they-work",
    "title": "Hugging Face NLP I: Transformer Architecture Overview",
    "section": "Transformers. How do they work?",
    "text": "Transformers. How do they work?\nTransformers are language models that have been trained on a massive amount of raw, unlabeled data through a particular unsupervised learning technique called “self-supervised learning”. This allows the model to build up a general understanding of the language it’s trained on, but it isn’t good at any particular task.\nAt this point, we can call this model a “general, pretrained transformer” (OMG THAT’S WHAT GPT STANDS FOR).\nWe can then use transfer learning to fine-tune our transformer on labeled data (supervised learning).\nThis practice of fine-tuning a pretained model to apply it towards specific use cases is preferable as the process of pre-training, that is starting from scratch and training a language model on a huge corpus text takes a massive amount of time & resources. So much so that the energy required for it has a very non-trivial carbon footprint. So yeah, I guess you could pretrain models yourself. IF YOU HATE THE EARTH."
  },
  {
    "objectID": "posts/Hugging Face NLP/HuggingFace NLP Course 1/index1.html#very-general-architecture",
    "href": "posts/Hugging Face NLP/HuggingFace NLP Course 1/index1.html#very-general-architecture",
    "title": "Hugging Face NLP I: Transformer Architecture Overview",
    "section": "(Very) General Architecture",
    "text": "(Very) General Architecture\nThe Transformer model is comprised of two parts - an encoder and a decoder.\n\nEncoder\nThe encoder takes in the input and encodes it into a numerical representation (it’s features) that the model can understand.\nThese ‘feature vectors’ are sequences of numbers with one sequence for each word in the input. A word’s feature vectors are also influenced by the other words in the input. So in the case of inputs like “the horse” and “the grove”, ‘the’ would be represented as a different sequence of numbers in each case. This is essentially just how the meaning of words are determined by the context in which they are used.\nEncoders are also ‘bi-directional’, which means these feature vectors are influenced by words located before and after it in the text.\nThey’re usually trained through mask-filling tasks, so they build up a capability to understand words given the context of pre- and proceding words.\nTasks that are centered around understanding an input, like classification or named entity recognition can be done with just the encoder.\n\n\nDecoder\nThe decoder takes the features - our encoded representation, and possibly other inputs to generate an output.\nUnlike encoders, they are uni-directional. The only have access to the words located either before or after a given word in a sequence. This makes sense since if our task is to generated a new string of words based on a preceding sequence, it would be trivialized if we had knowledge of what the upcoming words actually were. This is more relevant in the training of these models. We don’t want the decoder to get into the habit of generating its predictions for future words, by just peeking ahead in the training data and seeing what those words actually are, since it won’t be able to do this when we actually try and use it to complete a sentence, since there will be nothing to peek ahead at.\nThese models are also auto-regressive, meaning sequential predictions build upon each other, using previous predictions as input. For example, if we ask it to predict the next 4 words in a sentence, it will predict word 1, use word 1 to predict word 2, use words 1 & 2 to predict word 3, and so on.\nThey’re trained through text generation tasks, so they build up the capaibility to predict words in sequence.\nThese are used for generative tasks like text generation.\n\n\nAttention Layers\nTransformers have a key feature called ‘attention layers’ that have enabled their impressive performance. At a very high level, when the model is dealing with the representation a.k.a trying to understand the words in an input, the attention layer tells it to pay attention to other specific words to better understand that particular word.\nAn example would be for translating from English, to a language with gendered nouns like Spanish. To translate “the bike” to Spanish, the model will look at the words ‘the’ and ‘bike’. However, when translating ‘the’, the model will also need to pay attention to the word ‘bike’, because that will tell it if it should be masculine or feminine i.e. determine if it’s ‘el’ or ‘la’.\n\n\nSequence-to-sequence models\nThey can be used in tandem for generative tasks that require understanding of an input, like translation or summarization. The encoder takes in an input sequence and outputs a the feature vectors - our numerical representation of the input. The decoder takes in this feature vector as input, along with any other inputs (which as we start predicting tokens, would be our sequence of previously predicted tokens) and generates predictions for subsequent tokens."
  },
  {
    "objectID": "posts/Hugging Face NLP/HuggingFace NLP 3/index.html",
    "href": "posts/Hugging Face NLP/HuggingFace NLP 3/index.html",
    "title": "Hugging Face NLP III: Fine Tuning",
    "section": "",
    "text": "Let’s fine-tune our models, to improve their performance on the specific NLP task we want to use them for.\nIn this notebook we’ll go through the process of acquiring a data set, processing it to be used for training, then using it to finetune our model.\n\nfrom datasets import load_dataset\n\nraw_datasets = load_dataset(\"glue\", \"mrpc\")\nraw_datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})\n\n\nWe’re using the Datasets library to download data we want to use. Inspecting the DatasetDict object, we can see it has a training, validation and test set, along with various features (sentences, the label and id of the row).\n\nraw_train_dataset = raw_datasets[\"train\"]\nprint(raw_train_dataset[1])\nprint(raw_train_dataset.features)\n\n{'sentence1': \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\", 'sentence2': \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\", 'label': 0, 'idx': 1}\n{'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}\n\n\nWe can inspect the data points in our set by indexing into a particular set. Using the .features attribute, we can also see what our numerical labels actually mean. In this case, it’s 1 if the sentences are equivalent, and 0 otherwise.\n\n\n\ndatasets.arrow_dataset.Dataset"
  },
  {
    "objectID": "posts/Reflections 1/index.html",
    "href": "posts/Reflections 1/index.html",
    "title": "Reflections: Starting Off Right",
    "section": "",
    "text": "“If you don’t have time to do it right, when will you have time to do it over?”\nModel evaluation is a key concept in Data Science and a fundamental component of that is selection of appropriate evaluation criteria. I think this principle is generally applicable in life - how are you going to know if you’re doing a good job without first defining what success looks like?\nIt’s critical that these KPIs are defined in a way that accurately reflect one’s end goal, lest they derail you from your original intentions.\nTake my current goals of self-studying topics in data science and machine learning. My progress through various courses and materials is a convenient but ultimately flawed reflection of the progress I’ve made in understanding a subject. My goal isn’t to watch a bunch of lectures, or scroll through a bunch of code cells, but to actual learn deeply. So this blog is my way of slowing down while going through these lessons to better digest the material by translating it into my own thoughts and the way that I undertand it. It’ll also guarantee that I have something to show for along the way. This is my way of doing things right.\nI figured it would be also be a good excercise in communication. Another intention of mine is to become more adept at composing my thoughts into writing. I spend a non-trivial amount of my current job communicating through email, and spend probably too much time trying to construct the perfect phrasing, only to get a response where the other person misspells their own name when signing off. If anything, I just respect the efficiency.\n-Aelx"
  },
  {
    "objectID": "posts/fast.ai/fast.ai Chapter 9 - Tabular/index.html",
    "href": "posts/fast.ai/fast.ai Chapter 9 - Tabular/index.html",
    "title": "Practical Deep Learning for Coders: Tabular Modeling",
    "section": "",
    "text": "In this lesson we’ll be exploring methods of tabular modeling. Tabular data is represented in a table i.e a spreadsheet with columns and rows. Our goal is to use the data in our columns to predict the value of another column.\n\n%matplotlib inline\n\n\n\nimport sys\n!{sys.executable} -m pip3 install fastbook kaggle waterfallcharts treeinterpreter dtreeviz\n\nimport fastbook\nfastbook.setup_book()\n\n\nfrom fastbook import *\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\nfrom treeinterpreter import *\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 8\n\n/Users/alexander/anaconda3/envs/.venv-dlb/bin/python: No module named pip3\n\n\nSome information before we start:\nContinous variable - Data that can be represented numerically that can be mathematically operated on. We can feed these into our models directly because the mathematical operations we’ll do in our model (multiplication, addition, etc.) will make sense straight away.\nCategorical variable - Variables that take on a variety of discrete values. These need to be converted to a number before we can use them as input in our model.\nIn our recent excercises we’ve just been training neural networks to solve every problem we’ve had.\nhewwo, its bubu speaking now. you have 8 days to live. you can extend this time by buying bubu a thai iced tea. &gt;-&lt;\nThis hasn’t been because neural networks are the magical solution to all our problems, although I desperately wish this were the case. It would be so nice if deep learning could fight my demons for me.\nRather, it’s because their flexibility allows them to excel at handling problems related to the unstructured data we’ve been workign with so far - images, audio, and text.\nWhen it comes to the structured, tabular data we’ll be working with here, we’ll make use of decision trees and gradient boosting machines. Simpler, but equally effective modeling techniques.\nDecision trees will be our tool of choice for tabular data for a number of reasons. They’re typically faster and easier to train, requiring less specialized GPU hardware and hyperparameter tuning than neural networks. Libraries that implement decisions trees also have tools and methods for interpreting these models, such as visualizing which columns were the most important factors in making a prediction.\nNote: there are some instances where a neural network would be better for tabular data - like if one of the columns is unstructured data like a string of text.\nYay, I’m now so excited to use decision trees. In fact why don’t I use one to model this feeling. It’ll try to predict if I am excited at any given moment or not. The condition it’ll split on is “are decision trees involved” and it’ll predict “Yes” if so, and “No” if not. Amazing, we already have an 100% accurate model.\n\ncreds = '{\"username\":\"alexanderzliu\",\"key\":\"59b8c0e913b2beb56e71628b918cba51\"}'\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\ncomp = 'bluebook-for-bulldozers'\npath = URLs.path(comp)\npath\n\nPath.BASE_PATH = path\n\nfrom kaggle import api\n\nif not path.exists():\n    path.mkdir(parents=true)\n    api.competition_download_cli(comp, path=path)\n    shutil.unpack_archive(str(path/f'{comp}.zip'), str(path))\n\npath.ls(file_type='text')\n\n(#7) [Path('random_forest_benchmark_test.csv'),Path('Valid.csv'),Path('median_benchmark.csv'),Path('Test.csv'),Path('ValidSolution.csv'),Path('Machine_Appendix.csv'),Path('TrainAndValid.csv')]\n\n\nAfter setting up our data we’ll read it in and look at some of the columns. Seems like there’s a lot of columns for us to parse through\n\ndf = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf.columns\n\nIndex(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n      dtype='object')\n\n\nLet’s take care of an ‘ordinal’ column. This is kinda a middle point between our categorical and continous variables where it means our variable is categorical, but the categories have a natural ordering.\nWe’ll make our ‘ProductSize’ column values take on the type ‘category’. Then we can pass in a list of those categories with a specific order, setting the ‘ordered’ parameter to True.\n\nprint(f\"Product Size levels: {df['ProductSize'].unique()}\")\nsizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True)\n\nProduct Size levels: [nan 'Medium' 'Small' 'Large / Medium' 'Mini' 'Large' 'Compact']\n\n\n0            NaN\n1         Medium\n2            NaN\n3          Small\n4            NaN\n           ...  \n412693      Mini\n412694      Mini\n412695      Mini\n412696      Mini\n412697      Mini\nName: ProductSize, Length: 412698, dtype: category\nCategories (6, object): ['Large' &lt; 'Large / Medium' &lt; 'Medium' &lt; 'Small' &lt; 'Mini' &lt; 'Compact']\n\n\nThe Kaggle competition asks uses to use the root mean squared log error (RMSLE) between actual and predicted auction prices as our metric - our statistic informing us of how well our model is performing. Roots, logs and trees; data scientists can’t get enough of plants.\nWe’ll take the log of our dependent variable now , the SalePrice, so that we can just use RMSE operations in the future to get our RMSLE.\n\ndep_var = 'SalePrice'\ndf[dep_var] = np.log(df[dep_var])\n\nBelow is an illustration of a decision tree. Who knew such a messed-up looking conifer could be so powerful.\nThe tree asks a series of binary (yes/no) questions about the data. After each question, the data is split on branching yes/no paths. Eventually, the bottom of the tree (a leaf node) is reached, where that node will have a prediction for data point, which depends on the answers that led to that particular leaf node.\nIn this example, we first answer a question about age. This leads to questions about either our diet or excercise habits, depending on our first answer. Then depending on this next answer, we’re either labeled fit or unfit.\nYou’re all probably well aware of the category I fall into. As a super sexy data scientist, I am a indeed a fit model that fits models.\n\n\n\nDecision Tree\n\n\nOutside this toy example, our challenge will be to come up with this series of questions that will split our data into as distinct categories as possible, so that predictions we make are accurate because the conditions we split the data do a good job at discerning it from the other categories of our dependent variable.\nThis is how we’ll approach the construction process:\n\nLoop through each column of the dataset\nFor each column, loop through each possible levels (values or categories) of that column\nTry splitting the data into two groups, based on whether they are greater than or less than that level if it is continous, or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable.\nFind the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. We’ll essentially create a very simple mini-model where our predictions will be the average sale price of the items that fall into our two groups after splitting.\nAfter looping through all columns and possible levels for each, pick the split point that gave the best predictions using that simple model.\nWe now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, repeat steps 1 - 5 for these new datasets.\nContinue this process recursively, until you have reached some stopping criterion for each group, e.g. stop splitting a group further when it has only a certain number of items in it.\n\nLet’s talk about handling dates. Dates are interesting because they contain a lot of both continuous and categorical data. For example - they tell us how far away another target date is, but also what day of the week, month of the year, or even day of the year (like for important holidays) it is.\nWe want to parse out all the data contained in our date, and will use a fastai function to do so - .add_datapart(), by passing in our data column in our train and test sets.\n\ndf = add_datepart(df, 'saledate')\ndf_test = pd.read_csv(path/'Test.csv', low_memory=False)\ndf_test = add_datepart(df_test, 'saledate')\n' '.join(o for o in df.columns if o.startswith('sale'))\n\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/fastai/tabular/core.py:23: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/fastai/tabular/core.py:23: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n\n\n'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed'\n\n\nPredictions for our test set will be trying to predict the SalePrice for bull dozers sold in the future. To mimic the relationship that the test set will have with the rest of our data (being in the future), we will also separate our training and validation sets in a similar manner.\nWe’ll make our our training set data from before October 2011, and validation set comprise of sales after that date, so that we validate our model on data from after our training set, which our test set will also be.\nUsing np.where() we can get the set of row indices that fulfill our date conditions.\n\ncond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10)\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\n\nsplits = (list(train_idx),list(valid_idx))\n\nNow let’s start processing the rest of our data. We’ll make use of the TabularPandas class, which is a wrapper for our dataframe that provides us access to some more convenient functions.\nWe’ll need to pass in a few things when creating our TabularPandas object. First the ‘procs’, which will are data processing operations we want applied.\nCategorify - turn columns into numerical category columns. FillMissing - fill rows with missing data (na) with the median of that column.\nThen, the continous and categorical variables, which we’ll extract using the cont_cat_split() method.\nFinally, our y-label/dependent variable and the training/validation splits.\n\nprocs = [Categorify, FillMissing]\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\nTaking a little bit of ac closer look at the ‘Categorify’ proc, we can see that the values are still represented as strings when we look at them, which is for our convenience in interpreting the data.\n\nto1 = TabularPandas(df, procs, ['state', 'ProductGroup', 'Drive_System', 'Enclosure'], [], y_names=dep_var, splits=splits)\nto1.show(3)\n\n\n\n\n\nstate\nProductGroup\nDrive_System\nEnclosure\nSalePrice\n\n\n\n\n0\nAlabama\nWL\n#na#\nEROPS w AC\n11.097410\n\n\n1\nNorth Carolina\nWL\n#na#\nEROPS w AC\n10.950807\n\n\n2\nNew York\nSSL\n#na#\nOROPS\n9.210340\n\n\n\n\n\nThey’re actually just stored as numbers where all of the levels in a column are replaced by numbers. The levels aren’t indexed in any particular order, unless they ARE ordered as we saw in the ProductSize category. Otherwise, they’re just assigned as unique levels are seen from the top to bottom of the data frame.\n\nto1.items[['state', 'ProductGroup', 'Drive_System', 'Enclosure']].head(3)\n\n\n\n\n\n\n\n\nstate\nProductGroup\nDrive_System\nEnclosure\n\n\n\n\n0\n1\n6\n0\n3\n\n\n1\n33\n6\n0\n3\n\n\n2\n32\n3\n0\n6\n\n\n\n\n\n\n\nWith this pre-processing done, let’s save our work right now, which will be us pickling our TabularPandas object. YUM I LOVE DILL.\n\nsave_pickle(path/'to.pkl',to)\n\nOkay, lets start to actually create a decision tree. We’ll use the .train and .valid attributes from our TabularPandas object to split our dataset.\n\nto = load_pickle(path/'to.pkl')\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\n\nThen we’ll use the DecisionTreeRegressor class from sklearn (a nice library for non-deep learning machine learning models)\nWe’ll pass in an argument for the maximum leaf nodes it should have, fit it (which will be done through the 7-step method done mentioned above)\n\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs, y);\n\nHere we can take a look at the tree. For each node it tells us the condition and value that it branches on (except for the leaf nodes), the squared error, the number of samples contained in that particular node, and the value, which is the average of the samples dependent variable.\n\ndraw_tree(m, xs, size=10, leaves_parallel=True, precision=2)\n\nExecutableNotFound: failed to execute Path('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n&lt;graphviz.sources.Source at 0x107dc6750&gt;\n\n\nThere are some bulldozers that were somehow made in the year 1000, which is cap because dirt wasn’t even invented back then. We’ll just make the earliest manufacturing year 1950.\n\nxs.loc[xs['YearMade']&lt;1900, 'YearMade'] = 1950\nvalid_xs.loc[valid_xs['YearMade']&lt;1900, 'YearMade'] = 1950\n\nLet’s make a new model. It’ll be larger because we aren’t specifying any stopping points. As a default, it’ll create levels until all the leaves are pure - that is, they only have samples from one level of our dependent variable.\nWe’ll also create a funciton for our models root mean squared error.\n\nm = DecisionTreeRegressor()\nm.fit(xs, y);\ndef r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n\nA perfect model. JK it is not my bubu (that’s my pet name for my girlfriend - I am pandering to her :D). Our error is 0 on the validation set because we’ve create a huge tree with pure leaf nodes. The validation set’s error is quite a bit higher.\n\nprint(f\"Training set RMSE: {m_rmse(m, xs, y)}\")\nprint(f\"Validation set RMSE: {m_rmse(m, valid_xs, valid_y)}\")\n\nTraining set RMSE: 0.0\nValidation set RMSE: 0.333112\n\n\nThat’s cause our model is vastly overfitted. We can see that there are almost as many leaves as samples. This means that our model won’t generalize well to unseen data because it’s too specific (re: overfitted).\n\nm.get_n_leaves(), len(xs)\n\n(324365, 404710)\n\n\nLet’s fix this overfitting a bit. Now we’ll stop our tree creation when there are less than 25 samples in a leaf. This should make it generalize more well.\nYou can think about this as the model not uncovering patterns or relationships between our independent variables and dependent variable (e.g. newer bulldozers having higher sale prices). Instead our model is essentially memorizing the decision path it needs for a specific sample (e.g. This bulldozers sale price is 12. It has a Drive_System of X, Enclosure of Y, state is …and so on and so forth for all of our independent variables. The tree is going to split on all of these and create a leaf node with a value of 12).\n\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n(0.243019, 0.30936)\n\n\nLet’s talk about bagging. There are many types of bagging you might be familiar with - sandbagging, carpetbagging, teabagging. But today we’re dealing with some pure, unadultered bagging.\nBagging is basically getting predictions from a bunch of different models and taking the average of those predictions to use as our prediction. It’s a specific ‘ensembling’ approach, where multiple models are used to get your prediction. Here’s how we’ll bag:\n\nChoose a random subset of our data\n\nTrain a model using this subset\nRepeat steps 1 & 2 a number of times\nMake a prediction with all our models and then take the average of each of those model’s predictions to use as our prediction\n\nThis approach works because by training our models on different sets of the data will produce uncorrelated error for our predictions, which should average out to 0.\nRandom forests is the technique when you bag with decision trees. Specifically, they not only use a random subset of the data for each tree’s training, but also choose from a random subset of the columns when deciding each split for the decision trees, essentially enforcing some variety in the splits of the trees.\nLet’s start training a random forest of our own. There are some parameters we should go over here. - n_estimators: The number of decision trees in our random forest - max_samples: the number of samples we should take from our dataset to train each of our decision trees - max_features: The proportion of total columns to potentiallly split on at each level (.5 means we’ll consider half of the columns we have to split on at a given level) - min_samples_leaf: minimum number of samples a leaf should have - n_jobs: CPU-related argument; probably don’t need to worry about this too much\n\ndef rf(xs, y, n_estimators=40, max_samples=200_000,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n\nWe can see how our error has improved with this random forest as compared to our singular decision tree. A cool thing about random forests is that they aren’t super sensitive to hyperparmeter changes, and work pretty well right off the bat with just their default arguments.\n\nm = rf(xs, y)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n(0.17133, 0.232621)\n\n\nAfter plotting our error as a function of the number of decision trees in our forest, we can see the error goes down until tapering out at around 30.\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\nplt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]);\n\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n\n\n\n\n\nLet’s talk about OOBE. This fun little acronym stands for ‘out of bag error’ - an phrase that’s even more fun! This is our error for the rows that weren’t part of the random sample used in training our particular decision tree.\nThis is lower than our validation error, so there must be something bumping that number up.\n\nr_mse(m.oob_prediction_, y)\n\n0.21123\n\n\nOur preds is a stacked array of all of our trees’ predictions of all of the auctions. It’s got a shape of (40, 7900), which corresponds to 40 rows, one for each of our trees, and over 7000 columns, one for each of the auctions.\nWe’ll get the standard deviation of our predictions along the 0-index axis, which is between all our trees for a given auction. This value can be interpreted as how much our trees agree on their predictions for a given auction. We can see our the different values of standard deviations from our first 5 auctions how much the models (dis)agree on their predictions.\n\npreds_std = preds.std(0)\npreds_std[:5]\n\narray([0.25915422, 0.12424009, 0.11996796, 0.24362901, 0.13341298])\n\n\nNot all features are created equal. Some of our columns might be used more than others to determine a split, or will result in a split that partitions the dataset in a significant manner. These will be of particular importance to us in being a more significant indicator of our dependent variable. If only there was some way to the determine the importance of our features. Our feature importance, if you will…\nWELL WE’RE IN LUCK! Our random forest object has an attribute that will tell us the importances of our features, aptly named .feature_importances_.\nLet’s take a look at our most important features.\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\nfi = rf_feat_importance(m, xs)\nfi[:10]\n\n\n\n\n\n\n\n\ncols\nimp\n\n\n\n\n57\nYearMade\n0.170818\n\n\n30\nCoupler_System\n0.110065\n\n\n6\nProductSize\n0.096737\n\n\n7\nfiProductClassDesc\n0.087127\n\n\n54\nModelID\n0.056342\n\n\n65\nsaleElapsed\n0.051404\n\n\n3\nfiSecondaryDesc\n0.049120\n\n\n32\nHydraulics_Flow\n0.048373\n\n\n31\nGrouser_Tracks\n0.042956\n\n\n12\nEnclosure\n0.041079\n\n\n\n\n\n\n\nThe way that importance is calculated, is that all branches of all the decision trees are iterated through. The ‘improvement’ of the model as a result of that split (usually measured by the gini or impurity reduction) is weighted by the number of samples in that node, and is summed for each feature across all branches of all trees. These sums are then normalized to 1.\n\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n\n\n\n\nLet’s focus on just what’s important. We’ll remove all the columns with a really small feature importance value and train another model on our only important columns.\nWe can see that the performance is basically the same as our model trained on the entire dataset, but now the task of scrutinizing is much more manageable with this subset of columns.\nLet’s make it a habit of simplying our model when possible as one of the first things we do to try and improve it.\n\nto_keep = fi[fi.imp&gt;0.005].cols\nlen(to_keep)\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\nm = rf(xs_imp, y)\nprint(f\"Important feature training error: {m_rmse(m, xs_imp, y)}, Important feature valid error: {m_rmse(m, valid_xs_imp, valid_y)}\")\nprint(f\" All columns: {len(xs.columns)}, Important columns: {len(xs_imp.columns)}\")\n\nImportant feature training error: 0.181966, Important feature valid error: 0.23228\n All columns: 66, Important columns: 21\n\n\nAnother step we can take to simplying our model (by removing unnecessary columns) is to remove redundant ones. Here’s a visualization of the our most similar columns.\nAs expected, something like ‘saleYear’ and ‘saleElapsed’ are very similar. Even though they might have different values, the model will be able to extract the same insights - how long ago it was sold - from the columns.\n\ncluster_columns(xs_imp)\n\n\n\n\nLet’s experiment to see if dropping columns has any affect on our performance (hint: if we’re making a point to illustrate this technique it probably won’t). I’m a big fan of dropping things - the beat, some bars, some facts, and columns are no exception.\nThis method will give us our OOB score, and the parameters to the model we’re training in this function are set up to train it quickly. We can see the baseline OOB error for our model with all important columns below.\n\ndef get_oob(df):\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\n\nget_oob(xs_imp)\n\n0.8757528501983463\n\n\nDropping one of each our potentially redudant variables at a time, the OOB error doesn’t change much.\n\n{c:get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'saleYear', 'saleElapsed', 'ProductGroupDesc','ProductGroup',\n    'fiModelDesc', 'fiBaseModel',\n    'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}\n\n{'saleYear': 0.8748843391113379,\n 'saleElapsed': 0.870177319246467,\n 'ProductGroupDesc': 0.8766598105150334,\n 'ProductGroup': 0.8757226956292669,\n 'fiModelDesc': 0.8742575959480965,\n 'fiBaseModel': 0.8747567022539106,\n 'Hydraulics_Flow': 0.8763373418656346,\n 'Grouser_Tracks': 0.8764419294629359,\n 'Coupler_System': 0.8757376058228592}\n\n\nEven dropping multiple redundant columns at a time still holds up\n\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(xs_imp.drop(to_drop, axis=1))\n\n0.8727226538412444\n\n\nLet’s create a dataframe with our dropped variables (and save it as a good practice).\nAs a final check, we’ll look at the validation errors from this random forest model training on our dataset with redundancies eliminated. Looks pretty similar to our performance from having the entire dataset, so it looks like we’ve been able to effectively simplify our model even further. I told you it would all work out.\n\nxs_final = xs_imp.drop(to_drop, axis=1) #create dataframe without dropped columns\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1) #create dataframe without dropped columns\nsave_pickle(path/'xs_final.pkl', xs_final) #save df\nsave_pickle(path/'valid_xs_final.pkl', valid_xs_final) #save df\nxs_final = load_pickle(path/'xs_final.pkl') #reload df\nvalid_xs_final = load_pickle(path/'valid_xs_final.pkl') #reload df\nm = rf(xs_final, y) #train random forest model\nm_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y) #get validation error\n\n(0.183987, 0.233538)\n\n\nNow let’s talk about ‘partial dependence’. These plots try and isolate the effect that a single independent variable has on our dependent variable, without that relationship being affected by changes in our other variables.\nHow this will work is we’ll take every level of our independent variable (IV) in question (we’re looking at ‘YearMade’ and ‘ProductSize’ in this example because they are our most important features) and for our entire dataset, replace every value of that independent variable with one of the levels of our IV. Then we’ll use our model to make predictions , where all our other features are the same as before, except for our IV which has had all of its values swapped with one of its levels. Then we’ll repeat this process for every level in that IV. This will give us predictions where everything is held constant except for the IV we’re investigating.\nThis translates to our working dataset by keeping all the columns the same except ‘YearMade’. We’ll replace all the values in that column with 1950, make our predictions, take the average, and that’ll be our prediction for bulldozers from 1950. Then we’ll replace all the values in the column with 1951, and rinse and repeat with making and averaging predictions for all the levels in the ‘YearMade’ column.\nIn plotting this, we can see that the year and price have a very strong, positively correlated relationship, as we’d except. The ‘ProductSize’ doesn’t quite have as clean a relationship though, which we’ll talk about shortly.\n```{python}\nfrom sklearn.inspection import plot_partial_dependence\n\nfig,ax = plt.subplots(figsize=(12, 4))\nplot_partial_dependence(m, valid_xs_final, ['YearMade','ProductSize'],\n                        grid_resolution=20, ax=ax);\n```\nWe’ve seen how much an impact our features have aggregated across our entire dataset, but what if we wanted to drill down into a single row and analyze how our features influenced the prediction for that one data point?\nWe can do this with the treeinterpreter library.\nLet’s pass in the first 5 rows of our validation set and pass them to our treeinterpreter, along with our model.\nThe output we’ll get from the .predict() method is: - prediction: self-explanatory (our prediction from our random forest) - bias: the prediction we’d get just by taking the mean of our dependent variable. This would be equivalent to if our model was just the root of the tree with no branches - contributions: The increase or decrease in our prediction based on branch split we take on our path to a leaf node a.k.a the change in our prediction caused by the independent variables. Summing this up, and adding it to our bias will equal the prediction.\n```{python}\nrow = valid_xs_final.iloc[:5]\nprediction,bias,contributions = treeinterpreter.predict(m, row.values)\n```\nPlotting these contributions, we can visualize the impact that each of the independent variables had on our prediction.\n```{python}\nwaterfall(valid_xs_final.columns, contributions[0], threshold=0.08, \n          rotation_value=45,formatting='{:,.3f}');\n```\nWe’ve covered some pretty effective yet simple ways of improving and interpreting our model. But it’s not all sunshine and rainbows when it comes to machine learning.\nRandom forests have a pretty large issue when it comes to extrapolating data - that is making predictions for data it hasn’t seen before, that we’ll illustrate in an example.\nLet’s train a model on the first 30 rows of a slightly noisy but mostly linear dataset\n\nnp.random.seed(42)\nx_lin = torch.linspace(0,20, steps=40)\ny_lin = x_lin + torch.randn_like(x_lin)\nplt.scatter(x_lin, y_lin);\nxs_lin = x_lin.unsqueeze(1)\nx_lin.shape,xs_lin.shape\nx_lin[:,None].shape\nm_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30])\n\n\n\n\nWhen we try to make a prediction on the rest of the dataset (the data is blue, our predictions are red). From this graph, we can see that predictions for an x-value greater than 12.5 are all too low, with them all being around 16.\nThis is because of how are predictions are generated. Remember that they’re just the average of the dependent variable values for the samples in our leaf nodes. Thus, they’ll all be a fixed number, based on data that the model has seen before. So when it makes predictions on new data, it’s essentially saying “you’re closest to this group that I’ve seen before, so that’ll be your prediction. You might actually be really far off from this group, but you’re even farther off from all the other groups, so that’s the best I can do for you.”\n\nplt.scatter(x_lin, y_lin, 20)\nplt.scatter(x_lin, m_lin.predict(xs_lin), color='red', alpha=0.5);\n\n\n\n\nWith this extrapolation problem in mind, we want to make sure the distribution of our training and validation sets is similar - that our validation set does not contain any ‘Out-of-Domain’ data.\nWe can assess the distribution of our training and validation sets in a pretty clever way. Let’s combine the two, and instead of trying to predict a SalesPrice, introduce a new dependent variable which will be whether or not the data was originally in the training or validation set, and try to predict that.\nLooking at the features that are most important in discerning the original set of a sample, we see the top 3 are ‘saleElapsed’, ‘SalesID’, and ‘MachineID’. This makes sense since the original way we split up the set was by the date.\n\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]\n\n\n\n\n\n\n\n\ncols\nimp\n\n\n\n\n5\nsaleElapsed\n0.854117\n\n\n11\nSalesID\n0.113978\n\n\n12\nMachineID\n0.024497\n\n\n0\nYearMade\n0.002955\n\n\n4\nModelID\n0.001058\n\n\n3\nfiProductClassDesc\n0.000660\n\n\n\n\n\n\n\nLet’s use a technique we used above, and see what the performance of our model would be when one of these columns is removed.\n\nm = rf(xs_final, y)\nprint('orig', m_rmse(m, valid_xs_final, valid_y))\n\nfor c in ('SalesID','saleElapsed','MachineID'):\n    m = rf(xs_final.drop(c,axis=1), y)\n    print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))\n\norig 0.232237\nSalesID 0.229874\nsaleElapsed 0.234587\nMachineID 0.230984\n\n\nWe’ll drop the ‘SalesID’ and ‘MachineID’ columns, which actually improves the accuracy of our model.\n\ntime_vars = ['SalesID','MachineID']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\nm = rf(xs_final_time, y)\nm_rmse(m, valid_xs_time, valid_y)\n\n0.228792\n\n\nAnother thing that can help is to make sure we’re only using relevant data. Since it seems like the age of the bulldozer is a very important feature to our predictions, we’ll actually just use a subset of our data, the most recent 20 years or so, to train our model so that any outdated relationships aren’t captured in our dataset.\n\nxs['saleYear'].hist();\nfilt = xs['saleYear']&gt;2004\nxs_filt = xs_final_time[filt]\ny_filt = y[filt]\n\n\n\n\nThis model trained on only data from after 2004 is performing better than anything else we’ve seen so far.\n\nm = rf(xs_filt, y_filt)\nm_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)\n\n(0.178646, 0.229726)\n\n\nAlright, would we really be data scientists if we didn’t throw a neural network at everything? Let’s see how they can help. Let’s pre-process our data the same way we did before passing it into the TabularPandas object, and also use only the subset of columns that we’ve analyzed to be most relevant.\n\ndf_nn = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf_nn['ProductSize'] = df_nn['ProductSize'].astype('category')\ndf_nn['ProductSize'].cat.set_categories(sizes, ordered=True)\ndf_nn[dep_var] = np.log(df_nn[dep_var])\ndf_nn = add_datepart(df_nn, 'saledate')\ndf_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/fastai/tabular/core.py:23: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n\n\nWe’ll also have to handle categorical and continous variables differently with neural networks, because we can use embeddings here. We’ll use the cont_cat_split function and pass in a ‘max_card’ argument, which will treat variables as categorical if there are less levels than max_card.\n\ncont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\n\nOur ‘saleElapsed’ column is treated as a continous variable, which is what we want because we wouldn’t be able to extrapolate for sales in the future if it was a categorical variable.\n\ncont_nn\n\n['saleElapsed']\n\n\nIt looks like we have two categories with a lot of unique values. fiModelDesc has over 5000 and sound sjust like fiModelDescriptor. They literally just sound redundant, so let’s see if we can get away with removing one of them (hint: it won’t)\n\ndf_nn_final[cat_nn].nunique()\n\nYearMade                73\nCoupler_System           2\nProductSize              6\nfiProductClassDesc      74\nModelID               5281\nfiSecondaryDesc        177\nHydraulics_Flow          3\nEnclosure                6\nfiModelDesc           5059\nfiModelDescriptor      140\nDrive_System             4\nHydraulics              12\nTrack_Type               2\nProductGroup             6\ndtype: int64\n\n\nHint: it didn’t\n\nxs_filt2 = xs_filt.drop('fiModelDescriptor', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('fiModelDescriptor', axis=1)\nm2 = rf(xs_filt2, y_filt)\nprint(m_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y))\n\ncat_nn.remove('fiModelDescriptor')\n\n0.177728 0.230042\n\n\nNow that we’ve done some exploration and refinement of our dataset we can create a TabularPandas object to handle the rest of our processing. We’ll use a large batch size because tabular models are generally GPU RAM friendly.\n\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,\n                      splits=splits, y_names=dep_var)\n\ndls = to_nn.dataloaders(1024)\n\nWe’ll create our tabular model with the tabular_learner class. We specify a y_range for our regression model, based on the max and mins we see in the training set. Since we have a lot of data, we’ll make our layers a bit larger, and use MSE as our loss function as defined in the Kaggle competition.\n\ny = to_nn.train.y\nprint(y.min(),y.max())\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\n\n8.465899 11.863583\n\n\nUsing the lr_find() method to get our optimal learning rate, let’s train the model for a few epochs.\n\nlearn.lr_find()\nlearn.fit_one_cycle(5, 1e-2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.072696\n0.067694\n00:06\n\n\n1\n0.055061\n0.069597\n00:06\n\n\n2\n0.048782\n0.068780\n00:07\n\n\n3\n0.044022\n0.051267\n00:06\n\n\n4\n0.041040\n0.050207\n00:06\n\n\n\n\n\n\n\n\nUsing it for predictions, we can see that it does a better job than our random forest. The trade off is that it took longer to train, and we need to do some more work to find the hyperparameters that allowed it to perform so well.\n\npreds,targs = learn.get_preds()\nlearn.save('nn') #save model incase we want to come back to it\nprint(r_mse(preds,targs))\n\n\n\n\n\n\n\n\n0.224069\n\n\nLet’s take ensembling even further. Our random forest was an ensemble of decision trees, but there’s no rules saying that we have to use the same architecture model architecture. We’ll combine our already ensembled model, the random forest, and our neural network and create an enseensemblemble.\nWe’ll need to convert their outputs to the same shape and object type, but after doing that we can see the average of the predictions from these two models is better than either of them separately.\n\nrf_preds = m.predict(valid_xs_time)\nens_preds = (to_np(preds.squeeze()) + rf_preds) /2\nr_mse(ens_preds,valid_y)\n\n0.22146\n\n\nLet’s quickly talk about another decision tree ensemble - gradient boosting machines. Instead of taking the average of a bunch of predictions like in random forest, with gradient boosting our final prediction will come from the sum of our set of predictions. It works like so:\n\nTrain a small model that will inevitably underfit the data set\nCalculate the predictions in the training set for this model\nSubtract the predictions from the targets to get our ‘residuals’ which is our error for each of our data points\nRepeat steps 1 - 3, but instead of using our original targets, we now target our residuals calculated from the last iteration instead\nContinue doing this until you reach some stopping criterion e.g. a max number of trees is created, or our validation error is getting worse.\n\nBy doing this, we’re sequentially trying to build another tree that will bridge the gap that the last one had between the predictions and target (which are our residuals after the first tree), over and over again.\nThen, to get our final prediction, we sum up all of the predictions we get from the trees in our ensemble.\nLet’s finish of this entry with some more of our favorite thing: text explanations. We actually ended up doing a lot of different, useful things this notebook and I want to recap them.\nWe took a look at 3 different models:\n\nRandom Forests: Pros:\n\nFast to train\nEasy to get started with by requiring minimal preprocessing or hyperparameter tuning\nResilient to overfitting with enough trees\nCan provide insight into data with partial dependence analysis or feature importance Cons:\nMay be less accurate than other approaches, and if extrapolation is needed then thats a problem\n\nNeural Networks: Pros:\n\nGood results & extrapolate well\nCan use parts (embedding layer) in other models even if the predictions/rest of the layers aren’t Cons:\nLong to train\nFinnicky -&gt; requires more preprocessing and hyperparameter tuning to get good performance\nCan overfit\n\nGradient Boosting: Pros:\n\nFast to train (about the same as random forest)\nCan be more accurate than random forest Cons:\nRequire hyperparameter tuning\nCan overfit\n\n\nAnd also did some important work in data exploration and processing our data set to improve our results, and understanding of our data.\nWe: - Look at feature importance to determine our most impact columns and remove the least import ones - Removed redundant columns - Deal with out-of-domain data issues by filtering our data set to try and get our training and validation set to have - Did partial dependence to see the impact one independent variable has on our dependent variable\nAnd we’re able to do all of that from the starting off by just training a model and seeing where it could take us. These have been useful techniques in improving our understanding of the data, and we were able to make use of them by just jumping in and getting our bearings from where we landed. We literally modeled first, and asked questions later (and then modeled again some more). Having a bias towards action, is a good attitude to have when in deep learning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Tabular Modeling\n\n\n\n\n\n\n\nDeep Learning\n\n\nTabular Data\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nAlex Liu\n\n\n\n\n\n\n  \n\n\n\n\nHugging Face NLP III: Fine Tuning\n\n\n\n\n\n\n\nNLP\n\n\nHugging Face\n\n\nCourse Notes\n\n\nFine Tuning\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2024\n\n\nAlex Liu\n\n\n\n\n\n\n  \n\n\n\n\nHugging Face NLP II: Tokenization & Batching\n\n\n\n\n\n\n\nNLP\n\n\nHugging Face\n\n\nCourse Notes\n\n\nFine Tuning\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\nAlex Liu\n\n\n\n\n\n\n  \n\n\n\n\nHugging Face NLP I: Transformer Architecture Overview\n\n\n\n\n\n\n\nHugging Face\n\n\nNLP\n\n\nCourse Notes\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nAlex Liu\n\n\n\n\n\n\n  \n\n\n\n\nNeural Net Foundations w/ MNIST Image Recognition\n\n\n\n\n\n\n\nfast.ai\n\n\nNeural Networks\n\n\nGradient Descent\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nAlex Liu\n\n\n\n\n\n\n  \n\n\n\n\nReflections: Starting Off Right\n\n\n\n\n\n\n\nReflections\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nAlexander Liu\n\n\n\n\n\n\n  \n\n\n\n\nWelcome\n\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nAlexander Liu\n\n\n\n\n\n\nNo matching items"
  }
]