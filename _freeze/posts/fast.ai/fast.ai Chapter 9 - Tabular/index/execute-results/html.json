{
  "hash": "f3aa2f3439010cf7cc2b80fb81c9d7f7",
  "result": {
    "markdown": "---\ntitle: 'Practical Deep Learning for Coders: Tabular Modeling'\nauthor: Alex Liu\ndate: '2023-11-01'\ncategories:\n  - Deep Learning\n  - Tabular Data\nimage: image.jpg\n---\n\nIn this lesson we'll be exploring methods of tabular modeling. Tabular data is represented in a table i.e a spreadsheet with columns and rows. Our goal is to use the data in our columns to predict the value of another column. \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n%matplotlib inline\n\n\n\nimport sys\n!{sys.executable} -m pip3 install fastbook kaggle waterfallcharts treeinterpreter dtreeviz\n\nimport fastbook\nfastbook.setup_book()\n\n\nfrom fastbook import *\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\nfrom treeinterpreter import *\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 8\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n/Users/alexander/anaconda3/envs/.venv-dlb/bin/python: No module named pip3\r\n```\n:::\n:::\n\n\nSome information before we start:\n\nContinous variable - Data that can be represented numerically that can be mathematically operated on. We can feed these into our models directly because the mathematical operations we'll do in our model (multiplication, addition, etc.) will make sense straight away.\n\nCategorical variable - Variables that take on a variety of discrete values. These need to be converted to a number before we can use them as input in our model. \n\nIn our recent excercises we've just been training neural networks to solve every problem we've had. \n\nhewwo, its bubu speaking now. you have 8 days to live. you can extend this time by buying bubu a thai iced tea. >-<\n\nThis hasn't been because neural networks are the magical solution to all our problems, although I desperately wish this were the case. It would be so nice if deep learning could fight my demons for me. \n\nRather, it's because their flexibility allows them to excel at handling problems related to the unstructured data we've been workign with so far - images, audio, and text. \n\nWhen it comes to the structured, tabular data we'll be working with here, we'll make use of decision trees and gradient boosting machines. Simpler, but equally effective modeling techniques.\n\nDecision trees will be our tool of choice for tabular data for a number of reasons. They're typically faster and easier to train, requiring less specialized GPU hardware and hyperparameter tuning than neural networks. Libraries that implement decisions trees also have tools and methods for interpreting these models, such as visualizing which columns were the most important factors in making a prediction. \n\nNote: there are some instances where a neural network would be better for tabular data - like if one of the columns is unstructured data like a string of text.\n\nYay, I'm now so excited to use decision trees. In fact why don't I use one to model this feeling. It'll try to predict if I am excited at any given moment or not. The condition it'll split on is \"are decision trees involved\" and it'll predict \"Yes\" if so, and \"No\" if not. Amazing, we already have an 100% accurate model.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ncreds = '{\"username\":\"alexanderzliu\",\"key\":\"59b8c0e913b2beb56e71628b918cba51\"}'\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\ncomp = 'bluebook-for-bulldozers'\npath = URLs.path(comp)\npath\n\nPath.BASE_PATH = path\n\nfrom kaggle import api\n\nif not path.exists():\n    path.mkdir(parents=true)\n    api.competition_download_cli(comp, path=path)\n    shutil.unpack_archive(str(path/f'{comp}.zip'), str(path))\n\npath.ls(file_type='text')\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n(#7) [Path('random_forest_benchmark_test.csv'),Path('Valid.csv'),Path('median_benchmark.csv'),Path('Test.csv'),Path('ValidSolution.csv'),Path('Machine_Appendix.csv'),Path('TrainAndValid.csv')]\n```\n:::\n:::\n\n\nAfter setting up our data we'll read it in and look at some of the columns. Seems like there's a lot of columns for us to parse through\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf.columns\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nIndex(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n      dtype='object')\n```\n:::\n:::\n\n\nLet's take care of an 'ordinal' column. This is kinda a middle point between our categorical and continous variables where it means our variable is categorical, but the categories have a natural ordering. \n\nWe'll make our 'ProductSize' column values take on the type 'category'. Then we can pass in a list of those categories with a specific order, setting the 'ordered' parameter to True. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nprint(f\"Product Size levels: {df['ProductSize'].unique()}\")\nsizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProduct Size levels: [nan 'Medium' 'Small' 'Large / Medium' 'Mini' 'Large' 'Compact']\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n0            NaN\n1         Medium\n2            NaN\n3          Small\n4            NaN\n           ...  \n412693      Mini\n412694      Mini\n412695      Mini\n412696      Mini\n412697      Mini\nName: ProductSize, Length: 412698, dtype: category\nCategories (6, object): ['Large' < 'Large / Medium' < 'Medium' < 'Small' < 'Mini' < 'Compact']\n```\n:::\n:::\n\n\nThe Kaggle competition asks uses to use the root mean squared log error (RMSLE) between actual and predicted auction prices as our metric - our statistic informing us of how well our model is performing. Roots, logs and trees; data scientists can't get enough of plants.\n\nWe'll take the log of our dependent variable now , the SalePrice, so that we can just use RMSE operations in the future to get our RMSLE.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndep_var = 'SalePrice'\ndf[dep_var] = np.log(df[dep_var])\n```\n:::\n\n\nBelow is an illustration of a decision tree. Who knew such a messed-up looking conifer could be so powerful. \n\nThe tree asks a series of binary (yes/no) questions about the data. After each question, the data is split on branching yes/no paths. Eventually, the bottom of the tree (a leaf node) is reached, where that node will have a prediction for data point, which depends on the answers that led to that particular leaf node. \n\nIn this example, we first answer a question about age. This leads to questions about either our diet or excercise habits, depending on our first answer. Then depending on this next answer, we're either labeled fit or unfit.\n\nYou're all probably well aware of the category I fall into. As a super sexy data scientist, I am a indeed a fit model that fits models. \n\n![Decision Tree](decision_tree.PNG)\n\nOutside this toy example, our challenge will be to come up with this series of questions that will split our data into as distinct categories as possible, so that predictions we make are accurate because the conditions we split the data do a good job at discerning it from the other categories of our dependent variable. \n\nThis is how we'll approach the construction process: \n\n1. Loop through each column of the dataset \n2. For each column, loop through each possible levels (values or categories) of that column\n3. Try splitting the data into two groups, based on whether they are greater than or less than that level if it is continous, or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable.\n4. Find the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. We'll essentially create a very simple mini-model where our predictions will be the average sale price of the items that fall into our two groups after splitting. \n5. After looping through all columns and possible levels for each, pick the split point that gave the best predictions using that simple model.\n6. We now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, repeat steps 1 - 5 for these new datasets.\n7. Continue this process recursively, until you have reached some stopping criterion for each group, e.g. stop splitting a group further when it has only a certain number of items in it.\n\n\n\nLet's talk about handling dates. Dates are interesting because they contain a lot of both continuous and categorical data. For example - they tell us how far away another target date is, but also what day of the week, month of the year, or even day of the year (like for important holidays) it is. \n\nWe want to parse out all the data contained in our date, and will use a fastai function to do so - .add_datapart(), by passing in our data column in our train and test sets.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndf = add_datepart(df, 'saledate')\ndf_test = pd.read_csv(path/'Test.csv', low_memory=False)\ndf_test = add_datepart(df_test, 'saledate')\n' '.join(o for o in df.columns if o.startswith('sale'))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/fastai/tabular/core.py:23: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/fastai/tabular/core.py:23: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed'\n```\n:::\n:::\n\n\nPredictions for our test set will be trying to predict the SalePrice for bull dozers sold in the future. To mimic the relationship that the test set will have with the rest of our data (being in the future), we will also separate our training and validation sets in a similar manner. \n\nWe'll make our our training set data from before October 2011, and validation set comprise of sales after that date, so that we validate our model on data from after our training set, which our test set will also be. \n\nUsing np.where() we can get the set of row indices that fulfill our date conditions.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ncond = (df.saleYear<2011) | (df.saleMonth<10)\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\n\nsplits = (list(train_idx),list(valid_idx))\n```\n:::\n\n\nNow let's start processing the rest of our data. We'll make use of the TabularPandas class, which is a wrapper for our dataframe that provides us access to some more convenient functions. \n\nWe'll need to pass in a few things when creating our TabularPandas object. First the 'procs', which will are data processing operations we want applied. \n\nCategorify - turn columns into numerical category columns. \nFillMissing - fill rows with missing data (na) with the median of that column. \n\nThen, the continous and categorical variables, which we'll extract using the cont_cat_split() method. \n\nFinally, our y-label/dependent variable and the training/validation splits. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nprocs = [Categorify, FillMissing]\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n```\n:::\n\n\nTaking a little bit of ac closer look at the 'Categorify' proc, we can see that the values are still represented as strings when we look at them, which is for our convenience in interpreting the data. \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nto1 = TabularPandas(df, procs, ['state', 'ProductGroup', 'Drive_System', 'Enclosure'], [], y_names=dep_var, splits=splits)\nto1.show(3)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>state</th>\n      <th>ProductGroup</th>\n      <th>Drive_System</th>\n      <th>Enclosure</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alabama</td>\n      <td>WL</td>\n      <td>#na#</td>\n      <td>EROPS w AC</td>\n      <td>11.097410</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>North Carolina</td>\n      <td>WL</td>\n      <td>#na#</td>\n      <td>EROPS w AC</td>\n      <td>10.950807</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>New York</td>\n      <td>SSL</td>\n      <td>#na#</td>\n      <td>OROPS</td>\n      <td>9.210340</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nThey're actually just stored as numbers where all of the levels in a column are replaced by numbers. The levels aren't indexed in any particular order, unless they ARE ordered as we saw in the ProductSize category. Otherwise, they're just assigned as unique levels are seen from the top to bottom of the data frame. \n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nto1.items[['state', 'ProductGroup', 'Drive_System', 'Enclosure']].head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>state</th>\n      <th>ProductGroup</th>\n      <th>Drive_System</th>\n      <th>Enclosure</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>6</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>33</td>\n      <td>6</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>32</td>\n      <td>3</td>\n      <td>0</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWith this pre-processing done, let's save our work right now, which will be us pickling our TabularPandas object. YUM I LOVE DILL.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nsave_pickle(path/'to.pkl',to)\n```\n:::\n\n\nOkay, lets start to actually create a decision tree. We'll use the .train and .valid attributes from our TabularPandas object to split our dataset.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nto = load_pickle(path/'to.pkl')\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\n```\n:::\n\n\nThen we'll use the DecisionTreeRegressor class from sklearn (a nice library for non-deep learning machine learning models)\n\nWe'll pass in an argument for the maximum leaf nodes it should have, fit it (which will be done through the 7-step method done mentioned above) \n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs, y);\n```\n:::\n\n\nHere we can take a look at the tree. For each node it tells us the condition and value that it branches on (except for the leaf nodes), the squared error, the number of samples contained in that particular node, and the value, which is the average of the samples dependent variable. \n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ndraw_tree(m, xs, size=10, leaves_parallel=True, precision=2)\n```\n\n::: {.cell-output .cell-output-error}\n```\nExecutableNotFound: failed to execute Path('dot'), make sure the Graphviz executables are on your systems' PATH\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n<graphviz.sources.Source at 0x107dc6750>\n```\n:::\n:::\n\n\nThere are some bulldozers that were somehow made in the year 1000, which is cap because dirt wasn't even invented back then. We'll just make the earliest manufacturing year 1950. \n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nxs.loc[xs['YearMade']<1900, 'YearMade'] = 1950\nvalid_xs.loc[valid_xs['YearMade']<1900, 'YearMade'] = 1950\n```\n:::\n\n\nLet's make a new model. It'll be larger because we aren't specifying any stopping points. As a default, it'll create levels until all the leaves are pure - that is, they only have samples from one level of our dependent variable. \n\nWe'll also create  a funciton for our models root mean squared error. \n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nm = DecisionTreeRegressor()\nm.fit(xs, y);\ndef r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n```\n:::\n\n\nA perfect model. JK it is not my bubu (that's my pet name for my girlfriend - I am pandering to her :D). Our error is 0 on the validation set because we've create a huge tree with pure leaf nodes. The validation set's error is quite a bit higher. \n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nprint(f\"Training set RMSE: {m_rmse(m, xs, y)}\")\nprint(f\"Validation set RMSE: {m_rmse(m, valid_xs, valid_y)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining set RMSE: 0.0\nValidation set RMSE: 0.333112\n```\n:::\n:::\n\n\nThat's cause our model is vastly overfitted. We can see that there are almost as many leaves as samples. This means that our model won't generalize well to unseen data because it's too specific (re: overfitted).  \n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nm.get_n_leaves(), len(xs)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n(324365, 404710)\n```\n:::\n:::\n\n\nLet's fix this overfitting a bit. Now we'll stop our tree creation when there are less than 25 samples in a leaf. This should make it generalize more well. \n\nYou can think about this as the model not uncovering patterns or relationships between our independent variables and dependent variable (e.g. newer bulldozers having higher sale prices). Instead our model is essentially memorizing the decision path it needs for a specific sample (e.g. This bulldozers sale price is 12. It has a Drive_System of X, Enclosure of Y, state is ...and so on and so forth for all of our independent variables. The tree is going to split on all of these and create a leaf node with a value of 12). \n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n(0.243019, 0.30936)\n```\n:::\n:::\n\n\nLet's talk about bagging. There are many types of bagging you might be familiar with - sandbagging, carpetbagging, teabagging. But today we're dealing with some pure, unadultered bagging. \n\nBagging is basically getting predictions from a bunch of different models and taking the average of those predictions to use as our prediction. It's a specific 'ensembling' approach, where multiple models are used to get your prediction. Here's how we'll bag:\n\n1. Choose a random subset of our data  \n2. Train a model using this subset \n3. Repeat steps 1 & 2 a number of times \n4. Make a prediction with all our models and then take the average of each of those model's predictions to use as our prediction\n\nThis approach works because by training our models on different sets of the data will produce uncorrelated error for our predictions, which should average out to 0.\n\nRandom forests is the technique when you bag with decision trees. Specifically, they not only use a random subset of the data for each tree's training, but also choose from a random subset of the columns when deciding each split for the decision trees, essentially enforcing some variety in the splits of the trees.\n\nLet's start training a random forest of our own. There are some parameters we should go over here. \n- n_estimators: The number of decision trees in our random forest \n- max_samples: the number of samples we should take from our dataset to train each of our decision trees\n- max_features: The proportion of total columns to potentiallly split on at each level (.5 means we'll consider half of the columns we have to split on at a given level)\n- min_samples_leaf: minimum number of samples a leaf should have \n- n_jobs: CPU-related argument; probably don't need to worry about this too much \n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\ndef rf(xs, y, n_estimators=40, max_samples=200_000,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n```\n:::\n\n\nWe can see how our error has improved with this random forest as compared to our singular decision tree. A cool thing about random forests is that they aren't super sensitive to hyperparmeter changes, and work pretty well right off the bat with just their default arguments. \n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nm = rf(xs, y)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\n(0.17133, 0.232621)\n```\n:::\n:::\n\n\nAfter plotting our error as a function of the number of decision trees in our forest, we can see the error goes down until tapering out at around 30. \n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\nplt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]);\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-23-output-2.png){width=585 height=414}\n:::\n:::\n\n\nLet's talk about OOBE. This fun little acronym stands for 'out of bag error' - an phrase that's even more fun! This is our error for the rows that weren't part of the random sample used in training our particular decision tree. \n\nThis is lower than our validation error, so there must be something bumping that number up. \n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nr_mse(m.oob_prediction_, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\n0.21123\n```\n:::\n:::\n\n\nOur preds is a stacked array of all of our trees' predictions of all of the auctions. It's got a shape of (40, 7900), which corresponds to 40 rows, one for each of our trees, and over 7000 columns, one for each of the auctions. \n\nWe'll get the standard deviation of our predictions along the 0-index axis, which is between all our trees for a given auction. This value can be interpreted as how much our trees agree on their predictions for a given auction. We can see our the different values of standard deviations from our first 5 auctions how much the models (dis)agree on their predictions.  \n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\npreds_std = preds.std(0)\npreds_std[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\narray([0.25915422, 0.12424009, 0.11996796, 0.24362901, 0.13341298])\n```\n:::\n:::\n\n\nNot all features are created equal. Some of our columns might be used more than others to determine a split, or will result in a split that partitions the dataset in a significant manner. These will be of particular importance to us in being a more significant indicator of our dependent variable. If only there was some way to the determine the importance of our features. Our feature importance, if you will...\n\nWELL WE'RE IN LUCK! Our random forest object has an attribute that will tell us the importances of our features, aptly named .feature_importances_.\n\nLet's take a look at our most important features.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\nfi = rf_feat_importance(m, xs)\nfi[:10]\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cols</th>\n      <th>imp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>57</th>\n      <td>YearMade</td>\n      <td>0.170818</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>Coupler_System</td>\n      <td>0.110065</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ProductSize</td>\n      <td>0.096737</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>fiProductClassDesc</td>\n      <td>0.087127</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>ModelID</td>\n      <td>0.056342</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>saleElapsed</td>\n      <td>0.051404</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fiSecondaryDesc</td>\n      <td>0.049120</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>Hydraulics_Flow</td>\n      <td>0.048373</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Grouser_Tracks</td>\n      <td>0.042956</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Enclosure</td>\n      <td>0.041079</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe way that importance is calculated, is that all branches of all the decision trees are iterated through. The 'improvement' of the model as a result of that split (usually measured by the gini or impurity reduction) is weighted by the number of samples in that node, and is summed for each feature across all branches of all trees. These sums are then normalized to 1. \n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-27-output-1.png){width=1094 height=561}\n:::\n:::\n\n\nLet's focus on just what's important. We'll remove all the columns with a really small feature importance value and train another model on our only important columns. \n\nWe can see that the performance is basically the same as our model trained on the entire dataset, but now the task of scrutinizing is much more manageable with this subset of columns. \n\nLet's make it a habit of simplying our model when possible as one of the first things we do to try and improve it. \n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nto_keep = fi[fi.imp>0.005].cols\nlen(to_keep)\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\nm = rf(xs_imp, y)\nprint(f\"Important feature training error: {m_rmse(m, xs_imp, y)}, Important feature valid error: {m_rmse(m, valid_xs_imp, valid_y)}\")\nprint(f\" All columns: {len(xs.columns)}, Important columns: {len(xs_imp.columns)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImportant feature training error: 0.181966, Important feature valid error: 0.23228\n All columns: 66, Important columns: 21\n```\n:::\n:::\n\n\nAnother step we can take to simplying our model (by removing unnecessary columns) is to remove redundant ones. Here's a visualization of the our most similar columns. \n\nAs expected, something like 'saleYear' and 'saleElapsed' are very similar. Even though they might have different values, the model will be able to extract the same insights - how long ago it was sold - from the columns. \n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\ncluster_columns(xs_imp)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-29-output-1.png){width=925 height=488}\n:::\n:::\n\n\nLet's experiment to see if dropping columns has any affect on our performance (hint: if we're making a point to illustrate this technique it probably won't). I'm a big fan of dropping things - the beat, some bars, some facts, and columns are no exception. \n\nThis method will give us our OOB score, and the parameters to the model we're training in this function are set up to train it quickly. We can see the baseline OOB error for our model with all important columns below.\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\ndef get_oob(df):\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\n\nget_oob(xs_imp)\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\n0.8757528501983463\n```\n:::\n:::\n\n\nDropping one of each our potentially redudant variables at a time, the OOB error doesn't change much. \n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\n{c:get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'saleYear', 'saleElapsed', 'ProductGroupDesc','ProductGroup',\n    'fiModelDesc', 'fiBaseModel',\n    'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n{'saleYear': 0.8748843391113379,\n 'saleElapsed': 0.870177319246467,\n 'ProductGroupDesc': 0.8766598105150334,\n 'ProductGroup': 0.8757226956292669,\n 'fiModelDesc': 0.8742575959480965,\n 'fiBaseModel': 0.8747567022539106,\n 'Hydraulics_Flow': 0.8763373418656346,\n 'Grouser_Tracks': 0.8764419294629359,\n 'Coupler_System': 0.8757376058228592}\n```\n:::\n:::\n\n\nEven dropping multiple redundant columns at a time still holds up\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(xs_imp.drop(to_drop, axis=1))\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\n0.8727226538412444\n```\n:::\n:::\n\n\nLet's create a dataframe with our dropped variables (and save it as a good practice).\n\nAs a final check, we'll look at the validation errors from this random forest model training on our dataset with redundancies eliminated. Looks pretty similar to our performance from having the entire dataset, so it looks like we've been able to effectively simplify our model even further. I told you it would all work out.\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nxs_final = xs_imp.drop(to_drop, axis=1) #create dataframe without dropped columns\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1) #create dataframe without dropped columns\nsave_pickle(path/'xs_final.pkl', xs_final) #save df\nsave_pickle(path/'valid_xs_final.pkl', valid_xs_final) #save df\nxs_final = load_pickle(path/'xs_final.pkl') #reload df\nvalid_xs_final = load_pickle(path/'valid_xs_final.pkl') #reload df\nm = rf(xs_final, y) #train random forest model\nm_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y) #get validation error\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\n(0.183987, 0.233538)\n```\n:::\n:::\n\n\nNow let's talk about 'partial dependence'. These plots try and isolate the effect that a single independent variable has on our dependent variable, without that relationship being affected by changes in our other variables. \n\nHow this will work is we'll take every level of our independent variable (IV) in question (we're looking at 'YearMade' and 'ProductSize' in this example because they are our most important features) and for our entire dataset, replace every value of that independent variable with one of the levels of our IV. Then we'll use our model to make predictions , where all our other features are the same as before, except for our IV which has had all of its values swapped with one of its levels. Then we'll repeat this process for every level in that IV. This will give us predictions where everything is held constant except for the IV we're investigating.\n\nThis translates to our working dataset by keeping all the columns the same except 'YearMade'. We'll replace all the values in that column with 1950, make our predictions, take the average, and that'll be our prediction for bulldozers from 1950. Then we'll replace all the values in the column with 1951, and rinse and repeat with making and averaging predictions for all the levels in the 'YearMade' column.\n\nIn plotting this, we can see that the year and price have a very strong, positively correlated relationship, as we'd except. The 'ProductSize' doesn't quite have as clean a relationship though, which we'll talk about shortly. \n\n```{{python}}\nfrom sklearn.inspection import plot_partial_dependence\n\nfig,ax = plt.subplots(figsize=(12, 4))\nplot_partial_dependence(m, valid_xs_final, ['YearMade','ProductSize'],\n                        grid_resolution=20, ax=ax);\n```\n\n\nWe've seen how much an impact our features have aggregated across our entire dataset, but what if we wanted to drill down into a single row and analyze how our features influenced the prediction for that one data point? \n\nWe can do this with the treeinterpreter library. \n\nLet's pass in the first 5 rows of our validation set and pass them to our treeinterpreter, along with our model.\n\nThe output we'll get from the .predict() method is:\n- prediction: self-explanatory (our prediction from our random forest)\n- bias: the prediction we'd get just by taking the mean of our dependent variable. This would be equivalent to if our model was just the root of the tree with no branches\n- contributions: The increase or decrease in our prediction based on branch split we take on our path to a leaf node a.k.a the change in our prediction caused by the independent variables. Summing this up, and adding it to our bias will equal the prediction. \n\n```{{python}}\nrow = valid_xs_final.iloc[:5]\nprediction,bias,contributions = treeinterpreter.predict(m, row.values)\n```\n\nPlotting these contributions, we can visualize the impact that each of the independent variables had on our prediction. \n\n```{{python}}\nwaterfall(valid_xs_final.columns, contributions[0], threshold=0.08, \n          rotation_value=45,formatting='{:,.3f}');\n```\n\nWe've covered some pretty effective yet simple ways of improving and interpreting our model. But it's not all sunshine and rainbows when it comes to machine learning. \n\nRandom forests have a pretty large issue when it comes to extrapolating data - that is making predictions for data it hasn't seen before, that we'll illustrate in an example. \n\nLet's train a model on the first 30 rows of a slightly noisy but mostly linear dataset\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nnp.random.seed(42)\nx_lin = torch.linspace(0,20, steps=40)\ny_lin = x_lin + torch.randn_like(x_lin)\nplt.scatter(x_lin, y_lin);\nxs_lin = x_lin.unsqueeze(1)\nx_lin.shape,xs_lin.shape\nx_lin[:,None].shape\nm_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30])\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-34-output-1.png){width=570 height=414}\n:::\n:::\n\n\nWhen we try to make a prediction on the rest of the dataset (the data is blue, our predictions are red). From this graph, we can see that predictions for an x-value greater than 12.5 are all too low, with them all being around 16. \n\nThis is because of how are predictions are generated. Remember that they're just the average of the dependent variable values for the samples in our leaf nodes. Thus, they'll all be a fixed number, based on data that the model has seen before. So when it makes predictions on new data, it's essentially saying \"you're closest to this group that I've seen before, so that'll be your prediction. You might actually be really far off from this group, but you're even farther off from all the other groups, so that's the best I can do for you. \"\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nplt.scatter(x_lin, y_lin, 20)\nplt.scatter(x_lin, m_lin.predict(xs_lin), color='red', alpha=0.5);\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-35-output-1.png){width=570 height=414}\n:::\n:::\n\n\nWith this extrapolation problem in mind, we want to make sure the distribution of our training and validation sets is similar - that our validation set does not contain any 'Out-of-Domain' data. \n\nWe can assess the distribution of our training and validation sets in a pretty clever way. Let's combine the two, and instead of trying to predict a SalesPrice, introduce a new dependent variable which will be whether or not the data was originally in the training or validation set, and try to predict that. \n\nLooking at the features that are most important in discerning the original set of a sample, we see the top 3 are 'saleElapsed', 'SalesID', and 'MachineID'. This makes sense since the original way we split up the set was by the date. \n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cols</th>\n      <th>imp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>saleElapsed</td>\n      <td>0.854117</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>SalesID</td>\n      <td>0.113978</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>MachineID</td>\n      <td>0.024497</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>YearMade</td>\n      <td>0.002955</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ModelID</td>\n      <td>0.001058</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fiProductClassDesc</td>\n      <td>0.000660</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLet's use a technique we used above, and see what the performance of our model would be when one of these columns is removed. \n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nm = rf(xs_final, y)\nprint('orig', m_rmse(m, valid_xs_final, valid_y))\n\nfor c in ('SalesID','saleElapsed','MachineID'):\n    m = rf(xs_final.drop(c,axis=1), y)\n    print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\norig 0.232237\nSalesID 0.229874\nsaleElapsed 0.234587\nMachineID 0.230984\n```\n:::\n:::\n\n\nWe'll drop the 'SalesID' and 'MachineID' columns, which actually improves the accuracy of our model. \n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\ntime_vars = ['SalesID','MachineID']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\nm = rf(xs_final_time, y)\nm_rmse(m, valid_xs_time, valid_y)\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```\n0.228792\n```\n:::\n:::\n\n\nAnother thing that can help is to make sure we're only using relevant data. Since it seems like the age of the bulldozer is a very important feature to our predictions, we'll actually just use a subset of our data, the most recent 20 years or so, to train our model so that any outdated relationships aren't captured in our dataset. \n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nxs['saleYear'].hist();\nfilt = xs['saleYear']>2004\nxs_filt = xs_final_time[filt]\ny_filt = y[filt]\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-39-output-1.png){width=600 height=414}\n:::\n:::\n\n\nThis model trained on only data from after 2004 is performing better than anything else we've seen so far. \n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nm = rf(xs_filt, y_filt)\nm_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\n(0.178646, 0.229726)\n```\n:::\n:::\n\n\nAlright, would we really be data scientists if we didn't throw a neural network at everything? Let's see how they can help. Let's pre-process our data the same way we did before passing it into the TabularPandas object, and also use only the subset of columns that we've analyzed to be most relevant. \n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\ndf_nn = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf_nn['ProductSize'] = df_nn['ProductSize'].astype('category')\ndf_nn['ProductSize'].cat.set_categories(sizes, ordered=True)\ndf_nn[dep_var] = np.log(df_nn[dep_var])\ndf_nn = add_datepart(df_nn, 'saledate')\ndf_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/alexander/anaconda3/envs/.venv-dlb/lib/python3.11/site-packages/fastai/tabular/core.py:23: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n```\n:::\n:::\n\n\nWe'll also have to handle categorical and continous variables differently with neural networks, because we can use embeddings here. We'll use the cont_cat_split function and pass in a 'max_card' argument, which will treat variables as categorical if there are less levels than max_card. \n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\ncont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\n```\n:::\n\n\nOur 'saleElapsed' column is treated as a continous variable, which is what we want because we wouldn't be able to extrapolate for sales in the future if it was a categorical variable. \n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\ncont_nn\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\n['saleElapsed']\n```\n:::\n:::\n\n\nIt looks like we have two categories with a lot of unique values. fiModelDesc has over 5000 and sound sjust like fiModelDescriptor. They literally just sound redundant, so let's see if we can get away with removing one of them (hint: it won't)\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\ndf_nn_final[cat_nn].nunique()\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\nYearMade                73\nCoupler_System           2\nProductSize              6\nfiProductClassDesc      74\nModelID               5281\nfiSecondaryDesc        177\nHydraulics_Flow          3\nEnclosure                6\nfiModelDesc           5059\nfiModelDescriptor      140\nDrive_System             4\nHydraulics              12\nTrack_Type               2\nProductGroup             6\ndtype: int64\n```\n:::\n:::\n\n\nHint: it didn't\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\nxs_filt2 = xs_filt.drop('fiModelDescriptor', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('fiModelDescriptor', axis=1)\nm2 = rf(xs_filt2, y_filt)\nprint(m_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y))\n\ncat_nn.remove('fiModelDescriptor')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.177728 0.230042\n```\n:::\n:::\n\n\nNow that we've done some exploration and refinement of our dataset we can create a TabularPandas object to handle the rest of our processing. We'll use a large batch size because tabular models are generally GPU RAM friendly. \n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,\n                      splits=splits, y_names=dep_var)\n\ndls = to_nn.dataloaders(1024)\n```\n:::\n\n\nWe'll create our tabular model with the tabular_learner class. We specify a y_range for our regression model, based on the max and mins we see in the training set. Since we have a lot of data, we'll make our layers a bit larger, and use MSE as our loss function as defined in the Kaggle competition. \n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\ny = to_nn.train.y\nprint(y.min(),y.max())\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n8.465899 11.863583\n```\n:::\n:::\n\n\nUsing the lr_find() method to get our optimal learning rate, let's train the model for a few epochs.\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\nlearn.lr_find()\nlearn.fit_one_cycle(5, 1e-2)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.072696</td>\n      <td>0.067694</td>\n      <td>00:06</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.055061</td>\n      <td>0.069597</td>\n      <td>00:06</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.048782</td>\n      <td>0.068780</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.044022</td>\n      <td>0.051267</td>\n      <td>00:06</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.041040</td>\n      <td>0.050207</td>\n      <td>00:06</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-48-output-5.png){width=596 height=436}\n:::\n:::\n\n\nUsing it for predictions, we can see that it does a better job than our random forest. The trade off is that it took longer to train, and we need to do some more work to find the hyperparameters that allowed it to perform so well. \n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\npreds,targs = learn.get_preds()\nlearn.save('nn') #save model incase we want to come back to it\nprint(r_mse(preds,targs))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n0.224069\n```\n:::\n:::\n\n\nLet's take ensembling even further. Our random forest was an ensemble of decision trees, but there's no rules saying that we have to use the same architecture model architecture. We'll combine our already ensembled model, the random forest, and our neural network and create an enseensemblemble.\n\nWe'll need to convert their outputs to the same shape and object type, but after doing that we can see the average of the predictions from these two models is better than either of them separately. \n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\nrf_preds = m.predict(valid_xs_time)\nens_preds = (to_np(preds.squeeze()) + rf_preds) /2\nr_mse(ens_preds,valid_y)\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```\n0.22146\n```\n:::\n:::\n\n\nLet's quickly talk about another decision tree ensemble - gradient boosting machines. Instead of taking the average of a bunch of predictions like in random forest, with gradient boosting our final prediction will come from the sum of our set of predictions. It works like so:\n\n1. Train a small model that will inevitably underfit the data set\n2. Calculate the predictions in the training set for this model\n3. Subtract the predictions from the targets to get our 'residuals' which is our error for each of our data points\n4. Repeat steps 1 - 3, but instead of using our original targets, we now target our residuals calculated from the last iteration instead\n5. Continue doing this until you reach some stopping criterion e.g. a max number of trees is created, or our validation error is getting worse. \n\nBy doing this, we're sequentially trying to build another tree that will bridge the gap that the last one had between the predictions and target (which are our residuals after the first tree), over and over again.\n\nThen, to get our final prediction, we sum up all of the predictions we get from the trees in our ensemble. \n\nLet's finish of this entry with some more of our favorite thing: text explanations. We actually ended up doing a lot of different, useful things this notebook and I want to recap them.\n\nWe took a look at 3 different models:\n\n1. Random Forests:\n    Pros: \n     - Fast to train\n     - Easy to get started with by requiring minimal preprocessing or hyperparameter tuning\n     - Resilient to overfitting with enough trees\n     - Can provide insight into data with partial dependence analysis or feature importance\n    Cons:\n     - May be less accurate than other approaches, and if extrapolation is needed then thats a problem\n2. Neural Networks:\n    Pros:\n    - Good results & extrapolate well\n    - Can use parts (embedding layer) in other models even if the predictions/rest of the layers aren't\n    Cons:\n    - Long to train\n    - Finnicky -> requires more preprocessing and hyperparameter tuning to get good performance\n    - Can overfit\n3. Gradient Boosting:\n    Pros:\n    - Fast to train (about the same as random forest)\n    - Can be more accurate than random forest\n    Cons:\n    - Require hyperparameter tuning\n    - Can overfit\n\n\nAnd also did some important work in data exploration and processing our data set to improve our results, and understanding of our data. \n\nWe:\n- Look at feature importance to determine our most impact columns and remove the least import ones\n- Removed redundant columns\n- Deal with out-of-domain data issues by filtering our data set to try and get our training and validation set to have\n- Did partial dependence to see the impact one independent variable has on our dependent variable \n\nAnd we're able to do all of that from the starting off by just training a model and seeing where it could take us. These have been useful techniques in improving our understanding of the data, and we were able to make use of them by just jumping in and getting our bearings from where we landed. We literally modeled first, and asked questions later (and then modeled again some more). Having a bias towards action, is a good attitude to have when in deep learning. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}