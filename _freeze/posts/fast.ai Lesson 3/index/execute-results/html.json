{
  "hash": "6707a42b4f9cfb84e90db1e7ec3f5eb4",
  "result": {
    "markdown": "---\ntitle: 'Practical Deep Learning for Coders: Lesson 3 Notes'\nauthor: Alex Liu\ndate: '2023-10-17'\ncategories:\n  - fast.ai\n  - Neural Networks\n  - Gradient Descent\n---\n\nOkay let's go finally learning about what everyone want's to know about when learning about AI and Machine Learning: stochastic gradient descent.\n\nThe first two lesson's of this course had us familiarize ourselves with what a deep learning application looks like without really getting into any of the theory or inner workings. We used the fastai library that provides a layer of abstraction for the techniques used in fine-tuning and training models (as it should). Now we get into components of machine learning that so many other MOOCs start with right off the bat - loss functions and gradient descent. It was only inevitable.  \n\nAnyways, let's get on with the notebook. The example we're working through is an image classification model that can classify any image as a 3 or a 7. Or you could just go on r/truerateme for that.\n\nAlright let's import all our standard fastbook and fastai libraries.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport sys\n!{sys.executable} -m pip install fastbook\n\n%matplotlib inline\n\nimport fastbook\nfastbook.setup_book()\nfrom fastai.vision.all import *\nfrom fastbook import *\nmatplotlib.rc('image', cmap='Greys')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRequirement already satisfied: fastbook in /opt/anaconda3/lib/python3.8/site-packages (0.0.29)\r\nRequirement already satisfied: fastai>=2.6 in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (2.7.13)\r\nRequirement already satisfied: datasets in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (2.14.5)\r\nRequirement already satisfied: ipywidgets<8 in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (7.6.3)\r\nRequirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (2.25.1)\r\nRequirement already satisfied: pip in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (21.0.1)\r\nRequirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (0.1.99)\r\nRequirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (1.2.4)\r\nRequirement already satisfied: transformers in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (4.34.0)\r\nRequirement already satisfied: graphviz in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (0.20.1)\r\nRequirement already satisfied: packaging in /opt/anaconda3/lib/python3.8/site-packages (from fastbook) (20.9)\r\nRequirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (from fastai>=2.6->fastbook) (0.24.1)\r\nRequirement already satisfied: spacy<4 in /opt/anaconda3/lib/python3.8/site-packages (from fastai>=2.6->fastbook) (3.7.2)\r\nRequirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.8/site-packages (from fastai>=2.6->fastbook) (5.4.1)\r\nRequirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from fastai>=2.6->fastbook) (10.1.0)\r\nRequirement already satisfied: fastcore<1.6,>=1.5.29 in /opt/anaconda3/lib/python3.8/site-packages (from fastai>=2.6->fastbook) (1.5.29)\r\nRequirement already satisfied: fastdownload<2,>=0.0.5 in /opt/anaconda3/lib/python3.8/site-packages (from fastai>=2.6->fastbook) (0.0.7)\r\nRequirement already satisfied: torch<2.2,>=1.10 in /opt/anaconda3/lib/python3.8/site-packages (from fastai>=2.6->fastbook) (2.1.0)\r\nRequirement already satisfied: torchvision>=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from fastai>=2.6->fastbook) (0.16.0)\r\nRequirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.8/site-packages (from fastai>=2.6->fastbook) (3.3.4)\r\nRequirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from fastai>=2.6->fastbook) (1.6.2)\r\nRequirement already satisfied: fastprogress>=0.2.4 in /opt/anaconda3/lib/python3.8/site-packages (from fastai>=2.6->fastbook) (1.0.3)\r\nRequirement already satisfied: ipykernel>=4.5.1 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets<8->fastbook) (5.3.4)\r\nRequirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets<8->fastbook) (5.0.5)\r\nRequirement already satisfied: widgetsnbextension~=3.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets<8->fastbook) (3.5.1)\r\nRequirement already satisfied: ipython>=4.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets<8->fastbook) (7.22.0)\r\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets<8->fastbook) (1.0.0)\r\nRequirement already satisfied: nbformat>=4.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipywidgets<8->fastbook) (5.1.3)\r\nRequirement already satisfied: tornado>=4.2 in /opt/anaconda3/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets<8->fastbook) (6.1)\r\nRequirement already satisfied: appnope in /opt/anaconda3/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets<8->fastbook) (0.1.2)\r\nRequirement already satisfied: jupyter-client in /opt/anaconda3/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets<8->fastbook) (6.1.12)\r\nRequirement already satisfied: pygments in /opt/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (2.8.1)\r\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (3.0.17)\r\nRequirement already satisfied: decorator in /opt/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (5.0.6)\r\nRequirement already satisfied: setuptools>=18.5 in /opt/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (52.0.0.post20210125)\r\nRequirement already satisfied: backcall in /opt/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (0.2.0)\r\nRequirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (4.8.0)\r\nRequirement already satisfied: pickleshare in /opt/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (0.7.5)\r\nRequirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets<8->fastbook) (0.17.2)\r\nRequirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets<8->fastbook) (0.7.0)\r\nRequirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/anaconda3/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets<8->fastbook) (3.2.0)\r\nRequirement already satisfied: jupyter-core in /opt/anaconda3/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets<8->fastbook) (4.7.1)\r\nRequirement already satisfied: ipython-genutils in /opt/anaconda3/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets<8->fastbook) (0.2.0)\r\nRequirement already satisfied: pyrsistent>=0.14.0 in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8->fastbook) (0.17.3)\r\nRequirement already satisfied: six>=1.11.0 in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8->fastbook) (1.15.0)\r\nRequirement already satisfied: attrs>=17.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<8->fastbook) (20.3.0)\r\nRequirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets<8->fastbook) (0.7.0)\r\nRequirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<8->fastbook) (0.2.5)\r\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (2.0.8)\r\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (0.3.3)\r\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (6.4.0)\r\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (2.4.2)\r\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (1.0.5)\r\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (1.0.10)\r\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (4.66.1)\r\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (1.1.2)\r\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (8.2.1)\r\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (2.0.10)\r\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (0.9.0)\r\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (3.0.12)\r\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (2.4.8)\r\nRequirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (1.20.1)\r\nRequirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (2.11.3)\r\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (3.3.0)\r\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<4->fastai>=2.6->fastbook) (3.0.9)\r\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging->fastbook) (2.4.7)\r\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai>=2.6->fastbook) (0.6.0)\r\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai>=2.6->fastbook) (4.8.0)\r\nRequirement already satisfied: pydantic-core==2.10.1 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai>=2.6->fastbook) (2.10.1)\r\nRequirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->fastbook) (2.10)\r\nRequirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests->fastbook) (2020.12.5)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests->fastbook) (1.26.4)\r\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests->fastbook) (4.0.0)\r\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4->fastai>=2.6->fastbook) (0.7.11)\r\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4->fastai>=2.6->fastbook) (0.1.3)\r\nRequirement already satisfied: filelock in /opt/anaconda3/lib/python3.8/site-packages (from torch<2.2,>=1.10->fastai>=2.6->fastbook) (3.0.12)\r\nRequirement already satisfied: sympy in /opt/anaconda3/lib/python3.8/site-packages (from torch<2.2,>=1.10->fastai>=2.6->fastbook) (1.8)\r\nRequirement already satisfied: fsspec in /opt/anaconda3/lib/python3.8/site-packages (from torch<2.2,>=1.10->fastai>=2.6->fastbook) (2023.6.0)\r\nRequirement already satisfied: networkx in /opt/anaconda3/lib/python3.8/site-packages (from torch<2.2,>=1.10->fastai>=2.6->fastbook) (2.5)\r\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy<4->fastai>=2.6->fastbook) (7.1.2)\r\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/anaconda3/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy<4->fastai>=2.6->fastbook) (0.16.0)\r\nRequirement already satisfied: notebook>=4.4.1 in /opt/anaconda3/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (6.3.0)\r\nRequirement already satisfied: prometheus-client in /opt/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (0.10.1)\r\nRequirement already satisfied: argon2-cffi in /opt/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (20.1.0)\r\nRequirement already satisfied: terminado>=0.8.3 in /opt/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (0.9.4)\r\nRequirement already satisfied: nbconvert in /opt/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (6.0.7)\r\nRequirement already satisfied: pyzmq>=17 in /opt/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (20.0.0)\r\nRequirement already satisfied: Send2Trash>=1.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (1.5.0)\r\nRequirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/lib/python3.8/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<8->fastbook) (2.8.1)\r\nRequirement already satisfied: cffi>=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (1.14.5)\r\nRequirement already satisfied: pycparser in /opt/anaconda3/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (2.20)\r\nRequirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.8/site-packages (from datasets->fastbook) (0.70.15)\r\nRequirement already satisfied: xxhash in /opt/anaconda3/lib/python3.8/site-packages (from datasets->fastbook) (3.4.1)\r\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from datasets->fastbook) (0.3.7)\r\nRequirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.8/site-packages (from datasets->fastbook) (3.8.4)\r\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from datasets->fastbook) (13.0.0)\r\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/anaconda3/lib/python3.8/site-packages (from datasets->fastbook) (0.17.3)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets->fastbook) (6.0.4)\r\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets->fastbook) (4.0.2)\r\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets->fastbook) (1.3.1)\r\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets->fastbook) (1.3.3)\r\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets->fastbook) (3.0.1)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets->fastbook) (1.8.2)\r\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<4->fastai>=2.6->fastbook) (1.1.1)\r\nRequirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->fastai>=2.6->fastbook) (0.10.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->fastai>=2.6->fastbook) (1.3.1)\r\nRequirement already satisfied: jupyterlab-pygments in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (0.1.2)\r\nRequirement already satisfied: entrypoints>=0.2.2 in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (0.3)\r\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (0.8.4)\r\nRequirement already satisfied: testpath in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (0.4.4)\r\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (0.5.3)\r\nRequirement already satisfied: bleach in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (3.3.0)\r\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (1.4.3)\r\nRequirement already satisfied: defusedxml in /opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (0.7.1)\r\nRequirement already satisfied: async-generator in /opt/anaconda3/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (1.10)\r\nRequirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (1.5.1)\r\nRequirement already satisfied: webencodings in /opt/anaconda3/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8->fastbook) (0.5.1)\r\nRequirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas->fastbook) (2021.1)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->fastai>=2.6->fastbook) (2.1.0)\r\nRequirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->fastai>=2.6->fastbook) (1.0.1)\r\nRequirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.8/site-packages (from sympy->torch<2.2,>=1.10->fastai>=2.6->fastbook) (1.2.1)\r\nRequirement already satisfied: safetensors>=0.3.1 in /opt/anaconda3/lib/python3.8/site-packages (from transformers->fastbook) (0.4.0)\r\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /opt/anaconda3/lib/python3.8/site-packages (from transformers->fastbook) (0.14.1)\r\nRequirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.8/site-packages (from transformers->fastbook) (2021.4.4)\r\n```\n:::\n:::\n\n\nThen we'll download a our sample from MNIST containing images of 3s and 7s, and use the .ls method to put these items into a special fastai list that also displays the amount of items in the list. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\npath = untar_data(URLs.MNIST_SAMPLE)\nPath.BASE_PATH = path\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\npath.ls()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n(#3) [Path('valid'),Path('labels.csv'),Path('train')]\n```\n:::\n:::\n\n\nWhy don't we take a look at one of them. Oh yeah that's a 3 all right. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nim3_path = threes[1]\nim3 = Image.open(im3_path)\nim3\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n![](index_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nThen we'll turn the image into a 2-d array where the values represent the darkness of pixel. We then put this into put this into a dataframe with some condotional formatting to visualize that pixel's darkness value...for some reason. It's cool I guess\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nim3_t = tensor(im3)\ndf = pd.DataFrame(im3_t[4:15,4:22])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<style  type=\"text/css\" >\n#T_31d93_row0_col0,#T_31d93_row0_col1,#T_31d93_row0_col2,#T_31d93_row0_col3,#T_31d93_row0_col4,#T_31d93_row0_col5,#T_31d93_row0_col6,#T_31d93_row0_col7,#T_31d93_row0_col8,#T_31d93_row0_col9,#T_31d93_row0_col10,#T_31d93_row0_col11,#T_31d93_row0_col12,#T_31d93_row0_col13,#T_31d93_row0_col14,#T_31d93_row0_col15,#T_31d93_row0_col16,#T_31d93_row0_col17,#T_31d93_row1_col0,#T_31d93_row1_col1,#T_31d93_row1_col2,#T_31d93_row1_col3,#T_31d93_row1_col4,#T_31d93_row1_col15,#T_31d93_row1_col16,#T_31d93_row1_col17,#T_31d93_row2_col0,#T_31d93_row2_col1,#T_31d93_row2_col2,#T_31d93_row2_col15,#T_31d93_row2_col16,#T_31d93_row2_col17,#T_31d93_row3_col0,#T_31d93_row3_col15,#T_31d93_row3_col16,#T_31d93_row3_col17,#T_31d93_row4_col0,#T_31d93_row4_col6,#T_31d93_row4_col7,#T_31d93_row4_col8,#T_31d93_row4_col9,#T_31d93_row4_col10,#T_31d93_row4_col15,#T_31d93_row4_col16,#T_31d93_row4_col17,#T_31d93_row5_col0,#T_31d93_row5_col5,#T_31d93_row5_col6,#T_31d93_row5_col7,#T_31d93_row5_col8,#T_31d93_row5_col9,#T_31d93_row5_col15,#T_31d93_row5_col16,#T_31d93_row5_col17,#T_31d93_row6_col0,#T_31d93_row6_col1,#T_31d93_row6_col2,#T_31d93_row6_col3,#T_31d93_row6_col4,#T_31d93_row6_col5,#T_31d93_row6_col6,#T_31d93_row6_col7,#T_31d93_row6_col8,#T_31d93_row6_col9,#T_31d93_row6_col14,#T_31d93_row6_col15,#T_31d93_row6_col16,#T_31d93_row6_col17,#T_31d93_row7_col0,#T_31d93_row7_col1,#T_31d93_row7_col2,#T_31d93_row7_col3,#T_31d93_row7_col4,#T_31d93_row7_col5,#T_31d93_row7_col6,#T_31d93_row7_col13,#T_31d93_row7_col14,#T_31d93_row7_col15,#T_31d93_row7_col16,#T_31d93_row7_col17,#T_31d93_row8_col0,#T_31d93_row8_col1,#T_31d93_row8_col2,#T_31d93_row8_col3,#T_31d93_row8_col4,#T_31d93_row8_col13,#T_31d93_row8_col14,#T_31d93_row8_col15,#T_31d93_row8_col16,#T_31d93_row8_col17,#T_31d93_row9_col0,#T_31d93_row9_col1,#T_31d93_row9_col2,#T_31d93_row9_col3,#T_31d93_row9_col4,#T_31d93_row9_col16,#T_31d93_row9_col17,#T_31d93_row10_col0,#T_31d93_row10_col1,#T_31d93_row10_col2,#T_31d93_row10_col3,#T_31d93_row10_col4,#T_31d93_row10_col5,#T_31d93_row10_col6,#T_31d93_row10_col17{\n            font-size:  6pt;\n            background-color:  #ffffff;\n            color:  #000000;\n        }#T_31d93_row1_col5{\n            font-size:  6pt;\n            background-color:  #efefef;\n            color:  #000000;\n        }#T_31d93_row1_col6,#T_31d93_row1_col13{\n            font-size:  6pt;\n            background-color:  #7c7c7c;\n            color:  #000000;\n        }#T_31d93_row1_col7{\n            font-size:  6pt;\n            background-color:  #4a4a4a;\n            color:  #f1f1f1;\n        }#T_31d93_row1_col8,#T_31d93_row1_col9,#T_31d93_row1_col10,#T_31d93_row2_col5,#T_31d93_row2_col6,#T_31d93_row2_col7,#T_31d93_row2_col11,#T_31d93_row2_col12,#T_31d93_row2_col13,#T_31d93_row3_col4,#T_31d93_row3_col12,#T_31d93_row3_col13,#T_31d93_row4_col1,#T_31d93_row4_col2,#T_31d93_row4_col3,#T_31d93_row4_col12,#T_31d93_row4_col13,#T_31d93_row5_col12,#T_31d93_row6_col11,#T_31d93_row9_col11,#T_31d93_row10_col11,#T_31d93_row10_col12,#T_31d93_row10_col13,#T_31d93_row10_col14,#T_31d93_row10_col15,#T_31d93_row10_col16{\n            font-size:  6pt;\n            background-color:  #000000;\n            color:  #f1f1f1;\n        }#T_31d93_row1_col11{\n            font-size:  6pt;\n            background-color:  #606060;\n            color:  #f1f1f1;\n        }#T_31d93_row1_col12{\n            font-size:  6pt;\n            background-color:  #4d4d4d;\n            color:  #f1f1f1;\n        }#T_31d93_row1_col14{\n            font-size:  6pt;\n            background-color:  #bbbbbb;\n            color:  #000000;\n        }#T_31d93_row2_col3{\n            font-size:  6pt;\n            background-color:  #e4e4e4;\n            color:  #000000;\n        }#T_31d93_row2_col4,#T_31d93_row8_col6{\n            font-size:  6pt;\n            background-color:  #6b6b6b;\n            color:  #000000;\n        }#T_31d93_row2_col8,#T_31d93_row2_col14,#T_31d93_row3_col14{\n            font-size:  6pt;\n            background-color:  #171717;\n            color:  #f1f1f1;\n        }#T_31d93_row2_col9,#T_31d93_row3_col11{\n            font-size:  6pt;\n            background-color:  #4b4b4b;\n            color:  #f1f1f1;\n        }#T_31d93_row2_col10,#T_31d93_row7_col10,#T_31d93_row8_col8,#T_31d93_row8_col10,#T_31d93_row9_col8,#T_31d93_row9_col10{\n            font-size:  6pt;\n            background-color:  #010101;\n            color:  #f1f1f1;\n        }#T_31d93_row3_col1{\n            font-size:  6pt;\n            background-color:  #272727;\n            color:  #f1f1f1;\n        }#T_31d93_row3_col2{\n            font-size:  6pt;\n            background-color:  #0a0a0a;\n            color:  #f1f1f1;\n        }#T_31d93_row3_col3{\n            font-size:  6pt;\n            background-color:  #050505;\n            color:  #f1f1f1;\n        }#T_31d93_row3_col5{\n            font-size:  6pt;\n            background-color:  #333333;\n            color:  #f1f1f1;\n        }#T_31d93_row3_col6{\n            font-size:  6pt;\n            background-color:  #e6e6e6;\n            color:  #000000;\n        }#T_31d93_row3_col7,#T_31d93_row3_col10{\n            font-size:  6pt;\n            background-color:  #fafafa;\n            color:  #000000;\n        }#T_31d93_row3_col8{\n            font-size:  6pt;\n            background-color:  #fbfbfb;\n            color:  #000000;\n        }#T_31d93_row3_col9{\n            font-size:  6pt;\n            background-color:  #fdfdfd;\n            color:  #000000;\n        }#T_31d93_row4_col4{\n            font-size:  6pt;\n            background-color:  #1b1b1b;\n            color:  #f1f1f1;\n        }#T_31d93_row4_col5{\n            font-size:  6pt;\n            background-color:  #e0e0e0;\n            color:  #000000;\n        }#T_31d93_row4_col11{\n            font-size:  6pt;\n            background-color:  #4e4e4e;\n            color:  #f1f1f1;\n        }#T_31d93_row4_col14{\n            font-size:  6pt;\n            background-color:  #767676;\n            color:  #000000;\n        }#T_31d93_row5_col1{\n            font-size:  6pt;\n            background-color:  #fcfcfc;\n            color:  #000000;\n        }#T_31d93_row5_col2,#T_31d93_row5_col3{\n            font-size:  6pt;\n            background-color:  #f6f6f6;\n            color:  #000000;\n        }#T_31d93_row5_col4,#T_31d93_row7_col7{\n            font-size:  6pt;\n            background-color:  #f8f8f8;\n            color:  #000000;\n        }#T_31d93_row5_col10,#T_31d93_row10_col7{\n            font-size:  6pt;\n            background-color:  #e8e8e8;\n            color:  #000000;\n        }#T_31d93_row5_col11{\n            font-size:  6pt;\n            background-color:  #222222;\n            color:  #f1f1f1;\n        }#T_31d93_row5_col13,#T_31d93_row6_col12{\n            font-size:  6pt;\n            background-color:  #090909;\n            color:  #f1f1f1;\n        }#T_31d93_row5_col14{\n            font-size:  6pt;\n            background-color:  #d0d0d0;\n            color:  #000000;\n        }#T_31d93_row6_col10,#T_31d93_row7_col11,#T_31d93_row9_col6{\n            font-size:  6pt;\n            background-color:  #060606;\n            color:  #f1f1f1;\n        }#T_31d93_row6_col13{\n            font-size:  6pt;\n            background-color:  #979797;\n            color:  #000000;\n        }#T_31d93_row7_col8{\n            font-size:  6pt;\n            background-color:  #b6b6b6;\n            color:  #000000;\n        }#T_31d93_row7_col9{\n            font-size:  6pt;\n            background-color:  #252525;\n            color:  #f1f1f1;\n        }#T_31d93_row7_col12{\n            font-size:  6pt;\n            background-color:  #999999;\n            color:  #000000;\n        }#T_31d93_row8_col5{\n            font-size:  6pt;\n            background-color:  #f9f9f9;\n            color:  #000000;\n        }#T_31d93_row8_col7{\n            font-size:  6pt;\n            background-color:  #101010;\n            color:  #f1f1f1;\n        }#T_31d93_row8_col9,#T_31d93_row9_col9{\n            font-size:  6pt;\n            background-color:  #020202;\n            color:  #f1f1f1;\n        }#T_31d93_row8_col11{\n            font-size:  6pt;\n            background-color:  #545454;\n            color:  #f1f1f1;\n        }#T_31d93_row8_col12{\n            font-size:  6pt;\n            background-color:  #f1f1f1;\n            color:  #000000;\n        }#T_31d93_row9_col5{\n            font-size:  6pt;\n            background-color:  #f7f7f7;\n            color:  #000000;\n        }#T_31d93_row9_col7{\n            font-size:  6pt;\n            background-color:  #030303;\n            color:  #f1f1f1;\n        }#T_31d93_row9_col12{\n            font-size:  6pt;\n            background-color:  #181818;\n            color:  #f1f1f1;\n        }#T_31d93_row9_col13{\n            font-size:  6pt;\n            background-color:  #303030;\n            color:  #f1f1f1;\n        }#T_31d93_row9_col14{\n            font-size:  6pt;\n            background-color:  #a9a9a9;\n            color:  #000000;\n        }#T_31d93_row9_col15{\n            font-size:  6pt;\n            background-color:  #fefefe;\n            color:  #000000;\n        }#T_31d93_row10_col8,#T_31d93_row10_col9{\n            font-size:  6pt;\n            background-color:  #bababa;\n            color:  #000000;\n        }#T_31d93_row10_col10{\n            font-size:  6pt;\n            background-color:  #393939;\n            color:  #f1f1f1;\n        }</style><table id=\"T_31d93_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>        <th class=\"col_heading level0 col5\" >5</th>        <th class=\"col_heading level0 col6\" >6</th>        <th class=\"col_heading level0 col7\" >7</th>        <th class=\"col_heading level0 col8\" >8</th>        <th class=\"col_heading level0 col9\" >9</th>        <th class=\"col_heading level0 col10\" >10</th>        <th class=\"col_heading level0 col11\" >11</th>        <th class=\"col_heading level0 col12\" >12</th>        <th class=\"col_heading level0 col13\" >13</th>        <th class=\"col_heading level0 col14\" >14</th>        <th class=\"col_heading level0 col15\" >15</th>        <th class=\"col_heading level0 col16\" >16</th>        <th class=\"col_heading level0 col17\" >17</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_31d93_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n                        <td id=\"T_31d93_row0_col0\" class=\"data row0 col0\" >0</td>\n                        <td id=\"T_31d93_row0_col1\" class=\"data row0 col1\" >0</td>\n                        <td id=\"T_31d93_row0_col2\" class=\"data row0 col2\" >0</td>\n                        <td id=\"T_31d93_row0_col3\" class=\"data row0 col3\" >0</td>\n                        <td id=\"T_31d93_row0_col4\" class=\"data row0 col4\" >0</td>\n                        <td id=\"T_31d93_row0_col5\" class=\"data row0 col5\" >0</td>\n                        <td id=\"T_31d93_row0_col6\" class=\"data row0 col6\" >0</td>\n                        <td id=\"T_31d93_row0_col7\" class=\"data row0 col7\" >0</td>\n                        <td id=\"T_31d93_row0_col8\" class=\"data row0 col8\" >0</td>\n                        <td id=\"T_31d93_row0_col9\" class=\"data row0 col9\" >0</td>\n                        <td id=\"T_31d93_row0_col10\" class=\"data row0 col10\" >0</td>\n                        <td id=\"T_31d93_row0_col11\" class=\"data row0 col11\" >0</td>\n                        <td id=\"T_31d93_row0_col12\" class=\"data row0 col12\" >0</td>\n                        <td id=\"T_31d93_row0_col13\" class=\"data row0 col13\" >0</td>\n                        <td id=\"T_31d93_row0_col14\" class=\"data row0 col14\" >0</td>\n                        <td id=\"T_31d93_row0_col15\" class=\"data row0 col15\" >0</td>\n                        <td id=\"T_31d93_row0_col16\" class=\"data row0 col16\" >0</td>\n                        <td id=\"T_31d93_row0_col17\" class=\"data row0 col17\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_31d93_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n                        <td id=\"T_31d93_row1_col0\" class=\"data row1 col0\" >0</td>\n                        <td id=\"T_31d93_row1_col1\" class=\"data row1 col1\" >0</td>\n                        <td id=\"T_31d93_row1_col2\" class=\"data row1 col2\" >0</td>\n                        <td id=\"T_31d93_row1_col3\" class=\"data row1 col3\" >0</td>\n                        <td id=\"T_31d93_row1_col4\" class=\"data row1 col4\" >0</td>\n                        <td id=\"T_31d93_row1_col5\" class=\"data row1 col5\" >29</td>\n                        <td id=\"T_31d93_row1_col6\" class=\"data row1 col6\" >150</td>\n                        <td id=\"T_31d93_row1_col7\" class=\"data row1 col7\" >195</td>\n                        <td id=\"T_31d93_row1_col8\" class=\"data row1 col8\" >254</td>\n                        <td id=\"T_31d93_row1_col9\" class=\"data row1 col9\" >255</td>\n                        <td id=\"T_31d93_row1_col10\" class=\"data row1 col10\" >254</td>\n                        <td id=\"T_31d93_row1_col11\" class=\"data row1 col11\" >176</td>\n                        <td id=\"T_31d93_row1_col12\" class=\"data row1 col12\" >193</td>\n                        <td id=\"T_31d93_row1_col13\" class=\"data row1 col13\" >150</td>\n                        <td id=\"T_31d93_row1_col14\" class=\"data row1 col14\" >96</td>\n                        <td id=\"T_31d93_row1_col15\" class=\"data row1 col15\" >0</td>\n                        <td id=\"T_31d93_row1_col16\" class=\"data row1 col16\" >0</td>\n                        <td id=\"T_31d93_row1_col17\" class=\"data row1 col17\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_31d93_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n                        <td id=\"T_31d93_row2_col0\" class=\"data row2 col0\" >0</td>\n                        <td id=\"T_31d93_row2_col1\" class=\"data row2 col1\" >0</td>\n                        <td id=\"T_31d93_row2_col2\" class=\"data row2 col2\" >0</td>\n                        <td id=\"T_31d93_row2_col3\" class=\"data row2 col3\" >48</td>\n                        <td id=\"T_31d93_row2_col4\" class=\"data row2 col4\" >166</td>\n                        <td id=\"T_31d93_row2_col5\" class=\"data row2 col5\" >224</td>\n                        <td id=\"T_31d93_row2_col6\" class=\"data row2 col6\" >253</td>\n                        <td id=\"T_31d93_row2_col7\" class=\"data row2 col7\" >253</td>\n                        <td id=\"T_31d93_row2_col8\" class=\"data row2 col8\" >234</td>\n                        <td id=\"T_31d93_row2_col9\" class=\"data row2 col9\" >196</td>\n                        <td id=\"T_31d93_row2_col10\" class=\"data row2 col10\" >253</td>\n                        <td id=\"T_31d93_row2_col11\" class=\"data row2 col11\" >253</td>\n                        <td id=\"T_31d93_row2_col12\" class=\"data row2 col12\" >253</td>\n                        <td id=\"T_31d93_row2_col13\" class=\"data row2 col13\" >253</td>\n                        <td id=\"T_31d93_row2_col14\" class=\"data row2 col14\" >233</td>\n                        <td id=\"T_31d93_row2_col15\" class=\"data row2 col15\" >0</td>\n                        <td id=\"T_31d93_row2_col16\" class=\"data row2 col16\" >0</td>\n                        <td id=\"T_31d93_row2_col17\" class=\"data row2 col17\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_31d93_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n                        <td id=\"T_31d93_row3_col0\" class=\"data row3 col0\" >0</td>\n                        <td id=\"T_31d93_row3_col1\" class=\"data row3 col1\" >93</td>\n                        <td id=\"T_31d93_row3_col2\" class=\"data row3 col2\" >244</td>\n                        <td id=\"T_31d93_row3_col3\" class=\"data row3 col3\" >249</td>\n                        <td id=\"T_31d93_row3_col4\" class=\"data row3 col4\" >253</td>\n                        <td id=\"T_31d93_row3_col5\" class=\"data row3 col5\" >187</td>\n                        <td id=\"T_31d93_row3_col6\" class=\"data row3 col6\" >46</td>\n                        <td id=\"T_31d93_row3_col7\" class=\"data row3 col7\" >10</td>\n                        <td id=\"T_31d93_row3_col8\" class=\"data row3 col8\" >8</td>\n                        <td id=\"T_31d93_row3_col9\" class=\"data row3 col9\" >4</td>\n                        <td id=\"T_31d93_row3_col10\" class=\"data row3 col10\" >10</td>\n                        <td id=\"T_31d93_row3_col11\" class=\"data row3 col11\" >194</td>\n                        <td id=\"T_31d93_row3_col12\" class=\"data row3 col12\" >253</td>\n                        <td id=\"T_31d93_row3_col13\" class=\"data row3 col13\" >253</td>\n                        <td id=\"T_31d93_row3_col14\" class=\"data row3 col14\" >233</td>\n                        <td id=\"T_31d93_row3_col15\" class=\"data row3 col15\" >0</td>\n                        <td id=\"T_31d93_row3_col16\" class=\"data row3 col16\" >0</td>\n                        <td id=\"T_31d93_row3_col17\" class=\"data row3 col17\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_31d93_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n                        <td id=\"T_31d93_row4_col0\" class=\"data row4 col0\" >0</td>\n                        <td id=\"T_31d93_row4_col1\" class=\"data row4 col1\" >107</td>\n                        <td id=\"T_31d93_row4_col2\" class=\"data row4 col2\" >253</td>\n                        <td id=\"T_31d93_row4_col3\" class=\"data row4 col3\" >253</td>\n                        <td id=\"T_31d93_row4_col4\" class=\"data row4 col4\" >230</td>\n                        <td id=\"T_31d93_row4_col5\" class=\"data row4 col5\" >48</td>\n                        <td id=\"T_31d93_row4_col6\" class=\"data row4 col6\" >0</td>\n                        <td id=\"T_31d93_row4_col7\" class=\"data row4 col7\" >0</td>\n                        <td id=\"T_31d93_row4_col8\" class=\"data row4 col8\" >0</td>\n                        <td id=\"T_31d93_row4_col9\" class=\"data row4 col9\" >0</td>\n                        <td id=\"T_31d93_row4_col10\" class=\"data row4 col10\" >0</td>\n                        <td id=\"T_31d93_row4_col11\" class=\"data row4 col11\" >192</td>\n                        <td id=\"T_31d93_row4_col12\" class=\"data row4 col12\" >253</td>\n                        <td id=\"T_31d93_row4_col13\" class=\"data row4 col13\" >253</td>\n                        <td id=\"T_31d93_row4_col14\" class=\"data row4 col14\" >156</td>\n                        <td id=\"T_31d93_row4_col15\" class=\"data row4 col15\" >0</td>\n                        <td id=\"T_31d93_row4_col16\" class=\"data row4 col16\" >0</td>\n                        <td id=\"T_31d93_row4_col17\" class=\"data row4 col17\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_31d93_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n                        <td id=\"T_31d93_row5_col0\" class=\"data row5 col0\" >0</td>\n                        <td id=\"T_31d93_row5_col1\" class=\"data row5 col1\" >3</td>\n                        <td id=\"T_31d93_row5_col2\" class=\"data row5 col2\" >20</td>\n                        <td id=\"T_31d93_row5_col3\" class=\"data row5 col3\" >20</td>\n                        <td id=\"T_31d93_row5_col4\" class=\"data row5 col4\" >15</td>\n                        <td id=\"T_31d93_row5_col5\" class=\"data row5 col5\" >0</td>\n                        <td id=\"T_31d93_row5_col6\" class=\"data row5 col6\" >0</td>\n                        <td id=\"T_31d93_row5_col7\" class=\"data row5 col7\" >0</td>\n                        <td id=\"T_31d93_row5_col8\" class=\"data row5 col8\" >0</td>\n                        <td id=\"T_31d93_row5_col9\" class=\"data row5 col9\" >0</td>\n                        <td id=\"T_31d93_row5_col10\" class=\"data row5 col10\" >43</td>\n                        <td id=\"T_31d93_row5_col11\" class=\"data row5 col11\" >224</td>\n                        <td id=\"T_31d93_row5_col12\" class=\"data row5 col12\" >253</td>\n                        <td id=\"T_31d93_row5_col13\" class=\"data row5 col13\" >245</td>\n                        <td id=\"T_31d93_row5_col14\" class=\"data row5 col14\" >74</td>\n                        <td id=\"T_31d93_row5_col15\" class=\"data row5 col15\" >0</td>\n                        <td id=\"T_31d93_row5_col16\" class=\"data row5 col16\" >0</td>\n                        <td id=\"T_31d93_row5_col17\" class=\"data row5 col17\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_31d93_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n                        <td id=\"T_31d93_row6_col0\" class=\"data row6 col0\" >0</td>\n                        <td id=\"T_31d93_row6_col1\" class=\"data row6 col1\" >0</td>\n                        <td id=\"T_31d93_row6_col2\" class=\"data row6 col2\" >0</td>\n                        <td id=\"T_31d93_row6_col3\" class=\"data row6 col3\" >0</td>\n                        <td id=\"T_31d93_row6_col4\" class=\"data row6 col4\" >0</td>\n                        <td id=\"T_31d93_row6_col5\" class=\"data row6 col5\" >0</td>\n                        <td id=\"T_31d93_row6_col6\" class=\"data row6 col6\" >0</td>\n                        <td id=\"T_31d93_row6_col7\" class=\"data row6 col7\" >0</td>\n                        <td id=\"T_31d93_row6_col8\" class=\"data row6 col8\" >0</td>\n                        <td id=\"T_31d93_row6_col9\" class=\"data row6 col9\" >0</td>\n                        <td id=\"T_31d93_row6_col10\" class=\"data row6 col10\" >249</td>\n                        <td id=\"T_31d93_row6_col11\" class=\"data row6 col11\" >253</td>\n                        <td id=\"T_31d93_row6_col12\" class=\"data row6 col12\" >245</td>\n                        <td id=\"T_31d93_row6_col13\" class=\"data row6 col13\" >126</td>\n                        <td id=\"T_31d93_row6_col14\" class=\"data row6 col14\" >0</td>\n                        <td id=\"T_31d93_row6_col15\" class=\"data row6 col15\" >0</td>\n                        <td id=\"T_31d93_row6_col16\" class=\"data row6 col16\" >0</td>\n                        <td id=\"T_31d93_row6_col17\" class=\"data row6 col17\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_31d93_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n                        <td id=\"T_31d93_row7_col0\" class=\"data row7 col0\" >0</td>\n                        <td id=\"T_31d93_row7_col1\" class=\"data row7 col1\" >0</td>\n                        <td id=\"T_31d93_row7_col2\" class=\"data row7 col2\" >0</td>\n                        <td id=\"T_31d93_row7_col3\" class=\"data row7 col3\" >0</td>\n                        <td id=\"T_31d93_row7_col4\" class=\"data row7 col4\" >0</td>\n                        <td id=\"T_31d93_row7_col5\" class=\"data row7 col5\" >0</td>\n                        <td id=\"T_31d93_row7_col6\" class=\"data row7 col6\" >0</td>\n                        <td id=\"T_31d93_row7_col7\" class=\"data row7 col7\" >14</td>\n                        <td id=\"T_31d93_row7_col8\" class=\"data row7 col8\" >101</td>\n                        <td id=\"T_31d93_row7_col9\" class=\"data row7 col9\" >223</td>\n                        <td id=\"T_31d93_row7_col10\" class=\"data row7 col10\" >253</td>\n                        <td id=\"T_31d93_row7_col11\" class=\"data row7 col11\" >248</td>\n                        <td id=\"T_31d93_row7_col12\" class=\"data row7 col12\" >124</td>\n                        <td id=\"T_31d93_row7_col13\" class=\"data row7 col13\" >0</td>\n                        <td id=\"T_31d93_row7_col14\" class=\"data row7 col14\" >0</td>\n                        <td id=\"T_31d93_row7_col15\" class=\"data row7 col15\" >0</td>\n                        <td id=\"T_31d93_row7_col16\" class=\"data row7 col16\" >0</td>\n                        <td id=\"T_31d93_row7_col17\" class=\"data row7 col17\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_31d93_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n                        <td id=\"T_31d93_row8_col0\" class=\"data row8 col0\" >0</td>\n                        <td id=\"T_31d93_row8_col1\" class=\"data row8 col1\" >0</td>\n                        <td id=\"T_31d93_row8_col2\" class=\"data row8 col2\" >0</td>\n                        <td id=\"T_31d93_row8_col3\" class=\"data row8 col3\" >0</td>\n                        <td id=\"T_31d93_row8_col4\" class=\"data row8 col4\" >0</td>\n                        <td id=\"T_31d93_row8_col5\" class=\"data row8 col5\" >11</td>\n                        <td id=\"T_31d93_row8_col6\" class=\"data row8 col6\" >166</td>\n                        <td id=\"T_31d93_row8_col7\" class=\"data row8 col7\" >239</td>\n                        <td id=\"T_31d93_row8_col8\" class=\"data row8 col8\" >253</td>\n                        <td id=\"T_31d93_row8_col9\" class=\"data row8 col9\" >253</td>\n                        <td id=\"T_31d93_row8_col10\" class=\"data row8 col10\" >253</td>\n                        <td id=\"T_31d93_row8_col11\" class=\"data row8 col11\" >187</td>\n                        <td id=\"T_31d93_row8_col12\" class=\"data row8 col12\" >30</td>\n                        <td id=\"T_31d93_row8_col13\" class=\"data row8 col13\" >0</td>\n                        <td id=\"T_31d93_row8_col14\" class=\"data row8 col14\" >0</td>\n                        <td id=\"T_31d93_row8_col15\" class=\"data row8 col15\" >0</td>\n                        <td id=\"T_31d93_row8_col16\" class=\"data row8 col16\" >0</td>\n                        <td id=\"T_31d93_row8_col17\" class=\"data row8 col17\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_31d93_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n                        <td id=\"T_31d93_row9_col0\" class=\"data row9 col0\" >0</td>\n                        <td id=\"T_31d93_row9_col1\" class=\"data row9 col1\" >0</td>\n                        <td id=\"T_31d93_row9_col2\" class=\"data row9 col2\" >0</td>\n                        <td id=\"T_31d93_row9_col3\" class=\"data row9 col3\" >0</td>\n                        <td id=\"T_31d93_row9_col4\" class=\"data row9 col4\" >0</td>\n                        <td id=\"T_31d93_row9_col5\" class=\"data row9 col5\" >16</td>\n                        <td id=\"T_31d93_row9_col6\" class=\"data row9 col6\" >248</td>\n                        <td id=\"T_31d93_row9_col7\" class=\"data row9 col7\" >250</td>\n                        <td id=\"T_31d93_row9_col8\" class=\"data row9 col8\" >253</td>\n                        <td id=\"T_31d93_row9_col9\" class=\"data row9 col9\" >253</td>\n                        <td id=\"T_31d93_row9_col10\" class=\"data row9 col10\" >253</td>\n                        <td id=\"T_31d93_row9_col11\" class=\"data row9 col11\" >253</td>\n                        <td id=\"T_31d93_row9_col12\" class=\"data row9 col12\" >232</td>\n                        <td id=\"T_31d93_row9_col13\" class=\"data row9 col13\" >213</td>\n                        <td id=\"T_31d93_row9_col14\" class=\"data row9 col14\" >111</td>\n                        <td id=\"T_31d93_row9_col15\" class=\"data row9 col15\" >2</td>\n                        <td id=\"T_31d93_row9_col16\" class=\"data row9 col16\" >0</td>\n                        <td id=\"T_31d93_row9_col17\" class=\"data row9 col17\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_31d93_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n                        <td id=\"T_31d93_row10_col0\" class=\"data row10 col0\" >0</td>\n                        <td id=\"T_31d93_row10_col1\" class=\"data row10 col1\" >0</td>\n                        <td id=\"T_31d93_row10_col2\" class=\"data row10 col2\" >0</td>\n                        <td id=\"T_31d93_row10_col3\" class=\"data row10 col3\" >0</td>\n                        <td id=\"T_31d93_row10_col4\" class=\"data row10 col4\" >0</td>\n                        <td id=\"T_31d93_row10_col5\" class=\"data row10 col5\" >0</td>\n                        <td id=\"T_31d93_row10_col6\" class=\"data row10 col6\" >0</td>\n                        <td id=\"T_31d93_row10_col7\" class=\"data row10 col7\" >43</td>\n                        <td id=\"T_31d93_row10_col8\" class=\"data row10 col8\" >98</td>\n                        <td id=\"T_31d93_row10_col9\" class=\"data row10 col9\" >98</td>\n                        <td id=\"T_31d93_row10_col10\" class=\"data row10 col10\" >208</td>\n                        <td id=\"T_31d93_row10_col11\" class=\"data row10 col11\" >253</td>\n                        <td id=\"T_31d93_row10_col12\" class=\"data row10 col12\" >253</td>\n                        <td id=\"T_31d93_row10_col13\" class=\"data row10 col13\" >253</td>\n                        <td id=\"T_31d93_row10_col14\" class=\"data row10 col14\" >253</td>\n                        <td id=\"T_31d93_row10_col15\" class=\"data row10 col15\" >187</td>\n                        <td id=\"T_31d93_row10_col16\" class=\"data row10 col16\" >22</td>\n                        <td id=\"T_31d93_row10_col17\" class=\"data row10 col17\" >0</td>\n            </tr>\n    </tbody></table>\n```\n:::\n:::\n\n\nQuestion: How might a system that actually classifies 3s and 7s work? Well now we have a 28x28 grid of pixel values that visualizes a given digit. Perhaps we could create some grid that would represent a 'perfect' 3 and 7. A 7 would have a perfectly horizontal top line, maybe in row 2 or 3, that pivoted into a perfect slant as well. A 3 would have a couple of perfectly bulbous curves stacked ontop of one another. The values would be completely dark (with a value of 255) where the number is and 0 elsewhere. \n\nWe could then go cell by cell and find take the difference between the values that we're seeing in the digit we're trying to classify and our 'perfect' example, (and maybe even square those differences so that they're all positive, and larger differences are given more weight than smaller ones) and then sum them up to get a sense of how much difference there is between any given digit and our standardized example. \n\nInfact this is exactly what we're going to do. We'll create our representation of a 'perfect' 3 and 7 by averaging the values of each cell in our 28x28g grid for all of our 3s and 7s. We do this by using list comprehension to create a list of 2-D tensors that are all our examples of 3s or 7s. After that, we'll use the torch.stack method to put this into a tensor itself, along the 0th dimension, which will orient each cell in the 28x28 grid with the corresponding cell on other pictures. \n\nYou can imagine this as printing a picture of each 3/7 on a piece of paper, and stacking all those papers ontop of each other. We then use the .mean method to find the mean across the dimension we pass in as a parameter - 0, which means across all of our different examples of 3s/7s. This is like shining a bright light on our stack of papers from above and observing the general shape that appears through the opacity. \n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nseven_tensors = [tensor(Image.open(o)) for o in sevens]\nthree_tensors = [tensor(Image.open(o)) for o in threes]\nstacked_sevens = torch.stack(seven_tensors).float()/255\nstacked_threes = torch.stack(three_tensors).float()/255\nmean3 = stacked_threes.mean(0)\nshow_image(mean3);\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nYeah that looks like a pretty average 3. \n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\nAnd this looks like a pretty average 7. Good for us that this average 7 is actually perfect for us. \n\nNow we'll take the difference between the values in each cell/pixel for a given example to find how much it differs from our expectation of what a 3 or 7 should be. If we average these differences then it would offer us an idea of how different our digit is from our idea of a 3 or 7, and thus which one it is more likely to be. But just using the differences won't be good enough since it might be positive or negative (the digit might be darker than our average for any particular pixel or vice versa). \n\nTherefore if we use the absolute value of differences or square the differences before finding the average (and the taking the square root again to undo the square), it would make all our values positive. This is called the L1 and L2 norm respectively. \n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\na_3 = stacked_threes[1]\ndist_3_abs = (a_3 - mean3).abs().mean()\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\ndist_3_abs,dist_3_sqr\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n(tensor(0.1114), tensor(0.2021))\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){}\n:::\n:::\n\n\nDoing this we find the difference (l1 and l2 norm) of our digit to a 3\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndist_7_abs = (a_3 - mean7).abs().mean()\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\ndist_7_abs,dist_7_sqr\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n(tensor(0.1586), tensor(0.3021))\n```\n:::\n:::\n\n\nAnd the l1 and l2 norm of our digit to a 7. Since the norms are smaller for the 3, the interpretation is that it is less 'different' to our idea of a 3, thus more likely to be that digit, which is what we'll classify it as. \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nF.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n(tensor(0.1586), tensor(0.3021))\n```\n:::\n:::\n\n\nProtip: Do this a lot simpler with the l1_loss and mse_loss functions inside torch.nn.functional. These functions are usually used to optimize the way we generate predictions for a given sample, which we'll cover soon when going over gradient descent. \n\nIn this case, we're using the loss a little differently, as it is directly informing our prediction. Either way, is our measurement of the average difference between our prediction, and the actual value. In this case, our 'prediction' of a 3/7 is the average we've calculated, and finding the difference (l1/l2 norm) between this and the digit being classified (our actual value) is the loss, where the category with the smallest loss is the one classify our digit as. \n\nNow lets try and get a sense of how good this idea is. We'll take a set of our data specifically for the purpose of evaluating the performance of our model (the validation set) and make a bunch of predictions for the validation set. Since we know what the correct classification for the digits in the validation set is, we can just calculate the percentage we classify corectly. We'll create our validation set here:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nvalid_3_tens = torch.stack([tensor(Image.open(o)) \n                            for o in (path/'valid'/'3').ls()])\nvalid_3_tens = valid_3_tens.float()/255\nvalid_7_tens = torch.stack([tensor(Image.open(o)) \n                            for o in (path/'valid'/'7').ls()])\nvalid_7_tens = valid_7_tens.float()/255\nvalid_3_tens.shape,valid_7_tens.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n```\n:::\n:::\n\n\nWe'll also create a helper function to calculate the L1 norm of an image as compared to another image. This case, we are passing in the digit image being classified and our average 3. If you have a question about what the (-1,-2) argument in the mean() call is for, then I have just the thing for you. The thing for you being the answer to your question. \n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndef mnist_distance(a,b): return (a-b).abs().mean((-1,-2))\nmnist_distance(a_3, mean3)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\ntensor(0.1114)\n```\n:::\n:::\n\n\nNotice we can pass in an entire tensor of images and get back a tensor of L1 norms. To do this, PyTorch uses 'broadcasting'. This expands the tensor with a smaller rank to have the same size as the larger one, and then performs the operation on the tensors corresponding elements when they are the same size. \n\nFor this application, we have a rank-3 tensor (our list of 1000 or so rank-2 tensors which are our 28x28 images), and a rank-2 tensor, which is our 28x28 average digit. Broadcasting will create 1000 copies (in theory - in practice it doesn't actually literally allocate memory for 1000 copies) of this rank-2 tensor, and subtract it from each of our 1000 images to be classified. Then we call abs to make all of the values in our rank-3 tensor positive. Now, for each of the 1000 images in our set, we want to find the average difference for a given pixel compared to our 'average' 3/7. \n\nThis is essentially summing all 784 (28 * 28) pixel difference values and dividing it by 784. PyTorch lets us do this without loops by specifying the axes to take the mean on. By negative indexing with (-1,-2), we are saying take the mean on the last and second to last axes of this tensor, which are our rows and columns of pixels for each training image, leaving us with a rank-1 tensor of size 1000. Compare this to how we created our average image of a 3/7 by calling .mean(axis=0), which took the mean across all our images, leaving a rank-2 tensor of size [28,28], which is almost a complementary operation. \n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nvalid_3_dist = mnist_distance(valid_3_tens, mean3)\nvalid_3_dist, valid_3_dist.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n(tensor([0.1634, 0.1145, 0.1363,  ..., 0.1105, 0.1111, 0.1640]),\n torch.Size([1010]))\n```\n:::\n:::\n\n\nOkay, after that wall of text our next step is a little easier to understand. If our difference (l1 norm) for a 3 is larger than it is for a 7, it is 'farther away' from what a 3 should be than a 7. Thus, we will just calculate the l1 norms for 3 & 7, and provide an output based on what is lower. \n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndef is_3(x): return mnist_distance(x,mean3) < mnist_distance(x,mean7)\n```\n:::\n\n\nWe'll use broadcasting to apply this method to our validation set tensor. As it did in our mnist_distance(a,b) definition, it will expand the tensor with our average 3 and average 7 to the size of our x argument, which is our validation set, get the difference, absolute value, then average, and then perform an element-wise comparison on the values returned from both calls to mnist_distance()\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nis_3(valid_3_tens)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\ntensor([True, True, True,  ..., True, True, True])\n```\n:::\n:::\n\n\nThen we can easily measure our accuracy by calling is_3 on the list of 3s in our validation set, where each of these should values should be True. We can use the mean() method here after converting the elements to float since Trues will convert to 1.0, while False will be 0. \n\nOur accuracy for 7s is then calculated by using the same method on our list of 7s (where any True value pushing the mean above 0 is actually a 7 being misclassified as a 3), and then subtract that from 1. \n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\naccuracy_3s =      is_3(valid_3_tens).float() .mean()\naccuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n\naccuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n(tensor(0.9168), tensor(0.9854), tensor(0.9511))\n```\n:::\n:::\n\n\nWOW!!! That was actually a lot. But it didn't really feel like artificial intelligence or machine learning or whatever fancy buzzword you want to call it now. All those calculations felt pretty straightforward, fixed, and mechanical. How can we put the intelligence in artifical intelligence?? Well you know what they say: you can't spell 'Machine LearSGDning' without SGD!\n\nSo the basic loop for machine learning is \n1. initialize weights\n2. Predict\n3. Calculate loss (a measurement of how far away your prediction is from the its actual label - basically how wrong your model is. We want to minimize this)\n4. Calculate gradient and modify the weights based on this. The gradient is the rate of change of a function relative to one of the function's parameters for a given value of that parameter. We apply this to our loss function, relative to the parameters of our model. Basically, how does our loss function change when we increment/decrement one of our model's weights. If the gradient is positive, it means that increasing that weight will increase the lose (and thus make our model more inaccurate). A negative gradient means that increasing that weight will decrease the loss (and thus make our model more accurate). The inverse is also true, where decreasing a weight with a positive gradient will decrease the loss. As minimizing loss is our goal, we want to subtract this gradient from our weight, so that we move in the opposite direction of what will increase it. The magnitude of the gradient also tells us how large a change in the loss will result from a change in the weight. Therefore, we will make changes to the weight that are relative to the size of the gradient, which will be some fraction of the gradient based on our learning rate. \n5. Repeat steps 2 - 4 until we stop (until the accuracy of our model is at a certain point, or after a predetermined amount of iterations)\n\n\nNow lets apply this all to our digit classification. Instead of having an 'average' image of a 3 and 7 that we will compare our images to, lets try and create a mathematical formula that will tell us an image is a 3 or a 7. The quantitative data that we have to work with are the pixel intensity values of the image, so we will work with those. We have 784 pretty obvious numbers to use for any given image, so many we can find a weight for each of those pixels that will return a high value if a number is a 3 or a 7, and something low otherwise. We'll use gradient descent to automate the creation of this equation. \n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\ntrain_x.shape,train_y.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n(torch.Size([12396, 784]), torch.Size([12396, 1]))\n```\n:::\n:::\n\n\nWe'll put our pictures into a rank-2 tensor, with an element for each tensor of pixel values. Since our classification functions's weights will be a 1-d tensor, we'll format our picture tensors in 1 dimension as well with shape (784,1) rather than a 2-d one with shape (28,28). We'll reshape our labels as well, so that it's also a rank 2 tensor, with an element for each of our images that is a tensor (with just a single value in it) containing our images label. A will will represent that the image is a 3, and a 0 will represent it being a 7. \n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ndset = list(zip(train_x,train_y))\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nvalid_dset = list(zip(valid_x,valid_y))\n\n```\n:::\n\n\nWe'll use the zip method to create a list of corresponding (training data, label) pairs and do the same thing for our validation set\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\nweights = init_params((28*28,1))\nbias = init_params(1)\n```\n:::\n\n\nNow we'll write a helper method that will just return a tensor of randomized values in the shape that we give it, which we'll pass in the shape of one of our pieces of data (a tensor of size [784,1] representing all the pixel values of a given image) to it as an argument. We'll also add in our bias. You might be familiar with the general linear equation y = mx+b. The b is our bias, and will just be a constant value that we add. \n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n(train_x[0]*weights.T).sum() + bias\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\ntensor([-6.2330], grad_fn=<AddBackward0>)\n```\n:::\n:::\n\n\nNow our prediction is given by taking the dot-product of one of our data points, with our weights (making sure to use the transpose of our weights so that their are properly oriented for matrix multiplication) and adding the bias.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\ndef linear1(xb): return xb@weights + bias\npreds = linear1(train_x)\npreds\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\ntensor([[ -6.2330],\n        [-10.6388],\n        [-20.8865],\n        ...,\n        [-15.9176],\n        [ -1.6866],\n        [-11.3568]], grad_fn=<AddBackward0>)\n```\n:::\n:::\n\n\nThis is just the prediciton for one of our data points. In order to make predictions for our set of images, we will matrix multiply the matrix that they're contained in, with the matrix our weights are contained in.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\ncorrects = (preds>0.0).float() == train_y\ncorrects\ncorrects.float().mean().item()\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\n0.5379961133003235\n```\n:::\n:::\n\n\nSince our outputs from the matmul are random numbers, and our labels are either 1 or 0, then we'll translate all of our outputs into a boolean by comparing it to 0.0. Then we'll convert it to a 1 or 0 by calling .float(). Now, we can compare it to our known labels with \"== train_y\". Now by converting these booleans to 1/0 again, and taking the mean of this tensor, it will give us the percentage of our predictions that were correct. \n\nThis is good at seeing well our model is doing, but it actually isn't a good loss function for optimizing the weights in our predictive model. This is because this accuracy function will only change when a prediction that was previously incorrect or correct switches to the other category, which would probably require a large change in our models weights. Thus, the gradient will be 0 most of the time which won't be of use to us. \n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ndef mnist_loss(predictions, targets):\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n```\n:::\n\n\nInstead we'll define a new loss function for ourselves. The way we're predicting if a number is a 3 or 7 is if the output of our model is a 0, or higher. We will treat our predictions as probabilities that a certain number is a 3 or 7. If it is higher than .5, the model thinks it is more likely to be a 3 than a 7, and if less than .5, than it thinks it more likely to be a 7. With this, we will take our tensor of probabilities for all of our images, and use the .where() method which will use the first argument as a condition, and create a new tensor of the same size, filing in the values at the corresponding indices with the 2nd argument if the condition is true, otherwise with the 3rd argument (basically b[i] if a[i] else c[i]). \n\nWith this, we'll be able to create a tensor of how accurate our probabilities were. If an image is a 3, then the value populated is 1 - prediction, which should be closer to 0 the higher the prediction, which is how sure we are it is a 3. If it is a 7, then it will also be closer to 0, the lower the prediction (which is equal to unsure we are it is a 3 aka how sure we are it is 7). Basically, if we are right, it is inversley proportional to how confident we were in that prediction, and if we were wrong, it will be directly proportional to how confident we were in that wrong answer. \n\nWAIT! You have be thinking to yourself, probabilities should be between 1 and 0, but the output from our model can be of any size. It may be much greater than 1, or even negative. In order to get the output of our model into a number between 0 and 1, we'll use something called the sigmoid function in order to resize our number into someting in this range. It's defined below (but we'll be using the one in the PyTorch library as it's better optimized).\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\ndef mnist_loss(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n```\n:::\n\n\nWe'll rewrite our loss function to transform the output of our model with the sigmoid function to get it in the range of 0 to 1.\n\nSo again, why did we do all of this when we originally had a metric that was literally our accuracy of the model? The accuracy metric was for human understanding, while this loss function is for the machine to learn from. It is easy for us to interpret the percentage of our predictions that are accurate, but it lacked a meaningful gradient because it was flat for much of the values of our models weights. So in the end, these functions have different attributes, and they serve different purposes. \n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nweights = init_params((28*28,1))\nbias = init_params(1)\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape,yb.shape\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n```\n:::\n\n\nAdditionally, in our example we used our gradients to find out how we should adjust our parameters based on how they were performing across the entire dataset. In practice, this becomes very computationally demanding, and so we'll use a practice called mini-batching in order to select subsets of our data that are small enough as to enable us to train a model efficiently, but large enough so that they're still representative of our dataset. Choosing an effective  mini batch size is an essential and important practice in itself. \n\nNote - we'll use stochastic gradient descent for the rest of the notebook, because we're calculating the gradients for the loss and using it for updating our parameters after each data point, which is different from truly mini-batching or batch gradient descent where we calculate the gradient over the entire batch, take the average of those gradients, and that average to update our parameters. \n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\n#weights.grad.zero_()\n#bias.grad.zero_();\n```\n:::\n\n\nPutting this all together, we'll define calc_grad() which will create a prediction for a set of data based on our model, calculate the loss based on our defined loss function of mnist_loss, and calculate the gradient of the loss function. \n\nNote: loss.backward adds the gradients of loss to any stored gradients, so we'll have to reset the gradients after each call. The _() at the end of the method denotes that the object is modified in place. \n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n```\n:::\n\n\nNow lets put this gradient calculation into our larger epoch (a pass through the data set or all our batches) training step. We'll calculate the gradient, and for each one of our model's parameters, update that weight (or the bias constant) based on the calculated gradient for that parameter and our learning rate. Then reset the gradient as discussed above. \n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds>0.5) == yb\n    return correct.float().mean()\n```\n:::\n\n\nNow let's create a function using the sigmoid to facilitate the process of checking the accuracy of our predictions \n\nSince the sigmoid will transform our output to a value between 0 and 1 then our threshold for where we consider ourselves more confident in a number being a 3 or a 7 is the halfway at 0.5. As the name implies, this will get the accuracy of the particular batch we're working with.\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n```\n:::\n\n\nNow we'll write a helper method to get the accuracy of all of the batches in our validation set contained in our 'valid_dl' DataLoader by leveraging a call to batch_accuracy for all of our batches, and then taking the mean of those returned accuracies.\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nlr = 1.\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.8568 0.9095 0.9295 0.9398 0.9466 0.9545 0.9569 0.9628 0.9647 0.9661 0.9671 0.9681 0.9725 0.9725 0.9725 0.973 0.9735 0.974 0.974 0.9749 \n```\n:::\n:::\n\n\nWith all of these parts - the loss function, our gradient calculation method, the epoch training method, and our methods to determine the accuracy of a batch, and the entire validation set, we can train our model, and see how much our model is improving because of it.\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nlinear_model = nn.Linear(28*28,1)\nw,b = linear_model.parameters()\nw.shape,b.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n(torch.Size([1, 784]), torch.Size([1]))\n```\n:::\n:::\n\n\nWe'll use the Linear module from Pytorch's nn module to take care of our the functionality we defined in linear1. We'll set the parameters with the .parameters method\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n```\n:::\n\n\nLet's also create a BasicOptim class that will take care of the optimizing functionality that we've created methods for, which is the part where we update our model weights using gradient descent.\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nopt = BasicOptim(linear_model.parameters(), lr)\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n```\n:::\n\n\nNow let's use the Linear module and our optimizer class to simply the code for training our model. The dl comes from a variable defined earlier in this notebook where we put our dataset into a DataLoader. \n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n```\n:::\n\n\nAnd we'll also create a method that will run this train_epoch() method for a predetermined amount of epochs. \n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nlinear_model = nn.Linear(28*28,1)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\ndls = DataLoaders(dl, valid_dl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.4932 0.8706 0.8276 0.9101 0.9331 0.9458 0.9551 0.9629 0.9658 0.9673 0.9687 0.9712 0.9741 0.9751 0.9761 0.9761 0.9775 0.978 0.9785 0.9785 \n```\n:::\n:::\n\n\nBut this was actually all a prank because we have more fastai classes that will handle this functionality that we just defined for us. SGD will replace the BasicOptim class, and we are putting our training and validation dataloader into a DataLoaders class, so that we can use the Learner class's .fit method, which will replace our train_model() method. \n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\nlearn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nlearn.fit(10, lr=lr)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>batch_accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.636884</td>\n      <td>0.503483</td>\n      <td>0.495584</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.525323</td>\n      <td>0.197635</td>\n      <td>0.830716</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.193055</td>\n      <td>0.175610</td>\n      <td>0.842493</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.084568</td>\n      <td>0.105104</td>\n      <td>0.911678</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.044592</td>\n      <td>0.077108</td>\n      <td>0.933268</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.028991</td>\n      <td>0.061910</td>\n      <td>0.947007</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.022591</td>\n      <td>0.052414</td>\n      <td>0.954858</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.019752</td>\n      <td>0.046097</td>\n      <td>0.962218</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.018318</td>\n      <td>0.041659</td>\n      <td>0.966143</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.017461</td>\n      <td>0.038389</td>\n      <td>0.968106</td>\n      <td>00:00</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nTo use this Learner class, we must instanstiate it with our DataLoaders containing the training and validation set, the model (using the Linear class), our optimizing function, which is going to have the parameters of our model passed to it, along with the learning rate when we call .fit, along with our loss function and any metrics. \n\nThen we call .fit with the number of epochs and learning rate as arguments \n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\ndef simple_net(xb): \n    res = xb@w1 + b1\n    res = res.max(tensor(0.0))\n    res = res@w2 + b2\n    return res\n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n```\n:::\n\n\nLet's put the linear function we just created a model for using gradient descent into an actual neural net. This will be comprised of two linear functions with a ReLU (rectified linear unit) in the middle. This is our nonlinear component that gives this composition even more flexibility in the problems it can solve. The ReLU is simply taking the max of the output, and 0 (turning any negative number we have to 0). \n\nWe initalize our first and second set of weights to be of size (784, 30) and (30,1) respectively. In order to have the proper size matrices for multiplication - we need to have the number of rows in the weight matrix be equal to the the number of columns in the input. In this case, we have a column for each of the image's 784 pixels. The number of columns of our weight matrix will be equal to the amount of outputs we want to feed in as inputs to the next layer - which we will choose to be 30 a little bit arbitrarily. Now, since we only have two layers, the output of this next weight matrix should be a single number that will be used for our prediction, so this second weight matrix will only have 1 column to produce that singular output.\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n```\n:::\n\n\nUsing PyTorch, we can build out the same simple neural network with this code. The nn.Sequential model will call each of these layers in turn. We have our 2 Linear models that we used previously, sandwiching our ReLU layer. \n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nlearn.fit(40, 0.1)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>batch_accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.316185</td>\n      <td>0.399123</td>\n      <td>0.512758</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.146678</td>\n      <td>0.228968</td>\n      <td>0.805692</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.080991</td>\n      <td>0.114859</td>\n      <td>0.915604</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.053001</td>\n      <td>0.077410</td>\n      <td>0.939647</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.040057</td>\n      <td>0.060447</td>\n      <td>0.955839</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.033485</td>\n      <td>0.050950</td>\n      <td>0.963690</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.029739</td>\n      <td>0.044957</td>\n      <td>0.966634</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.027323</td>\n      <td>0.040858</td>\n      <td>0.969578</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.025593</td>\n      <td>0.037880</td>\n      <td>0.970559</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.024256</td>\n      <td>0.035617</td>\n      <td>0.972031</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.023172</td>\n      <td>0.033827</td>\n      <td>0.973013</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.022266</td>\n      <td>0.032365</td>\n      <td>0.973994</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.021493</td>\n      <td>0.031136</td>\n      <td>0.974975</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.020823</td>\n      <td>0.030082</td>\n      <td>0.975957</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.020234</td>\n      <td>0.029161</td>\n      <td>0.975957</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.019711</td>\n      <td>0.028346</td>\n      <td>0.975957</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.019243</td>\n      <td>0.027618</td>\n      <td>0.976938</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.018820</td>\n      <td>0.026962</td>\n      <td>0.978410</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.018435</td>\n      <td>0.026369</td>\n      <td>0.978901</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.018083</td>\n      <td>0.025831</td>\n      <td>0.979392</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.017759</td>\n      <td>0.025340</td>\n      <td>0.979392</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.017459</td>\n      <td>0.024891</td>\n      <td>0.979882</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.017179</td>\n      <td>0.024480</td>\n      <td>0.980373</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.016918</td>\n      <td>0.024103</td>\n      <td>0.980373</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.016674</td>\n      <td>0.023755</td>\n      <td>0.980373</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.016444</td>\n      <td>0.023435</td>\n      <td>0.980373</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.016227</td>\n      <td>0.023138</td>\n      <td>0.980864</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.016021</td>\n      <td>0.022864</td>\n      <td>0.980864</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.015826</td>\n      <td>0.022609</td>\n      <td>0.981354</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.015641</td>\n      <td>0.022372</td>\n      <td>0.981845</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.015465</td>\n      <td>0.022152</td>\n      <td>0.981354</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.015298</td>\n      <td>0.021946</td>\n      <td>0.981354</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.015138</td>\n      <td>0.021753</td>\n      <td>0.981354</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.014985</td>\n      <td>0.021573</td>\n      <td>0.981354</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.014838</td>\n      <td>0.021404</td>\n      <td>0.981845</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.014698</td>\n      <td>0.021244</td>\n      <td>0.981845</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.014563</td>\n      <td>0.021094</td>\n      <td>0.981845</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.014434</td>\n      <td>0.020952</td>\n      <td>0.982336</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.014310</td>\n      <td>0.020818</td>\n      <td>0.982336</td>\n      <td>00:00</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.014190</td>\n      <td>0.020691</td>\n      <td>0.982826</td>\n      <td>00:00</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nAfter putting this new neural network model into our Learner and fitting it for a number of epochs, we can see the performance from our batch_accuracy metric doing better than just our simple single layered linear model. \n\nBRUH. Wow we just basically implemented our first machine learning model and have gone over the foundational aspects that underlie every machine learning model with using some sort of optimization function (usually stochastic gradient descent) with respect to our loss function to tweak and optimize the parameters of our model. We've seen that once we have our data, a learning rate, and the architecture of our neural network, it's actually basically just a formality to implement a deep learning model at that point and can be done in a few lines of code. The real work with deep learning comes before all that, in doing the necessary work to properly prepare all of those components, and determine what optimal hyperparameters are. \n\nLet's cap off this lesson with a celebration: going over some jargon that will be necessary to continue our understanding of this subject. As you've just seen, none of this is actually particularly complicated. And at this point, we have the luxury of having very powerful machines do all of the manual computation for us. Thus, in order for academics and practioners  in this field to make themselves feel better about themselves, they introduced a lot of complicated sounding jargon to make what they were talking about more mystical and technical than it really is. Here's a breakdown:\n\nActivations - The numbers calculated by our layers. The number that we put into the sigmoid to give us our probability that a certain image was a 3 or 7 was an activation.  \nParameters - Numbers that we use to calculate these activation values. They are the weights of our model. In this excercise, they were the 784 values that we multiplied by each corresponding pixel value in a given image by, as well as our bias. \n\nTensors - regularly shaped arrays. They have a rank, which is equal to the number of axes/dimensions they have. For example:\n\nRank-0 Tensor: Scalar (a single value)\nRank-1 Tensor: Vector (a list of scalars)\nRank-2 Tensor: Matrix (a list of vectors aka a list of a list of scalars)\nThus, a Rank-3 Tensor would be a list of matrices, a rank-4 would be matrix of matrices (wtf trippy) and so on and so forth. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}