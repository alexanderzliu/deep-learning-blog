{
  "hash": "3fb69a504ffd43750fa1fdfabe329101",
  "result": {
    "markdown": "---\ntitle: 'Hugging Face NLP I: Transformer Architecture Overview'\nauthor: Alex Liu\ndate: '2023-11-31'\ncategories:\n  - Hugging Face\n  - NLP\n  - Course Notes\nimage: huggingface1.png\n---\n\n## What NLP tasks can transforms do?\n\n##### Zero-shot classification\n\nLabel text with your own provided set of labels without fine tuning the model on any of your own data\n\n```{{python}}\nclassifier = pipeline(\"zero-shot-classification\")\nclassifier(\n    \"This is a course about the Transformers library\",\n    candidate_labels=[\"education\", \"politics\", \"business\"],\n)\n\n{'sequence': 'This is a course about the Transformers library',\n 'labels': ['education', 'business', 'politics'],\n 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}\n```\n\n\n##### Text Generation\n\nAuto-complete subsequent words based on a provided prompt \n\n```{{python}}\ngenerator = pipeline(\"text-generation\")\ngenerator(\"In this course, we will teach you how to\")\n\n[{'generated_text': 'In this course, we will teach you how to understand and use '\n                    'data flow and data interchange when handling user data. We '\n                    'will be working with one or more of the most commonly used '\n                    'data flows â€” data flows of various types, as seen by the '\n                    'HTTP'}]\n```\n\n##### Mask Filling\n\nFill in the blanks of a given text\n\n```{{python}}\nunmasker = pipeline(\"fill-mask\")\nunmasker(\"This course will teach you all about <mask> models.\", top_k=2)\nCopied\n[{'sequence': 'This course will teach you all about mathematical models.',\n  'score': 0.19619831442832947,\n  'token': 30412,\n  'token_str': ' mathematical'},\n {'sequence': 'This course will teach you all about computational models.',\n  'score': 0.04052725434303284,\n  'token': 38163,\n  'token_str': ' computational'}]\n```\n\n##### Named Entity Recognition\n\nFind which parts of an input text are certain entities like persons, locations, organizations. Different models can tag inputs with different entity labels, like for parts of speech. \n\n```{{python}}\nner = pipeline(\"ner\", grouped_entities=True)\nner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\nCopied\n[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, \n {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, \n {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n]\n```\n\n\n##### Question Answering \n\nAnswer a question based on given context\n\n```{{python}}\nquestion_answerer = pipeline(\"question-answering\")\nquestion_answerer(\n    question=\"Where do I work?\",\n    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n)\n{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}\n```\n\n##### Summarization\n\nSummarize a given input - turn it into a smaller portion of text, while still retaining all of the important information  \n\n```{{python}}\nsummarizer = pipeline(\"summarization\")\nsummarizer(\n    \"\"\"\n    America has changed dramatically during recent years. Not only has the number of \n    graduates in traditional engineering disciplines such as mechanical, civil, \n    electrical, chemical, and aeronautical engineering declined, but in most of \n    the premier American universities engineering curricula now concentrate on \n    and encourage largely the study of engineering science. As a result, there \n    are declining offerings in engineering subjects dealing with infrastructure, \n    the environment, and related issues, and greater concentration on high \n    technology subjects, largely supporting increasingly complex scientific \n    developments. While the latter is important, it should not be at the expense \n    of more traditional engineering.\n\n    Rapidly developing economies such as China and India, as well as other \n    industrial countries in Europe and Asia, continue to encourage and advance \n    the teaching of engineering. Both China and India, respectively, graduate \n    six and eight times as many traditional engineers as does the United States. \n    Other industrial countries at minimum maintain their output, while America \n    suffers an increasingly serious decline in the number of engineering graduates \n    and a lack of well-educated engineers.\n\"\"\"\n)\n[{'summary_text': ' America has changed dramatically during recent years . The '\n                  'number of engineering graduates in the U.S. has declined in '\n                  'traditional engineering disciplines such as mechanical, civil '\n                  ', electrical, chemical, and aeronautical engineering . Rapidly '\n                  'developing economies such as China and India, as well as other '\n                  'industrial countries in Europe and Asia, continue to encourage '\n                  'and advance engineering .'}]\n```\n\n##### Translation\n\nConvert text from one language into another\n\n```{{python}}\ntranslator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\ntranslator(\"Ce cours est produit par Hugging Face.\")\n[{'translation_text': 'This course is produced by Hugging Face.'}]\n```\n\n## Transformers. How do they work?\n\nTransformers are language models that have been trained on a massive amount of raw, unlabeled data through a particular unsupervised learning technique called \"self-supervised learning\". This allows the model to build up a general understanding of the language it's trained on, but it isn't good at any particular task. \n\nAt this point, we can call this model a \"general, pretrained transformer\" (OMG THAT'S WHAT GPT STANDS FOR).\n\nWe can then use *transfer learning* to fine-tune our transformer on labeled data (supervised learning).\n\nThis practice of fine-tuning a pretained model to apply it towards specific use cases is preferable as the process of pre-training, that is starting from scratch and training a language model on a huge corpus text takes a massive amount of time & resources. So much so that the energy required for it has a very non-trivial carbon footprint. So yeah, I guess you could pretrain models yourself. IF YOU HATE THE EARTH.\n\n## (Very) General Architecture\n\nThe Transformer model is comprised of two parts - an encoder and a decoder. \n\n\n#### Encoder\nThe **encoder** takes in the input and encodes it into a numerical representation (it's features) that the model can understand.  \n\nThese 'feature vectors' are sequences of numbers with one sequence for each word in the input. A word's feature vectors are also influenced by the other words in the input. So in the case of inputs like \"the horse\" and \"the grove\", 'the' would be represented as a different sequence of numbers in each case. This is essentially just how the meaning of words are determined by the context in which they are used. \n\nEncoders are also 'bi-directional', which means these feature vectors are influenced by words located before and after it in the text.\n\nThey're usually trained through mask-filling tasks, so they build up a capability to understand words given the context of pre- and proceding words.\n\nTasks that are centered around understanding an input, like classification or named entity recognition can be done with just the encoder.\n\n\n#### Decoder\nThe **decoder** takes the features - our encoded representation, and possibly other inputs to generate an output. \n\nUnlike encoders, they are uni-directional. The only have access to the words located either before or after a given word in a sequence. This makes sense since if our task is to generated a new string of words based on a preceding sequence, it would be trivialized if we had knowledge of what the upcoming words actually were. This is more relevant in the training of these models. We don't want the decoder to get into the habit of generating its predictions for future words, by just peeking ahead in the training data and seeing what those words actually are, since it won't be able to do this when we actually try and use it to complete a sentence, since there will be nothing to peek ahead at.\n\nThese models are also auto-regressive, meaning sequential predictions build upon each other, using previous predictions as input. For example, if we ask it to predict the next 4 words in a sentence, it will predict word 1, use word 1 to predict word 2, use words 1 & 2 to predict word 3, and so on. \n\nThey're trained through text generation tasks, so they build up the capaibility to predict words in sequence.\n\nThese are used for generative tasks like text generation. \n\n#### Attention Layers\n\nTransformers have a key feature called 'attention layers' that have enabled their impressive performance. At a very high level, when the model is dealing with the representation a.k.a trying to understand  the words in an input, the attention layer tells it to pay attention to other specific words to better understand that particular word. \n\nAn example would be for translating from English, to a language with gendered nouns like Spanish. To translate \"the bike\" to Spanish, the model will look at the words 'the' and 'bike'. However, when translating 'the', the model will also need to pay attention to the word 'bike', because that will tell it if it should be masculine or feminine i.e. determine if it's 'el' or 'la'. \n\n#### Sequence-to-sequence models\n\nThey can be used in tandem for generative tasks that require understanding of an input, like translation or summarization. The encoder takes in an input sequence and outputs a the feature vectors - our numerical representation of the input. The decoder takes in this feature vector as input, along with any other inputs (which as we start predicting tokens, would be our sequence of previously predicted tokens) and generates predictions for subsequent tokens. \n\n",
    "supporting": [
      "index1_files"
    ],
    "filters": [],
    "includes": {}
  }
}